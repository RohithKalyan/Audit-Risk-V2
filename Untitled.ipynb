{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "616482cc-c1cf-4993-9756-721048f98431",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install fastapi uvicorn pydantic requests pandas numpy catboost shap sentence-transformers scikit-learn umap-learn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffc210de-7bf4-4ba1-8fb9-4cab57f76eb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from model_logic import run_full_pipeline   # using the old file you uploaded\n",
    "\n",
    "# Path to your old CSV file\n",
    "file_path = \"test1 (3).csv\"\n",
    "\n",
    "# Run pipeline\n",
    "results = run_full_pipeline(file_path)\n",
    "\n",
    "# Preview results\n",
    "display(results.head())\n",
    "\n",
    "# Check schema\n",
    "print(results.columns.tolist())\n",
    "\n",
    "# Save to Excel if you want to inspect further\n",
    "results.to_excel(\"old_code_output.xlsx\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93597181-18bb-4f95-9db7-7cf3864ac294",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from app.model_logic import run_full_pipeline  # after you replace the file\n",
    "\n",
    "# 1) Define the locked schema (exact names + order)\n",
    "EXPECTED = [\n",
    "    \"S.No\",\"Entity Name\",\"Accounting Date\",\"Approval Type\",\"Document Type\",\"Invoice Date\",\"Day\",\"Nature\",\n",
    "    \"Account Code\",\"PL BS\",\"Report Group\",\"Account Name\",\"Nature In Balance Sheet\",\"Document Number\",\"Je Line Num\",\n",
    "    \"Source Number\",\"Source Name\",\"Source Desc\",\"Line Desc\",\"Project Code\",\"Internal Reference\",\"Posted Date\",\"Branch\",\n",
    "    \"Batch Name\",\"Entered Dr SUM\",\"Entered Cr SUM\",\"Accounted Dr SUM\",\"Accounted Cr SUM\",\"Net Amount\",\n",
    "    \"Model Score\",\"CP Score\",\"Triggered C Ps\",\"Results Explanation Summary\"\n",
    "]\n",
    "\n",
    "# 2) Run on the same old input file you used for the baseline\n",
    "NEW = run_full_pipeline(\"test1 (3).csv\")\n",
    "\n",
    "# 3) Schema checks\n",
    "assert list(NEW.columns) == EXPECTED, f\"Column mismatch.\\nGot: {list(NEW.columns)}\"\n",
    "print(\"✅ Schema OK (names + order)\")\n",
    "\n",
    "# 4) Quick sanity: types for key numeric columns (tweak if you expect strings)\n",
    "for c in [\"Entered Dr SUM\",\"Entered Cr SUM\",\"Accounted Dr SUM\",\"Accounted Cr SUM\",\"Net Amount\",\"Model Score\",\"CP Score\"]:\n",
    "    assert c in NEW.columns, f\"Missing {c}\"\n",
    "    assert pd.api.types.is_numeric_dtype(NEW[c]), f\"{c} is not numeric\"\n",
    "\n",
    "print(\"✅ Key dtypes OK\")\n",
    "NEW.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "874060b5-3a4f-4b18-954f-15fc1cb84f20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save pipeline results to an Excel file (schema locked to old 33 columns)\n",
    "\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "\n",
    "# 1) Import your updated pipeline\n",
    "try:\n",
    "    from app.model_logic import run_full_pipeline\n",
    "except ImportError:\n",
    "    from model_logic import run_full_pipeline  # if file is in the same folder as notebook\n",
    "\n",
    "# 2) Point to your input file (CSV or Excel both OK)\n",
    "INPUT_PATH = r\"test1 (3).csv\"   # <-- change to your file path if needed\n",
    "\n",
    "# 3) Run\n",
    "df = run_full_pipeline(INPUT_PATH)\n",
    "\n",
    "# 4) (Optional, safety) verify schema is the expected 33 columns\n",
    "EXPECTED = [\n",
    "    \"S.No\",\"Entity Name\",\"Accounting Date\",\"Approval Type\",\"Document Type\",\"Invoice Date\",\"Day\",\"Nature\",\n",
    "    \"Account Code\",\"PL BS\",\"Report Group\",\"Account Name\",\"Nature In Balance Sheet\",\"Document Number\",\"Je Line Num\",\n",
    "    \"Source Number\",\"Source Name\",\"Source Desc\",\"Line Desc\",\"Project Code\",\"Internal Reference\",\"Posted Date\",\"Branch\",\n",
    "    \"Batch Name\",\"Entered Dr SUM\",\"Entered Cr SUM\",\"Accounted Dr SUM\",\"Accounted Cr SUM\",\"Net Amount\",\n",
    "    \"Model Score\",\"CP Score\",\"Triggered C Ps\",\"Results Explanation Summary\"\n",
    "]\n",
    "assert list(df.columns) == EXPECTED, f\"Schema mismatch.\\nGot: {list(df.columns)}\"\n",
    "\n",
    "# 5) Prepare output path\n",
    "out_dir = Path(\"outputs\")\n",
    "out_dir.mkdir(parents=True, exist_ok=True)\n",
    "base = Path(INPUT_PATH).stem\n",
    "ts = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "out_path = out_dir / f\"{base}__risk_results__{ts}.xlsx\"\n",
    "\n",
    "# 6) Choose an Excel engine you have installed\n",
    "try:\n",
    "    import openpyxl  # noqa: F401\n",
    "    engine = \"openpyxl\"\n",
    "except Exception:\n",
    "    try:\n",
    "        import xlsxwriter  # noqa: F401\n",
    "        engine = \"xlsxwriter\"\n",
    "    except Exception as e:\n",
    "        raise ImportError(\"Install 'openpyxl' or 'xlsxwriter' to write Excel files.\") from e\n",
    "\n",
    "# 7) Write Results + Meta\n",
    "with pd.ExcelWriter(out_path, engine=engine) as xls:\n",
    "    df.to_excel(xls, sheet_name=\"Results\", index=False)\n",
    "    meta = pd.DataFrame({\n",
    "        \"Key\": [\"GeneratedAt\", \"InputFile\", \"Rows\", \"Columns\"],\n",
    "        \"Value\": [\n",
    "            datetime.now().isoformat(timespec=\"seconds\"),\n",
    "            str(Path(INPUT_PATH).resolve()),\n",
    "            len(df),\n",
    "            \", \".join(df.columns)\n",
    "        ]\n",
    "    })\n",
    "    meta.to_excel(xls, sheet_name=\"Meta\", index=False)\n",
    "\n",
    "print(f\"✅ Saved Excel to: {out_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99fc2076-5857-4d14-a7ca-575f70e6a3d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Debug classifications\n",
    "print(\"=== DEBUGGING RISK CLASSIFICATIONS ===\")\n",
    "for i in range(min(10, len(df))):\n",
    "    model_score = df.iloc[i]['Model Score']\n",
    "    cp_score = df.iloc[i]['CP Score'] \n",
    "    \n",
    "    # Manual classification check\n",
    "    model_class = \"High\" if model_score >= 0.995 else (\"Medium\" if model_score >= 0.5 else \"Low\")\n",
    "    cp_class = \"High\" if cp_score >= 0.95 else (\"Medium\" if cp_score > 0.8 else \"Low\") \n",
    "    final_risk = \"High\" if model_class == \"High\" or cp_class == \"High\" else (\"Medium\" if model_class == \"Medium\" or cp_class == \"Medium\" else \"Low\")\n",
    "    \n",
    "    explanation = df.iloc[i]['Results Explanation Summary']\n",
    "    has_explanation = len(str(explanation).strip()) > 100  # Check if there's actual content\n",
    "    \n",
    "    print(f\"Row {i+1}: Model={model_score:.3f}({model_class}), CP={cp_score:.4f}({cp_class}) → Final={final_risk}\")\n",
    "    print(f\"  Has explanation: {has_explanation}\")\n",
    "    if final_risk != \"High\" and has_explanation:\n",
    "        print(f\"  ❌ PROBLEM: Non-High risk has explanation!\")\n",
    "    elif final_risk == \"High\" and \"Risk patterns detected requiring enhanced evaluation and review\" in explanation:\n",
    "        print(f\"  ❌ PROBLEM: High risk has generic explanation instead of specific CP explanations!\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "025a04c5-55f7-4575-b580-66fad93d9df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n=== CHECKING EXPLANATION CONTENT ===\")\n",
    "for i in range(min(5, len(df))):\n",
    "    if i in [2, 3, 4]:  # Check rows 3, 4, 5 (High risk)\n",
    "        row = df.iloc[i]\n",
    "        print(f\"Row {i+1} (High Risk):\")\n",
    "        print(f\"  Top_Risky_Feature_Groups: '{row.get('Top_Risky_Feature_Groups', 'COLUMN MISSING')}'\")\n",
    "        print(f\"  Triggered_CPs: '{row.get('Triggered C Ps', 'COLUMN MISSING')}'\")\n",
    "        print(f\"  Results length: {len(str(row['Results Explanation Summary']))}\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56f854fd-581b-424b-a0a9-ba4d7809a714",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== CHECKING ACTUAL COLUMNS ===\")\n",
    "print(f\"Total columns: {len(df.columns)}\")\n",
    "print(\"Column names:\")\n",
    "for i, col in enumerate(df.columns):\n",
    "    print(f\"  {i+1:2d}. '{col}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9730834d-b139-472c-ba16-3ccc9060d821",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save pipeline results to an Excel file (schema locked to old 33 columns)\n",
    "\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "\n",
    "# 1) Import your updated pipeline\n",
    "try:\n",
    "    from app.model_logic import run_full_pipeline\n",
    "except ImportError:\n",
    "    from model_logic import run_full_pipeline  # if file is in the same folder as notebook\n",
    "\n",
    "# 2) Point to your input file (CSV or Excel both OK)\n",
    "INPUT_PATH = r\"test1 (3).csv\"   # <-- change to your file path if needed\n",
    "\n",
    "# 3) Run\n",
    "df = run_full_pipeline(INPUT_PATH)\n",
    "\n",
    "# 4) (Optional, safety) verify schema is the expected 33 columns\n",
    "EXPECTED = [\n",
    "    \"S.No\",\"Entity Name\",\"Accounting Date\",\"Approval Type\",\"Document Type\",\"Invoice Date\",\"Day\",\"Nature\",\n",
    "    \"Account Code\",\"PL BS\",\"Report Group\",\"Account Name\",\"Nature In Balance Sheet\",\"Document Number\",\"Je Line Num\",\n",
    "    \"Source Number\",\"Source Name\",\"Source Desc\",\"Line Desc\",\"Project Code\",\"Internal Reference\",\"Posted Date\",\"Branch\",\n",
    "    \"Batch Name\",\"Entered Dr SUM\",\"Entered Cr SUM\",\"Accounted Dr SUM\",\"Accounted Cr SUM\",\"Net Amount\",\n",
    "    \"Model Score\",\"CP Score\",\"Triggered C Ps\",\"Results Explanation Summary\"\n",
    "]\n",
    "assert list(df.columns) == EXPECTED, f\"Schema mismatch.\\nGot: {list(df.columns)}\"\n",
    "\n",
    "# 5) Prepare output path\n",
    "out_dir = Path(\"outputs\")\n",
    "out_dir.mkdir(parents=True, exist_ok=True)\n",
    "base = Path(INPUT_PATH).stem\n",
    "ts = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "out_path = out_dir / f\"{base}__risk_results__{ts}.xlsx\"\n",
    "\n",
    "# 6) Choose an Excel engine you have installed\n",
    "try:\n",
    "    import openpyxl  # noqa: F401\n",
    "    engine = \"openpyxl\"\n",
    "except Exception:\n",
    "    try:\n",
    "        import xlsxwriter  # noqa: F401\n",
    "        engine = \"xlsxwriter\"\n",
    "    except Exception as e:\n",
    "        raise ImportError(\"Install 'openpyxl' or 'xlsxwriter' to write Excel files.\") from e\n",
    "\n",
    "# 7) Write Results + Meta\n",
    "with pd.ExcelWriter(out_path, engine=engine) as xls:\n",
    "    df.to_excel(xls, sheet_name=\"Results\", index=False)\n",
    "    meta = pd.DataFrame({\n",
    "        \"Key\": [\"GeneratedAt\", \"InputFile\", \"Rows\", \"Columns\"],\n",
    "        \"Value\": [\n",
    "            datetime.now().isoformat(timespec=\"seconds\"),\n",
    "            str(Path(INPUT_PATH).resolve()),\n",
    "            len(df),\n",
    "            \", \".join(df.columns)\n",
    "        ]\n",
    "    })\n",
    "    meta.to_excel(xls, sheet_name=\"Meta\", index=False)\n",
    "\n",
    "print(f\"✅ Saved Excel to: {out_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0b0d06d-c209-42be-8de5-1ef048406302",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"CURRENT WORKING DIRECTORY:\")\n",
    "print(Path.cwd())\n",
    "\n",
    "print(\"\\nFILE STRUCTURE:\")\n",
    "for root, dirs, files in os.walk('.'):\n",
    "    level = root.replace('.', '').count(os.sep)\n",
    "    indent = ' ' * 2 * level\n",
    "    print(f\"{indent}{os.path.basename(root)}/\")\n",
    "    subindent = ' ' * 2 * (level + 1)\n",
    "    for file in files:\n",
    "        if file.endswith(('.py', '.csv', '.xlsx')):\n",
    "            print(f\"{subindent}{file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cffc10a-a4d6-4a97-9a3e-577c74ed70e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# COMPLETE DIAGNOSTIC SCRIPT - RUN IN JUPYTER\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import traceback\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"DIAGNOSTIC SCRIPT - FINDING THE ISSUE\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Check the specific files\n",
    "print(\"FILE CONTENT CHECK:\")\n",
    "model_logic_app = Path(\"app/model_logic.py\")\n",
    "checkpoint_file = Path(\"app/.ipynb_checkpoints/model_logic-checkpoint.py\")\n",
    "\n",
    "print(f\"app/model_logic.py size: {model_logic_app.stat().st_size:,} bytes\")\n",
    "if checkpoint_file.exists():\n",
    "    print(f\"Checkpoint file size: {checkpoint_file.stat().st_size:,} bytes\")\n",
    "\n",
    "# Check content\n",
    "with open(model_logic_app, 'r', encoding='utf-8') as f:\n",
    "    content = f.read()\n",
    "    if \"VERSION CHECK\" in content:\n",
    "        print(\"✅ Modified version detected in app/model_logic.py\")\n",
    "    else:\n",
    "        print(\"❌ Old version in app/model_logic.py\")\n",
    "\n",
    "if checkpoint_file.exists():\n",
    "    with open(checkpoint_file, 'r', encoding='utf-8') as f:\n",
    "        checkpoint_content = f.read()\n",
    "        if \"VERSION CHECK\" in checkpoint_content:\n",
    "            print(\"✅ Modified version in checkpoint\")\n",
    "        else:\n",
    "            print(\"❌ Old version in checkpoint - THIS IS THE PROBLEM!\")\n",
    "\n",
    "# Test import and execution\n",
    "print(\"\\nIMPORT TEST:\")\n",
    "try:\n",
    "    from app.model_logic import run_full_pipeline\n",
    "    print(\"✅ Import successful\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Import failed: {e}\")\n",
    "\n",
    "# Test function with input file\n",
    "INPUT_PATH = r\"test1 (3).csv\"\n",
    "print(f\"\\nTESTING WITH: {INPUT_PATH}\")\n",
    "\n",
    "try:\n",
    "    print(\"Calling run_full_pipeline...\")\n",
    "    df = run_full_pipeline(INPUT_PATH)\n",
    "    print(f\"✅ Success! Shape: {df.shape}\")\n",
    "    \n",
    "    # Check for debug output by examining the explanation column\n",
    "    print(\"\\nSAMPLE EXPLANATIONS:\")\n",
    "    for i in range(min(3, len(df))):\n",
    "        explanation = df.iloc[i]['Results Explanation Summary']\n",
    "        print(f\"Row {i+1}: '{explanation}'\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"❌ Function failed: {e}\")\n",
    "    traceback.print_exc()\n",
    "\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac89dd02-b4b0-4168-accb-8d3a2d5c5a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import sys\n",
    "\n",
    "# Remove from cache if already imported\n",
    "if 'app.model_logic' in sys.modules:\n",
    "    del sys.modules['app.model_logic']\n",
    "\n",
    "# Now import fresh\n",
    "from app.model_logic import run_full_pipeline\n",
    "\n",
    "# Test immediately\n",
    "INPUT_PATH = r\"test1 (3).csv\"\n",
    "df = run_full_pipeline(INPUT_PATH)\n",
    "\n",
    "# Check results\n",
    "for i in range(min(3, len(df))):\n",
    "    explanation = df.iloc[i]['Results Explanation Summary']\n",
    "    print(f\"Row {i+1}: '{explanation}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e745bb79-56a7-4cb4-9965-f4c7302f175e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, let's check if our changes are actually in the file\n",
    "with open(\"app/model_logic.py\", \"r\") as f:\n",
    "    content = f.read()\n",
    "    print(\"File contains our debug messages:\", \"CP_Score column created successfully\" in content)\n",
    "    print(\"File size:\", len(content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "be4dc16c-81ab-4b8d-8359-e0215cdde807",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚨 USING MODIFIED MODEL_LOGIC.PY - VERSION CHECK\n",
      "📦 Loading CatBoost model...\n",
      "✅ CatBoost model loaded\n",
      "📦 Downloading SentenceTransformer...\n",
      "✅ SentenceTransformer loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  File \"C:\\Users\\Acer\\anaconda3\\Lib\\site-packages\\joblib\\externals\\loky\\backend\\context.py\", line 257, in _count_physical_cores\n",
      "    cpu_info = subprocess.run(\n",
      "        \"wmic CPU Get NumberOfCores /Format:csv\".split(),\n",
      "        capture_output=True,\n",
      "        text=True,\n",
      "    )\n",
      "  File \"C:\\Users\\Acer\\anaconda3\\Lib\\subprocess.py\", line 554, in run\n",
      "    with Popen(*popenargs, **kwargs) as process:\n",
      "         ~~~~~^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Acer\\anaconda3\\Lib\\subprocess.py\", line 1039, in __init__\n",
      "    self._execute_child(args, executable, preexec_fn, close_fds,\n",
      "    ~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "                        pass_fds, cwd, env,\n",
      "                        ^^^^^^^^^^^^^^^^^^^\n",
      "    ...<5 lines>...\n",
      "                        gid, gids, uid, umask,\n",
      "                        ^^^^^^^^^^^^^^^^^^^^^^\n",
      "                        start_new_session, process_group)\n",
      "                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Acer\\anaconda3\\Lib\\subprocess.py\", line 1554, in _execute_child\n",
      "    hp, ht, pid, tid = _winapi.CreateProcess(executable, args,\n",
      "                       ~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^\n",
      "                             # no special security\n",
      "                             ^^^^^^^^^^^^^^^^^^^^^\n",
      "    ...<4 lines>...\n",
      "                             cwd,\n",
      "                             ^^^^\n",
      "                             startupinfo)\n",
      "                             ^^^^^^^^^^^^\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ BERT embeddings and clustering complete\n",
      "🚨 ABOUT TO START SHAP SECTION\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'CP_Score'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[0;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mindex.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7081\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7089\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'CP_Score'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mapp\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_logic\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m run_full_pipeline\n\u001b[0;32m      4\u001b[0m INPUT_PATH \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest1 (3).csv\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 5\u001b[0m df \u001b[38;5;241m=\u001b[39m run_full_pipeline(INPUT_PATH)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Check results\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mmin\u001b[39m(\u001b[38;5;241m3\u001b[39m, \u001b[38;5;28mlen\u001b[39m(df))):\n",
      "File \u001b[1;32m~\\Audit-Risk-V2\\app\\model_logic.py:712\u001b[0m, in \u001b[0;36mrun_full_pipeline\u001b[1;34m(file_path)\u001b[0m\n\u001b[0;32m    709\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLow\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    711\u001b[0m test_df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel Classification\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m test_df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFinal_Score\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(classify_model_score)\n\u001b[1;32m--> 712\u001b[0m test_df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCP Classification\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m test_df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCP_Score\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(classify_cp_score)\n\u001b[0;32m    713\u001b[0m test_df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFinal Risk Classification\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m test_df\u001b[38;5;241m.\u001b[39mapply(\n\u001b[0;32m    714\u001b[0m     \u001b[38;5;28;01mlambda\u001b[39;00m row: determine_final_risk(row[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel Classification\u001b[39m\u001b[38;5;124m\"\u001b[39m], row[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCP Classification\u001b[39m\u001b[38;5;124m\"\u001b[39m]), axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    715\u001b[0m )\n\u001b[0;32m    717\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mget_cp_explanation\u001b[39m(cp_code, row):\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\frame.py:4102\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   4100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   4101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 4102\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mget_loc(key)\n\u001b[0;32m   4103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   4104\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3807\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[0;32m   3808\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[0;32m   3809\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[0;32m   3810\u001b[0m     ):\n\u001b[0;32m   3811\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[1;32m-> 3812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3814\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3815\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3816\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3817\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'CP_Score'"
     ]
    }
   ],
   "source": [
    "# Fresh start - no cache clearing needed after kernel restart\n",
    "from app.model_logic import run_full_pipeline\n",
    "\n",
    "INPUT_PATH = r\"test1 (3).csv\"\n",
    "df = run_full_pipeline(INPUT_PATH)\n",
    "\n",
    "# Check results\n",
    "for i in range(min(3, len(df))):\n",
    "    explanation = df.iloc[i]['Results Explanation Summary']\n",
    "    print(f\"Row {i+1}: '{explanation}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "61f6e116-bbb5-4660-bf43-7d25bde176bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚨 USING MODIFIED MODEL_LOGIC.PY - VERSION CHECK\n",
      "📦 Loading CatBoost model...\n",
      "✅ CatBoost model loaded\n",
      "📦 Downloading SentenceTransformer...\n",
      "✅ SentenceTransformer loaded\n",
      "✅ BERT embeddings and clustering complete\n",
      "🚨 ABOUT TO START SHAP SECTION\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'CP_Score'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[0;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mindex.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7081\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7089\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'CP_Score'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 17\u001b[0m\n\u001b[0;32m     14\u001b[0m INPUT_PATH \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest1 (3).csv\u001b[39m\u001b[38;5;124m\"\u001b[39m   \u001b[38;5;66;03m# <-- change to your file path if needed\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# 3) Run\u001b[39;00m\n\u001b[1;32m---> 17\u001b[0m df \u001b[38;5;241m=\u001b[39m run_full_pipeline(INPUT_PATH)\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# 4) (Optional, safety) verify schema is the expected 33 columns\u001b[39;00m\n\u001b[0;32m     20\u001b[0m EXPECTED \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     21\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mS.No\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEntity Name\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAccounting Date\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mApproval Type\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDocument Type\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvoice Date\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDay\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNature\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     22\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAccount Code\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPL BS\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReport Group\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAccount Name\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNature In Balance Sheet\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDocument Number\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mJe Line Num\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     25\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel Score\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCP Score\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTriggered C Ps\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mResults Explanation Summary\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     26\u001b[0m ]\n",
      "File \u001b[1;32m~\\Audit-Risk-V2\\app\\model_logic.py:712\u001b[0m, in \u001b[0;36mrun_full_pipeline\u001b[1;34m(file_path)\u001b[0m\n\u001b[0;32m    709\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLow\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    711\u001b[0m test_df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel Classification\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m test_df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFinal_Score\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(classify_model_score)\n\u001b[1;32m--> 712\u001b[0m test_df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCP Classification\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m test_df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCP_Score\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(classify_cp_score)\n\u001b[0;32m    713\u001b[0m test_df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFinal Risk Classification\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m test_df\u001b[38;5;241m.\u001b[39mapply(\n\u001b[0;32m    714\u001b[0m     \u001b[38;5;28;01mlambda\u001b[39;00m row: determine_final_risk(row[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel Classification\u001b[39m\u001b[38;5;124m\"\u001b[39m], row[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCP Classification\u001b[39m\u001b[38;5;124m\"\u001b[39m]), axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    715\u001b[0m )\n\u001b[0;32m    717\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mget_cp_explanation\u001b[39m(cp_code, row):\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\frame.py:4102\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   4100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   4101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 4102\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mget_loc(key)\n\u001b[0;32m   4103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   4104\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3807\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[0;32m   3808\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[0;32m   3809\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[0;32m   3810\u001b[0m     ):\n\u001b[0;32m   3811\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[1;32m-> 3812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3814\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3815\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3816\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3817\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'CP_Score'"
     ]
    }
   ],
   "source": [
    "# Save pipeline results to an Excel file (schema locked to old 33 columns)\n",
    "\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "\n",
    "# 1) Import your updated pipeline\n",
    "try:\n",
    "    from app.model_logic import run_full_pipeline\n",
    "except ImportError:\n",
    "    from model_logic import run_full_pipeline  # if file is in the same folder as notebook\n",
    "\n",
    "# 2) Point to your input file (CSV or Excel both OK)\n",
    "INPUT_PATH = r\"test1 (3).csv\"   # <-- change to your file path if needed\n",
    "\n",
    "# 3) Run\n",
    "df = run_full_pipeline(INPUT_PATH)\n",
    "\n",
    "# 4) (Optional, safety) verify schema is the expected 33 columns\n",
    "EXPECTED = [\n",
    "    \"S.No\",\"Entity Name\",\"Accounting Date\",\"Approval Type\",\"Document Type\",\"Invoice Date\",\"Day\",\"Nature\",\n",
    "    \"Account Code\",\"PL BS\",\"Report Group\",\"Account Name\",\"Nature In Balance Sheet\",\"Document Number\",\"Je Line Num\",\n",
    "    \"Source Number\",\"Source Name\",\"Source Desc\",\"Line Desc\",\"Project Code\",\"Internal Reference\",\"Posted Date\",\"Branch\",\n",
    "    \"Batch Name\",\"Entered Dr SUM\",\"Entered Cr SUM\",\"Accounted Dr SUM\",\"Accounted Cr SUM\",\"Net Amount\",\n",
    "    \"Model Score\",\"CP Score\",\"Triggered C Ps\",\"Results Explanation Summary\"\n",
    "]\n",
    "assert list(df.columns) == EXPECTED, f\"Schema mismatch.\\nGot: {list(df.columns)}\"\n",
    "\n",
    "# 5) Prepare output path\n",
    "out_dir = Path(\"outputs\")\n",
    "out_dir.mkdir(parents=True, exist_ok=True)\n",
    "base = Path(INPUT_PATH).stem\n",
    "ts = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "out_path = out_dir / f\"{base}__risk_results__{ts}.xlsx\"\n",
    "\n",
    "# 6) Choose an Excel engine you have installed\n",
    "try:\n",
    "    import openpyxl  # noqa: F401\n",
    "    engine = \"openpyxl\"\n",
    "except Exception:\n",
    "    try:\n",
    "        import xlsxwriter  # noqa: F401\n",
    "        engine = \"xlsxwriter\"\n",
    "    except Exception as e:\n",
    "        raise ImportError(\"Install 'openpyxl' or 'xlsxwriter' to write Excel files.\") from e\n",
    "\n",
    "# 7) Write Results + Meta\n",
    "with pd.ExcelWriter(out_path, engine=engine) as xls:\n",
    "    df.to_excel(xls, sheet_name=\"Results\", index=False)\n",
    "    meta = pd.DataFrame({\n",
    "        \"Key\": [\"GeneratedAt\", \"InputFile\", \"Rows\", \"Columns\"],\n",
    "        \"Value\": [\n",
    "            datetime.now().isoformat(timespec=\"seconds\"),\n",
    "            str(Path(INPUT_PATH).resolve()),\n",
    "            len(df),\n",
    "            \", \".join(df.columns)\n",
    "        ]\n",
    "    })\n",
    "    meta.to_excel(xls, sheet_name=\"Meta\", index=False)\n",
    "\n",
    "print(f\"✅ Saved Excel to: {out_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "384fa245-c434-4242-b3d0-8606f36730cb",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"The input data does not contain all features expected by the model.\\nMissing features: ['Accounting_Month', 'Accounting_Weekday', 'Invoice_Month', 'Invoice_Weekday', 'Posted_Month', 'Posted_Weekday']\\nEnsure your preprocessing produces the same feature names used in training.\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 15\u001b[0m\n\u001b[0;32m     12\u001b[0m input_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mold_test_file.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m   \u001b[38;5;66;03m# change to .xlsx if you want the Excel variant\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# Run the pipeline\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m df_out \u001b[38;5;241m=\u001b[39m run_full_pipeline(input_path)\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# Show the first few rows\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28mprint\u001b[39m(df_out\u001b[38;5;241m.\u001b[39mhead())\n",
      "File \u001b[1;32m~\\Audit-Risk-V2\\model_logic.py:781\u001b[0m, in \u001b[0;36mrun_full_pipeline\u001b[1;34m(file_path)\u001b[0m\n\u001b[0;32m    778\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m j, name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(expected_text_feats):\n\u001b[0;32m    779\u001b[0m         X_full[name] \u001b[38;5;241m=\u001b[39m emb_use[:, j]\n\u001b[1;32m--> 781\u001b[0m X \u001b[38;5;241m=\u001b[39m scorer\u001b[38;5;241m.\u001b[39mensure_features(X_full)\n\u001b[0;32m    783\u001b[0m proba \u001b[38;5;241m=\u001b[39m scorer\u001b[38;5;241m.\u001b[39mpredict_proba(X)\n\u001b[0;32m    784\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m proba\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m proba\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m:\n",
      "File \u001b[1;32m~\\Audit-Risk-V2\\model_logic.py:312\u001b[0m, in \u001b[0;36mCatBoostScorer.ensure_features\u001b[1;34m(self, df)\u001b[0m\n\u001b[0;32m    310\u001b[0m     missing \u001b[38;5;241m=\u001b[39m [c \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeature_names \u001b[38;5;28;01mif\u001b[39;00m c \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m df\u001b[38;5;241m.\u001b[39mcolumns]\n\u001b[0;32m    311\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m missing:\n\u001b[1;32m--> 312\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\n\u001b[0;32m    313\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe input data does not contain all features expected by the model.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    314\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing features: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmissing\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    315\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEnsure your preprocessing produces the same feature names used in training.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    316\u001b[0m         )\n\u001b[0;32m    317\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m df[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeature_names]\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[0;32m    319\u001b[0m \u001b[38;5;66;03m# If model doesn't expose names, attempt a heuristic: drop legacy output fields and meta-only columns.\u001b[39;00m\n",
      "\u001b[1;31mKeyError\u001b[0m: \"The input data does not contain all features expected by the model.\\nMissing features: ['Accounting_Month', 'Accounting_Weekday', 'Invoice_Month', 'Invoice_Weekday', 'Posted_Month', 'Posted_Weekday']\\nEnsure your preprocessing produces the same feature names used in training.\""
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import sys\n",
    "\n",
    "# If model_logic.py is in the same folder as your notebook, this is enough.\n",
    "# Otherwise adjust the path (e.g., \"../app/model_logic.py\").\n",
    "sys.path.append(\".\")\n",
    "\n",
    "from model_logic import run_full_pipeline\n",
    "\n",
    "# Path to your old test file (CSV or Excel).\n",
    "# You uploaded \"old_test_file.csv\", so I'll assume CSV here.\n",
    "input_path = \"old_test_file.csv\"   # change to .xlsx if you want the Excel variant\n",
    "\n",
    "# Run the pipeline\n",
    "df_out = run_full_pipeline(input_path)\n",
    "\n",
    "# Show the first few rows\n",
    "print(df_out.head())\n",
    "\n",
    "# Confirm column order is exactly as expected (33 columns)\n",
    "print(\"\\nNumber of columns:\", len(df_out.columns))\n",
    "print(\"Columns in order:\")\n",
    "print(list(df_out.columns))\n",
    "\n",
    "# Save to Excel so you can inspect the full output easily\n",
    "df_out.to_excel(\"old_test_file_output.xlsx\", index=False)\n",
    "print(\"\\n✅ Output written to 'old_test_file_output.xlsx'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "345ae6b6-deeb-4bf0-893b-7e41b9543535",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (model_logic.py, line 815)",
     "output_type": "error",
     "traceback": [
      "Traceback \u001b[1;36m(most recent call last)\u001b[0m:\n",
      "\u001b[0m  File \u001b[0;32m~\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:3577\u001b[0m in \u001b[0;35mrun_code\u001b[0m\n    exec(code_obj, self.user_global_ns, self.user_ns)\u001b[0m\n",
      "\u001b[1;36m  Cell \u001b[1;32mIn[2], line 1\u001b[1;36m\n\u001b[1;33m    from model_logic import run_full_pipeline\u001b[1;36m\n",
      "\u001b[1;36m  File \u001b[1;32m~\\Audit-Risk-V2\\model_logic.py:815\u001b[1;36m\u001b[0m\n\u001b[1;33m    proba = scorer.predict_proba(X)\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "from model_logic import run_full_pipeline\n",
    "import pandas as pd\n",
    "\n",
    "# Use your old test file (you uploaded it as old_test_file.csv)\n",
    "input_path = \"old_test_file.csv\"  # adjust if it's in a subfolder\n",
    "\n",
    "# Run the pipeline\n",
    "df_out = run_full_pipeline(input_path)\n",
    "\n",
    "# Preview first few rows\n",
    "print(df_out.head())\n",
    "\n",
    "# Confirm the column schema\n",
    "print(\"\\nNumber of columns:\", len(df_out.columns))\n",
    "print(\"Columns in order:\")\n",
    "print(list(df_out.columns))\n",
    "\n",
    "# Save full output to Excel for manual inspection\n",
    "df_out.to_excel(\"old_test_file_output.xlsx\", index=False)\n",
    "print(\"\\n✅ Output written to 'old_test_file_output.xlsx'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "93b7ed4a-83ce-42cb-808a-cbea787b8a1a",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (model_logic.py, line 815)",
     "output_type": "error",
     "traceback": [
      "Traceback \u001b[1;36m(most recent call last)\u001b[0m:\n",
      "\u001b[0m  File \u001b[0;32m~\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:3577\u001b[0m in \u001b[0;35mrun_code\u001b[0m\n    exec(code_obj, self.user_global_ns, self.user_ns)\u001b[0m\n",
      "\u001b[1;36m  Cell \u001b[1;32mIn[3], line 1\u001b[1;36m\n\u001b[1;33m    from model_logic import run_full_pipeline\u001b[1;36m\n",
      "\u001b[1;36m  File \u001b[1;32m~\\Audit-Risk-V2\\model_logic.py:815\u001b[1;36m\u001b[0m\n\u001b[1;33m    proba = scorer.predict_proba(X)\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "from model_logic import run_full_pipeline\n",
    "\n",
    "df_out = run_full_pipeline(\"old_test_file.csv\")\n",
    "print(df_out.head())\n",
    "print(\"✅ Columns:\", len(df_out.columns))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8413965f-2d0e-411e-b188-62333bbed053",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "TESTING ENHANCED MODEL LOGIC WITH EXPLANATION INTEGRATION\n",
      "======================================================================\n",
      "\n",
      "🔄 STEP 1: Testing with old test file...\n",
      "📁 Loading test file: old_test_file.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-20 11:15:09,136 - INFO - Use pytorch device_name: cpu\n",
      "2025-09-20 11:15:09,138 - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading CatBoost model...\n",
      "CatBoost model loaded\n",
      "Downloading SentenceTransformer...\n",
      "SentenceTransformer loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  File \"C:\\Users\\Acer\\anaconda3\\Lib\\site-packages\\joblib\\externals\\loky\\backend\\context.py\", line 257, in _count_physical_cores\n",
      "    cpu_info = subprocess.run(\n",
      "        \"wmic CPU Get NumberOfCores /Format:csv\".split(),\n",
      "        capture_output=True,\n",
      "        text=True,\n",
      "    )\n",
      "  File \"C:\\Users\\Acer\\anaconda3\\Lib\\subprocess.py\", line 554, in run\n",
      "    with Popen(*popenargs, **kwargs) as process:\n",
      "         ~~~~~^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Acer\\anaconda3\\Lib\\subprocess.py\", line 1039, in __init__\n",
      "    self._execute_child(args, executable, preexec_fn, close_fds,\n",
      "    ~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "                        pass_fds, cwd, env,\n",
      "                        ^^^^^^^^^^^^^^^^^^^\n",
      "    ...<5 lines>...\n",
      "                        gid, gids, uid, umask,\n",
      "                        ^^^^^^^^^^^^^^^^^^^^^^\n",
      "                        start_new_session, process_group)\n",
      "                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Acer\\anaconda3\\Lib\\subprocess.py\", line 1554, in _execute_child\n",
      "    hp, ht, pid, tid = _winapi.CreateProcess(executable, args,\n",
      "                       ~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^\n",
      "                             # no special security\n",
      "                             ^^^^^^^^^^^^^^^^^^^^^\n",
      "    ...<4 lines>...\n",
      "                             cwd,\n",
      "                             ^^^^\n",
      "                             startupinfo)\n",
      "                             ^^^^^^^^^^^^\n",
      "2025-09-20 11:15:57,652 - INFO - Initializing BERT Risk Explainer...\n",
      "2025-09-20 11:15:57,654 - INFO - Initialized with 9 Priority 1 patterns, 6 Priority 2 dimensions, and 8 Priority 3 clusters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing transactions with enhanced explanations...\n",
      "✅ Pipeline completed successfully!\n",
      "📊 Total transactions processed: 874\n",
      "✅ All required columns present\n",
      "\n",
      "🔄 STEP 2: Analyzing Risk Classifications...\n",
      "\n",
      "📈 Risk Classification Distribution:\n",
      "   Medium: 396 (45.3%)\n",
      "   High: 394 (45.1%)\n",
      "   Low: 84 (9.6%)\n",
      "\n",
      "🔍 High Risk Transactions: 394\n",
      "📝 Explanation Coverage: 394/394 (100.0%)\n",
      "\n",
      "🔄 STEP 3: Examining High Risk Explanations...\n",
      "\n",
      "📋 Sample High Risk Explanations (First 5 transactions):\n",
      "====================================================================================================\n",
      "\n",
      "🆔 Transaction 1:\n",
      "   Model Score: 0.042 | CP Score: 0.965\n",
      "   Classifications: Model=Low, CP=High\n",
      "   Net Amount: ₹-224,400\n",
      "   Account: Cash in Bank - CITIBANK INR 8007\n",
      "   Triggered CPs: CP_03 (78), CP_07 (84), CP_26 (0)\n",
      "\n",
      "   📝 EXPLANATION:\n",
      "      Duplicate Patterns - Transaction matches multiple duplicate detection criteria indicating potential data quality issues (CP_03)\n",
      "      Document Imbalance - Document entries do not balance to zero, indicating posting errors requiring correction (CP_07)\n",
      "      Multiple risk indicators: documentation and processing issues detected\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "🆔 Transaction 2:\n",
      "   Model Score: 0.045 | CP Score: 0.965\n",
      "   Classifications: Model=Low, CP=High\n",
      "   Net Amount: ₹-5,187,574\n",
      "   Account: Cash in Bank - CITIBANK INR 8007\n",
      "   Triggered CPs: CP_03 (78), CP_07 (84), CP_26 (0)\n",
      "\n",
      "   📝 EXPLANATION:\n",
      "      Duplicate Patterns - Transaction matches multiple duplicate detection criteria indicating potential data quality issues (CP_03)\n",
      "      Document Imbalance - Document entries do not balance to zero, indicating posting errors requiring correction (CP_07)\n",
      "      Multiple risk indicators: documentation and processing issues detected\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "🆔 Transaction 3:\n",
      "   Model Score: 0.024 | CP Score: 0.989\n",
      "   Classifications: Model=Low, CP=High\n",
      "   Net Amount: ₹5,000,000\n",
      "   Account: Cash in Bank - CITIBANK INR 8007\n",
      "   Triggered CPs: CP_07 (84), CP_21 (69), CP_24 (78)\n",
      "\n",
      "   📝 EXPLANATION:\n",
      "      Document Imbalance - Document entries do not balance to zero, indicating posting errors requiring correction (CP_07)\n",
      "      Period-End Timing - Transaction occurs on month-end date when period-end adjustments are commonly made (CP_21)\n",
      "      Multiple risk indicators: documentation and processing issues detected\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "🆔 Transaction 4:\n",
      "   Model Score: 0.924 | CP Score: 0.989\n",
      "   Classifications: Model=Medium, CP=High\n",
      "   Net Amount: ₹74,000,000\n",
      "   Account: Cash in Bank - CITIBANK INR 8007\n",
      "   Triggered CPs: CP_07 (84), CP_21 (69), CP_24 (78)\n",
      "\n",
      "   📝 EXPLANATION:\n",
      "      Document Imbalance - Document entries do not balance to zero, indicating posting errors requiring correction (CP_07)\n",
      "      Period-End Timing - Transaction occurs on month-end date when period-end adjustments are commonly made (CP_21)\n",
      "      Unusual Amount Pattern - Transaction amount ending in 000 follows rare sequential or repetitive digit patterns (CP_24)\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "🆔 Transaction 5:\n",
      "   Model Score: 0.034 | CP Score: 0.989\n",
      "   Classifications: Model=Low, CP=High\n",
      "   Net Amount: ₹-1,000\n",
      "   Account: Cash in Bank - CITIBANK INR 8007\n",
      "   Triggered CPs: CP_07 (84), CP_21 (69), CP_24 (78)\n",
      "\n",
      "   📝 EXPLANATION:\n",
      "      Document Imbalance - Document entries do not balance to zero, indicating posting errors requiring correction (CP_07)\n",
      "      Period-End Timing - Transaction occurs on month-end date when period-end adjustments are commonly made (CP_21)\n",
      "      Multiple risk indicators: documentation and processing issues detected\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "🔄 STEP 4: Analyzing Risk Drivers...\n",
      "\n",
      "🎯 Risk Driver Analysis:\n",
      "   Model-driven High Risk: 0\n",
      "   CP-driven High Risk: 394\n",
      "   Both Model & CP High: 0\n",
      "\n",
      "📊 Explanation Type Distribution:\n",
      "   Model-only explanations: 0\n",
      "   CP-only explanations: 394\n",
      "   Combined explanations: 0\n",
      "\n",
      "🔄 STEP 5: Verifying Output Schema...\n",
      "\n",
      "📋 Output Schema Verification:\n",
      "   Total columns: 68\n",
      "   Explanation-related columns: ['Explanation_Summary', 'Top_Risky_Feature_Groups']\n",
      "\n",
      "🔍 Key Column Data Types:\n",
      "   Final_Score: float64 (874/874 non-null)\n",
      "   CP_Score: float64 (874/874 non-null)\n",
      "   Final Risk Classification: object (874/874 non-null)\n",
      "   Explanation_Summary: object (874/874 non-null)\n",
      "\n",
      "💾 Sample output saved to: sample_enhanced_output.xlsx\n",
      "\n",
      "🔄 STEP 6: Testing with Enhanced Test File (if available)...\n",
      "⚠️  Enhanced test file not found: Enhanced Test File.xlsx\n",
      "   Skipping enhanced file test...\n",
      "\n",
      "======================================================================\n",
      "🏁 TESTING SUMMARY\n",
      "======================================================================\n",
      "✅ Pipeline Status: Completed Successfully\n",
      "📊 Total Transactions: 874\n",
      "🔴 High Risk Count: 394\n",
      "🟡 Medium Risk Count: 396\n",
      "🟢 Low Risk Count: 84\n",
      "📝 Explanation Coverage: 100.0% of high risk transactions\n",
      "\n",
      "🎯 Next Steps:\n",
      "   1. Review sample_enhanced_output.xlsx for quality check\n",
      "   2. Test the API endpoint with main.py\n",
      "   3. Deploy to Render when satisfied with results\n",
      "   4. Update n8n workflow if needed\n",
      "\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# === JUPYTER TESTING CODE FOR ENHANCED MODEL LOGIC ===\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the app directory to Python path\n",
    "sys.path.append('./app')\n",
    "\n",
    "# Import your enhanced model logic\n",
    "from model_logic import run_full_pipeline\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"TESTING ENHANCED MODEL LOGIC WITH EXPLANATION INTEGRATION\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# === STEP 1: Test with your old test file ===\n",
    "print(\"\\n🔄 STEP 1: Testing with old test file...\")\n",
    "try:\n",
    "    # Use your old test file\n",
    "    old_test_file = \"old_test_file.csv\"  # Adjust path if needed\n",
    "    \n",
    "    print(f\"📁 Loading test file: {old_test_file}\")\n",
    "    result_df = run_full_pipeline(old_test_file)\n",
    "    \n",
    "    print(f\"✅ Pipeline completed successfully!\")\n",
    "    print(f\"📊 Total transactions processed: {len(result_df):,}\")\n",
    "    \n",
    "    # Check key columns exist\n",
    "    required_columns = [\n",
    "        'Final_Score', 'CP_Score', 'Final Risk Classification',\n",
    "        'Model Classification', 'CP Classification', \n",
    "        'Explanation_Summary', 'Top_Risky_Feature_Groups'\n",
    "    ]\n",
    "    \n",
    "    missing_cols = [col for col in required_columns if col not in result_df.columns]\n",
    "    if missing_cols:\n",
    "        print(f\"❌ Missing columns: {missing_cols}\")\n",
    "    else:\n",
    "        print(\"✅ All required columns present\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"❌ Error in STEP 1: {str(e)}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "\n",
    "# === STEP 2: Analyze Risk Classifications ===\n",
    "print(\"\\n🔄 STEP 2: Analyzing Risk Classifications...\")\n",
    "try:\n",
    "    risk_summary = result_df['Final Risk Classification'].value_counts()\n",
    "    print(\"\\n📈 Risk Classification Distribution:\")\n",
    "    for risk_level, count in risk_summary.items():\n",
    "        percentage = (count / len(result_df)) * 100\n",
    "        print(f\"   {risk_level}: {count:,} ({percentage:.1f}%)\")\n",
    "    \n",
    "    # Focus on High Risk transactions\n",
    "    high_risk_df = result_df[result_df['Final Risk Classification'] == 'High'].copy()\n",
    "    print(f\"\\n🔍 High Risk Transactions: {len(high_risk_df):,}\")\n",
    "    \n",
    "    if len(high_risk_df) > 0:\n",
    "        # Check explanation coverage\n",
    "        explained_count = sum(1 for exp in high_risk_df['Explanation_Summary'] if str(exp).strip() != '')\n",
    "        explanation_coverage = (explained_count / len(high_risk_df)) * 100\n",
    "        print(f\"📝 Explanation Coverage: {explained_count:,}/{len(high_risk_df):,} ({explanation_coverage:.1f}%)\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Error in STEP 2: {str(e)}\")\n",
    "\n",
    "# === STEP 3: Sample High Risk Explanations ===\n",
    "print(\"\\n🔄 STEP 3: Examining High Risk Explanations...\")\n",
    "try:\n",
    "    if len(high_risk_df) > 0:\n",
    "        # Get first 5 high risk transactions with explanations\n",
    "        sample_high_risk = high_risk_df[high_risk_df['Explanation_Summary'].str.strip() != ''].head(5)\n",
    "        \n",
    "        print(f\"\\n📋 Sample High Risk Explanations (First {len(sample_high_risk)} transactions):\")\n",
    "        print(\"=\" * 100)\n",
    "        \n",
    "        for idx, row in sample_high_risk.iterrows():\n",
    "            print(f\"\\n🆔 Transaction {idx + 1}:\")\n",
    "            print(f\"   Model Score: {row['Final_Score']:.3f} | CP Score: {row['CP_Score']:.3f}\")\n",
    "            print(f\"   Classifications: Model={row['Model Classification']}, CP={row['CP Classification']}\")\n",
    "            print(f\"   Net Amount: ₹{row.get('Net Amount', 0):,.0f}\")\n",
    "            print(f\"   Account: {row.get('Account Name', 'N/A')}\")\n",
    "            print(f\"   Triggered CPs: {row.get('Triggered_CPs', 'None')}\")\n",
    "            print(f\"\\n   📝 EXPLANATION:\")\n",
    "            explanation = str(row['Explanation_Summary']).strip()\n",
    "            if explanation:\n",
    "                # Split by newlines and indent each line\n",
    "                for line in explanation.split('\\n'):\n",
    "                    if line.strip():\n",
    "                        print(f\"      {line.strip()}\")\n",
    "            else:\n",
    "                print(\"      No explanation provided\")\n",
    "            print(\"-\" * 80)\n",
    "    else:\n",
    "        print(\"   No high risk transactions found to display explanations\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"❌ Error in STEP 3: {str(e)}\")\n",
    "\n",
    "# === STEP 4: Compare Model vs CP Driven Risks ===\n",
    "print(\"\\n🔄 STEP 4: Analyzing Risk Drivers...\")\n",
    "try:\n",
    "    if len(high_risk_df) > 0:\n",
    "        model_high = len(high_risk_df[high_risk_df['Model Classification'] == 'High'])\n",
    "        cp_high = len(high_risk_df[high_risk_df['CP Classification'] == 'High'])\n",
    "        both_high = len(high_risk_df[(high_risk_df['Model Classification'] == 'High') & \n",
    "                                    (high_risk_df['CP Classification'] == 'High')])\n",
    "        \n",
    "        print(f\"\\n🎯 Risk Driver Analysis:\")\n",
    "        print(f\"   Model-driven High Risk: {model_high:,}\")\n",
    "        print(f\"   CP-driven High Risk: {cp_high:,}\")\n",
    "        print(f\"   Both Model & CP High: {both_high:,}\")\n",
    "        \n",
    "        # Show breakdown of explanation types\n",
    "        model_only = high_risk_df[(high_risk_df['Model Classification'] == 'High') & \n",
    "                                 (high_risk_df['CP Classification'] != 'High')]\n",
    "        cp_only = high_risk_df[(high_risk_df['Model Classification'] != 'High') & \n",
    "                              (high_risk_df['CP Classification'] == 'High')]\n",
    "        \n",
    "        print(f\"\\n📊 Explanation Type Distribution:\")\n",
    "        print(f\"   Model-only explanations: {len(model_only):,}\")\n",
    "        print(f\"   CP-only explanations: {len(cp_only):,}\")\n",
    "        print(f\"   Combined explanations: {both_high:,}\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"❌ Error in STEP 4: {str(e)}\")\n",
    "\n",
    "# === STEP 5: Verify Column Schema Compatibility ===\n",
    "print(\"\\n🔄 STEP 5: Verifying Output Schema...\")\n",
    "try:\n",
    "    print(f\"\\n📋 Output Schema Verification:\")\n",
    "    print(f\"   Total columns: {len(result_df.columns)}\")\n",
    "    \n",
    "    # Check for key explanation columns\n",
    "    explanation_cols = [col for col in result_df.columns if 'explanation' in col.lower() or 'risky' in col.lower()]\n",
    "    print(f\"   Explanation-related columns: {explanation_cols}\")\n",
    "    \n",
    "    # Check data types\n",
    "    print(f\"\\n🔍 Key Column Data Types:\")\n",
    "    key_columns = ['Final_Score', 'CP_Score', 'Final Risk Classification', 'Explanation_Summary']\n",
    "    for col in key_columns:\n",
    "        if col in result_df.columns:\n",
    "            dtype = result_df[col].dtype\n",
    "            non_null = result_df[col].notna().sum()\n",
    "            print(f\"   {col}: {dtype} ({non_null:,}/{len(result_df):,} non-null)\")\n",
    "    \n",
    "    # Export sample for manual review\n",
    "    sample_output = result_df.head(100)\n",
    "    sample_filename = \"sample_enhanced_output.xlsx\"\n",
    "    sample_output.to_excel(sample_filename, index=False)\n",
    "    print(f\"\\n💾 Sample output saved to: {sample_filename}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Error in STEP 5: {str(e)}\")\n",
    "\n",
    "# === STEP 6: Test with Enhanced Test File (if available) ===\n",
    "print(\"\\n🔄 STEP 6: Testing with Enhanced Test File (if available)...\")\n",
    "try:\n",
    "    enhanced_test_file = \"Enhanced Test File.xlsx\"  # Adjust path if needed\n",
    "    \n",
    "    if os.path.exists(enhanced_test_file):\n",
    "        print(f\"📁 Found enhanced test file: {enhanced_test_file}\")\n",
    "        result_enhanced = run_full_pipeline(enhanced_test_file)\n",
    "        \n",
    "        print(f\"✅ Enhanced file processed successfully!\")\n",
    "        print(f\"📊 Enhanced file transactions: {len(result_enhanced):,}\")\n",
    "        \n",
    "        # Compare results\n",
    "        enhanced_high_risk = len(result_enhanced[result_enhanced['Final Risk Classification'] == 'High'])\n",
    "        print(f\"🔍 Enhanced file high risk transactions: {enhanced_high_risk:,}\")\n",
    "        \n",
    "    else:\n",
    "        print(f\"⚠️  Enhanced test file not found: {enhanced_test_file}\")\n",
    "        print(\"   Skipping enhanced file test...\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"❌ Error in STEP 6: {str(e)}\")\n",
    "\n",
    "# === FINAL SUMMARY ===\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"🏁 TESTING SUMMARY\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "try:\n",
    "    print(f\"✅ Pipeline Status: Completed Successfully\")\n",
    "    print(f\"📊 Total Transactions: {len(result_df):,}\")\n",
    "    print(f\"🔴 High Risk Count: {len(result_df[result_df['Final Risk Classification'] == 'High']):,}\")\n",
    "    print(f\"🟡 Medium Risk Count: {len(result_df[result_df['Final Risk Classification'] == 'Medium']):,}\")\n",
    "    print(f\"🟢 Low Risk Count: {len(result_df[result_df['Final Risk Classification'] == 'Low']):,}\")\n",
    "    \n",
    "    # Check explanation quality\n",
    "    high_risk_with_explanations = len(result_df[\n",
    "        (result_df['Final Risk Classification'] == 'High') & \n",
    "        (result_df['Explanation_Summary'].str.strip() != '')\n",
    "    ])\n",
    "    \n",
    "    total_high_risk = len(result_df[result_df['Final Risk Classification'] == 'High'])\n",
    "    if total_high_risk > 0:\n",
    "        explanation_rate = (high_risk_with_explanations / total_high_risk) * 100\n",
    "        print(f\"📝 Explanation Coverage: {explanation_rate:.1f}% of high risk transactions\")\n",
    "    \n",
    "    print(f\"\\n🎯 Next Steps:\")\n",
    "    print(f\"   1. Review sample_enhanced_output.xlsx for quality check\")\n",
    "    print(f\"   2. Test the API endpoint with main.py\")\n",
    "    print(f\"   3. Deploy to Render when satisfied with results\")\n",
    "    print(f\"   4. Update n8n workflow if needed\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Error in final summary: {str(e)}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a4d3dbd8-4b46-4bf2-84a6-f8e5a2e2415c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-20 11:27:36,861 - INFO - Use pytorch device_name: cpu\n",
      "2025-09-20 11:27:36,863 - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "TESTING ENHANCED MODEL LOGIC WITH EXPLANATION INTEGRATION\n",
      "======================================================================\n",
      "\n",
      "🔄 STEP 1: Testing with old test file...\n",
      "📁 Loading test file: old_test_file.csv\n",
      "Loading CatBoost model...\n",
      "CatBoost model loaded\n",
      "Downloading SentenceTransformer...\n",
      "SentenceTransformer loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-20 11:28:00,315 - INFO - Initializing BERT Risk Explainer...\n",
      "2025-09-20 11:28:00,317 - INFO - Initialized with 9 Priority 1 patterns, 6 Priority 2 dimensions, and 8 Priority 3 clusters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing transactions with enhanced explanations...\n",
      "✅ Pipeline completed successfully!\n",
      "📊 Total transactions processed: 874\n",
      "✅ All required columns present\n",
      "\n",
      "🔄 STEP 2: Analyzing Risk Classifications...\n",
      "\n",
      "📈 Risk Classification Distribution:\n",
      "   Medium: 396 (45.3%)\n",
      "   High: 394 (45.1%)\n",
      "   Low: 84 (9.6%)\n",
      "\n",
      "🔍 High Risk Transactions: 394\n",
      "📝 Explanation Coverage: 394/394 (100.0%)\n",
      "\n",
      "🔄 STEP 3: Examining High Risk Explanations...\n",
      "\n",
      "📋 Sample High Risk Explanations (First 5 transactions):\n",
      "====================================================================================================\n",
      "\n",
      "🆔 Transaction 1:\n",
      "   Model Score: 0.042 | CP Score: 0.965\n",
      "   Classifications: Model=Low, CP=High\n",
      "   Net Amount: ₹-224,400\n",
      "   Account: Cash in Bank - CITIBANK INR 8007\n",
      "   Triggered CPs: CP_03 (78), CP_07 (84), CP_26 (0)\n",
      "\n",
      "   📝 EXPLANATION:\n",
      "      Duplicate Patterns - Transaction matches multiple duplicate detection criteria indicating potential data quality issues (CP_03)\n",
      "      Document Imbalance - Document entries do not balance to zero, indicating posting errors requiring correction (CP_07)\n",
      "      Multiple risk indicators: documentation and processing issues detected\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "🆔 Transaction 2:\n",
      "   Model Score: 0.045 | CP Score: 0.965\n",
      "   Classifications: Model=Low, CP=High\n",
      "   Net Amount: ₹-5,187,574\n",
      "   Account: Cash in Bank - CITIBANK INR 8007\n",
      "   Triggered CPs: CP_03 (78), CP_07 (84), CP_26 (0)\n",
      "\n",
      "   📝 EXPLANATION:\n",
      "      Duplicate Patterns - Transaction matches multiple duplicate detection criteria indicating potential data quality issues (CP_03)\n",
      "      Document Imbalance - Document entries do not balance to zero, indicating posting errors requiring correction (CP_07)\n",
      "      Multiple risk indicators: documentation and processing issues detected\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "🆔 Transaction 3:\n",
      "   Model Score: 0.024 | CP Score: 0.989\n",
      "   Classifications: Model=Low, CP=High\n",
      "   Net Amount: ₹5,000,000\n",
      "   Account: Cash in Bank - CITIBANK INR 8007\n",
      "   Triggered CPs: CP_07 (84), CP_21 (69), CP_24 (78)\n",
      "\n",
      "   📝 EXPLANATION:\n",
      "      Document Imbalance - Document entries do not balance to zero, indicating posting errors requiring correction (CP_07)\n",
      "      Period-End Timing - Transaction occurs on month-end date when period-end adjustments are commonly made (CP_21)\n",
      "      Multiple risk indicators: documentation and processing issues detected\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "🆔 Transaction 4:\n",
      "   Model Score: 0.924 | CP Score: 0.989\n",
      "   Classifications: Model=Medium, CP=High\n",
      "   Net Amount: ₹74,000,000\n",
      "   Account: Cash in Bank - CITIBANK INR 8007\n",
      "   Triggered CPs: CP_07 (84), CP_21 (69), CP_24 (78)\n",
      "\n",
      "   📝 EXPLANATION:\n",
      "      Document Imbalance - Document entries do not balance to zero, indicating posting errors requiring correction (CP_07)\n",
      "      Period-End Timing - Transaction occurs on month-end date when period-end adjustments are commonly made (CP_21)\n",
      "      Unusual Amount Pattern - Transaction amount ending in 000 follows rare sequential or repetitive digit patterns (CP_24)\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "🆔 Transaction 5:\n",
      "   Model Score: 0.034 | CP Score: 0.989\n",
      "   Classifications: Model=Low, CP=High\n",
      "   Net Amount: ₹-1,000\n",
      "   Account: Cash in Bank - CITIBANK INR 8007\n",
      "   Triggered CPs: CP_07 (84), CP_21 (69), CP_24 (78)\n",
      "\n",
      "   📝 EXPLANATION:\n",
      "      Document Imbalance - Document entries do not balance to zero, indicating posting errors requiring correction (CP_07)\n",
      "      Period-End Timing - Transaction occurs on month-end date when period-end adjustments are commonly made (CP_21)\n",
      "      Multiple risk indicators: documentation and processing issues detected\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "🔄 STEP 4: Analyzing Risk Drivers...\n",
      "\n",
      "🎯 Risk Driver Analysis:\n",
      "   Model-driven High Risk: 0\n",
      "   CP-driven High Risk: 394\n",
      "   Both Model & CP High: 0\n",
      "\n",
      "📊 Explanation Type Distribution:\n",
      "   Model-only explanations: 0\n",
      "   CP-only explanations: 394\n",
      "   Combined explanations: 0\n",
      "\n",
      "🔄 STEP 5: Verifying Output Schema...\n",
      "\n",
      "📋 Output Schema Verification:\n",
      "   Total columns: 68\n",
      "   Explanation-related columns: ['Explanation_Summary', 'Top_Risky_Feature_Groups']\n",
      "\n",
      "🔍 Key Column Data Types:\n",
      "   Final_Score: float64 (874/874 non-null)\n",
      "   CP_Score: float64 (874/874 non-null)\n",
      "   Final Risk Classification: object (874/874 non-null)\n",
      "   Explanation_Summary: object (874/874 non-null)\n",
      "\n",
      "💾 Sample output saved to: sample_enhanced_output.xlsx\n",
      "\n",
      "🔄 STEP 6: Testing with Enhanced Test File (if available)...\n",
      "⚠️  Enhanced test file not found: Enhanced Test File.xlsx\n",
      "   Skipping enhanced file test...\n",
      "\n",
      "======================================================================\n",
      "🏁 TESTING SUMMARY\n",
      "======================================================================\n",
      "✅ Pipeline Status: Completed Successfully\n",
      "📊 Total Transactions: 874\n",
      "🔴 High Risk Count: 394\n",
      "🟡 Medium Risk Count: 396\n",
      "🟢 Low Risk Count: 84\n",
      "📝 Explanation Coverage: 100.0% of high risk transactions\n",
      "\n",
      "🎯 Next Steps:\n",
      "   1. Review sample_enhanced_output.xlsx for quality check\n",
      "   2. Test the API endpoint with main.py\n",
      "   3. Deploy to Render when satisfied with results\n",
      "   4. Update n8n workflow if needed\n",
      "\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# === JUPYTER TESTING CODE FOR ENHANCED MODEL LOGIC ===\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the app directory to Python path\n",
    "sys.path.append('./app')\n",
    "\n",
    "# Import your enhanced model logic\n",
    "from model_logic import run_full_pipeline\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"TESTING ENHANCED MODEL LOGIC WITH EXPLANATION INTEGRATION\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# === STEP 1: Test with your old test file ===\n",
    "print(\"\\n🔄 STEP 1: Testing with old test file...\")\n",
    "try:\n",
    "    # Use your old test file\n",
    "    old_test_file = \"old_test_file.csv\"  # Adjust path if needed\n",
    "    \n",
    "    print(f\"📁 Loading test file: {old_test_file}\")\n",
    "    result_df = run_full_pipeline(old_test_file)\n",
    "    \n",
    "    print(f\"✅ Pipeline completed successfully!\")\n",
    "    print(f\"📊 Total transactions processed: {len(result_df):,}\")\n",
    "    \n",
    "    # Check key columns exist\n",
    "    required_columns = [\n",
    "        'Final_Score', 'CP_Score', 'Final Risk Classification',\n",
    "        'Model Classification', 'CP Classification', \n",
    "        'Explanation_Summary', 'Top_Risky_Feature_Groups'\n",
    "    ]\n",
    "    \n",
    "    missing_cols = [col for col in required_columns if col not in result_df.columns]\n",
    "    if missing_cols:\n",
    "        print(f\"❌ Missing columns: {missing_cols}\")\n",
    "    else:\n",
    "        print(\"✅ All required columns present\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"❌ Error in STEP 1: {str(e)}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "\n",
    "# === STEP 2: Analyze Risk Classifications ===\n",
    "print(\"\\n🔄 STEP 2: Analyzing Risk Classifications...\")\n",
    "try:\n",
    "    risk_summary = result_df['Final Risk Classification'].value_counts()\n",
    "    print(\"\\n📈 Risk Classification Distribution:\")\n",
    "    for risk_level, count in risk_summary.items():\n",
    "        percentage = (count / len(result_df)) * 100\n",
    "        print(f\"   {risk_level}: {count:,} ({percentage:.1f}%)\")\n",
    "    \n",
    "    # Focus on High Risk transactions\n",
    "    high_risk_df = result_df[result_df['Final Risk Classification'] == 'High'].copy()\n",
    "    print(f\"\\n🔍 High Risk Transactions: {len(high_risk_df):,}\")\n",
    "    \n",
    "    if len(high_risk_df) > 0:\n",
    "        # Check explanation coverage\n",
    "        explained_count = sum(1 for exp in high_risk_df['Explanation_Summary'] if str(exp).strip() != '')\n",
    "        explanation_coverage = (explained_count / len(high_risk_df)) * 100\n",
    "        print(f\"📝 Explanation Coverage: {explained_count:,}/{len(high_risk_df):,} ({explanation_coverage:.1f}%)\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Error in STEP 2: {str(e)}\")\n",
    "\n",
    "# === STEP 3: Sample High Risk Explanations ===\n",
    "print(\"\\n🔄 STEP 3: Examining High Risk Explanations...\")\n",
    "try:\n",
    "    if len(high_risk_df) > 0:\n",
    "        # Get first 5 high risk transactions with explanations\n",
    "        sample_high_risk = high_risk_df[high_risk_df['Explanation_Summary'].str.strip() != ''].head(5)\n",
    "        \n",
    "        print(f\"\\n📋 Sample High Risk Explanations (First {len(sample_high_risk)} transactions):\")\n",
    "        print(\"=\" * 100)\n",
    "        \n",
    "        for idx, row in sample_high_risk.iterrows():\n",
    "            print(f\"\\n🆔 Transaction {idx + 1}:\")\n",
    "            print(f\"   Model Score: {row['Final_Score']:.3f} | CP Score: {row['CP_Score']:.3f}\")\n",
    "            print(f\"   Classifications: Model={row['Model Classification']}, CP={row['CP Classification']}\")\n",
    "            print(f\"   Net Amount: ₹{row.get('Net Amount', 0):,.0f}\")\n",
    "            print(f\"   Account: {row.get('Account Name', 'N/A')}\")\n",
    "            print(f\"   Triggered CPs: {row.get('Triggered_CPs', 'None')}\")\n",
    "            print(f\"\\n   📝 EXPLANATION:\")\n",
    "            explanation = str(row['Explanation_Summary']).strip()\n",
    "            if explanation:\n",
    "                # Split by newlines and indent each line\n",
    "                for line in explanation.split('\\n'):\n",
    "                    if line.strip():\n",
    "                        print(f\"      {line.strip()}\")\n",
    "            else:\n",
    "                print(\"      No explanation provided\")\n",
    "            print(\"-\" * 80)\n",
    "    else:\n",
    "        print(\"   No high risk transactions found to display explanations\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"❌ Error in STEP 3: {str(e)}\")\n",
    "\n",
    "# === STEP 4: Compare Model vs CP Driven Risks ===\n",
    "print(\"\\n🔄 STEP 4: Analyzing Risk Drivers...\")\n",
    "try:\n",
    "    if len(high_risk_df) > 0:\n",
    "        model_high = len(high_risk_df[high_risk_df['Model Classification'] == 'High'])\n",
    "        cp_high = len(high_risk_df[high_risk_df['CP Classification'] == 'High'])\n",
    "        both_high = len(high_risk_df[(high_risk_df['Model Classification'] == 'High') & \n",
    "                                    (high_risk_df['CP Classification'] == 'High')])\n",
    "        \n",
    "        print(f\"\\n🎯 Risk Driver Analysis:\")\n",
    "        print(f\"   Model-driven High Risk: {model_high:,}\")\n",
    "        print(f\"   CP-driven High Risk: {cp_high:,}\")\n",
    "        print(f\"   Both Model & CP High: {both_high:,}\")\n",
    "        \n",
    "        # Show breakdown of explanation types\n",
    "        model_only = high_risk_df[(high_risk_df['Model Classification'] == 'High') & \n",
    "                                 (high_risk_df['CP Classification'] != 'High')]\n",
    "        cp_only = high_risk_df[(high_risk_df['Model Classification'] != 'High') & \n",
    "                              (high_risk_df['CP Classification'] == 'High')]\n",
    "        \n",
    "        print(f\"\\n📊 Explanation Type Distribution:\")\n",
    "        print(f\"   Model-only explanations: {len(model_only):,}\")\n",
    "        print(f\"   CP-only explanations: {len(cp_only):,}\")\n",
    "        print(f\"   Combined explanations: {both_high:,}\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"❌ Error in STEP 4: {str(e)}\")\n",
    "\n",
    "# === STEP 5: Verify Column Schema Compatibility ===\n",
    "print(\"\\n🔄 STEP 5: Verifying Output Schema...\")\n",
    "try:\n",
    "    print(f\"\\n📋 Output Schema Verification:\")\n",
    "    print(f\"   Total columns: {len(result_df.columns)}\")\n",
    "    \n",
    "    # Check for key explanation columns\n",
    "    explanation_cols = [col for col in result_df.columns if 'explanation' in col.lower() or 'risky' in col.lower()]\n",
    "    print(f\"   Explanation-related columns: {explanation_cols}\")\n",
    "    \n",
    "    # Check data types\n",
    "    print(f\"\\n🔍 Key Column Data Types:\")\n",
    "    key_columns = ['Final_Score', 'CP_Score', 'Final Risk Classification', 'Explanation_Summary']\n",
    "    for col in key_columns:\n",
    "        if col in result_df.columns:\n",
    "            dtype = result_df[col].dtype\n",
    "            non_null = result_df[col].notna().sum()\n",
    "            print(f\"   {col}: {dtype} ({non_null:,}/{len(result_df):,} non-null)\")\n",
    "    \n",
    "    # Export sample for manual review\n",
    "    sample_output = result_df.head(100)\n",
    "    sample_filename = \"sample_enhanced_output.xlsx\"\n",
    "    sample_output.to_excel(sample_filename, index=False)\n",
    "    print(f\"\\n💾 Sample output saved to: {sample_filename}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Error in STEP 5: {str(e)}\")\n",
    "\n",
    "# === STEP 6: Test with Enhanced Test File (if available) ===\n",
    "print(\"\\n🔄 STEP 6: Testing with Enhanced Test File (if available)...\")\n",
    "try:\n",
    "    enhanced_test_file = \"Enhanced Test File.xlsx\"  # Adjust path if needed\n",
    "    \n",
    "    if os.path.exists(enhanced_test_file):\n",
    "        print(f\"📁 Found enhanced test file: {enhanced_test_file}\")\n",
    "        result_enhanced = run_full_pipeline(enhanced_test_file)\n",
    "        \n",
    "        print(f\"✅ Enhanced file processed successfully!\")\n",
    "        print(f\"📊 Enhanced file transactions: {len(result_enhanced):,}\")\n",
    "        \n",
    "        # Compare results\n",
    "        enhanced_high_risk = len(result_enhanced[result_enhanced['Final Risk Classification'] == 'High'])\n",
    "        print(f\"🔍 Enhanced file high risk transactions: {enhanced_high_risk:,}\")\n",
    "        \n",
    "    else:\n",
    "        print(f\"⚠️  Enhanced test file not found: {enhanced_test_file}\")\n",
    "        print(\"   Skipping enhanced file test...\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"❌ Error in STEP 6: {str(e)}\")\n",
    "\n",
    "# === FINAL SUMMARY ===\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"🏁 TESTING SUMMARY\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "try:\n",
    "    print(f\"✅ Pipeline Status: Completed Successfully\")\n",
    "    print(f\"📊 Total Transactions: {len(result_df):,}\")\n",
    "    print(f\"🔴 High Risk Count: {len(result_df[result_df['Final Risk Classification'] == 'High']):,}\")\n",
    "    print(f\"🟡 Medium Risk Count: {len(result_df[result_df['Final Risk Classification'] == 'Medium']):,}\")\n",
    "    print(f\"🟢 Low Risk Count: {len(result_df[result_df['Final Risk Classification'] == 'Low']):,}\")\n",
    "    \n",
    "    # Check explanation quality\n",
    "    high_risk_with_explanations = len(result_df[\n",
    "        (result_df['Final Risk Classification'] == 'High') & \n",
    "        (result_df['Explanation_Summary'].str.strip() != '')\n",
    "    ])\n",
    "    \n",
    "    total_high_risk = len(result_df[result_df['Final Risk Classification'] == 'High'])\n",
    "    if total_high_risk > 0:\n",
    "        explanation_rate = (high_risk_with_explanations / total_high_risk) * 100\n",
    "        print(f\"📝 Explanation Coverage: {explanation_rate:.1f}% of high risk transactions\")\n",
    "    \n",
    "    print(f\"\\n🎯 Next Steps:\")\n",
    "    print(f\"   1. Review sample_enhanced_output.xlsx for quality check\")\n",
    "    print(f\"   2. Test the API endpoint with main.py\")\n",
    "    print(f\"   3. Deploy to Render when satisfied with results\")\n",
    "    print(f\"   4. Update n8n workflow if needed\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Error in final summary: {str(e)}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "90f99b51-1ea9-4c2a-bd30-5dc5e7dc6f88",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-20 12:33:43,783 - INFO - Use pytorch device_name: cpu\n",
      "2025-09-20 12:33:43,784 - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "TESTING ENHANCED MODEL LOGIC WITH EXPLANATION INTEGRATION\n",
      "======================================================================\n",
      "\n",
      "🔄 STEP 1: Testing with old test file...\n",
      "📁 Loading test file: old_test_file.csv\n",
      "Loading CatBoost model...\n",
      "CatBoost model loaded\n",
      "Downloading SentenceTransformer...\n",
      "SentenceTransformer loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-20 12:34:02,085 - INFO - Initializing BERT Risk Explainer...\n",
      "2025-09-20 12:34:02,089 - INFO - Initialized with 9 Priority 1 patterns, 6 Priority 2 dimensions, and 8 Priority 3 clusters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing transactions with enhanced explanations...\n",
      "✅ Pipeline completed successfully!\n",
      "📊 Total transactions processed: 874\n",
      "✅ All required columns present\n",
      "\n",
      "🔄 STEP 2: Analyzing Risk Classifications...\n",
      "\n",
      "📈 Risk Classification Distribution:\n",
      "   Medium: 396 (45.3%)\n",
      "   High: 394 (45.1%)\n",
      "   Low: 84 (9.6%)\n",
      "\n",
      "🔍 High Risk Transactions: 394\n",
      "📝 Explanation Coverage: 394/394 (100.0%)\n",
      "\n",
      "🔄 STEP 3: Examining High Risk Explanations...\n",
      "\n",
      "📋 Sample High Risk Explanations (First 5 transactions):\n",
      "====================================================================================================\n",
      "\n",
      "🆔 Transaction 1:\n",
      "   Model Score: 0.042 | CP Score: 0.965\n",
      "   Classifications: Model=Low, CP=High\n",
      "   Net Amount: ₹-224,400\n",
      "   Account: Cash in Bank - CITIBANK INR 8007\n",
      "   Triggered CPs: CP_03 (78), CP_07 (84), CP_26 (0)\n",
      "\n",
      "   📝 EXPLANATION:\n",
      "      Duplicate Patterns - Transaction matches multiple duplicate detection criteria indicating potential data quality issues (CP_03)\n",
      "      Document Imbalance - Document entries do not balance to zero, indicating posting errors requiring correction (CP_07)\n",
      "      Multiple risk indicators: documentation and processing issues detected\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "🆔 Transaction 2:\n",
      "   Model Score: 0.045 | CP Score: 0.965\n",
      "   Classifications: Model=Low, CP=High\n",
      "   Net Amount: ₹-5,187,574\n",
      "   Account: Cash in Bank - CITIBANK INR 8007\n",
      "   Triggered CPs: CP_03 (78), CP_07 (84), CP_26 (0)\n",
      "\n",
      "   📝 EXPLANATION:\n",
      "      Duplicate Patterns - Transaction matches multiple duplicate detection criteria indicating potential data quality issues (CP_03)\n",
      "      Document Imbalance - Document entries do not balance to zero, indicating posting errors requiring correction (CP_07)\n",
      "      Multiple risk indicators: documentation and processing issues detected\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "🆔 Transaction 3:\n",
      "   Model Score: 0.024 | CP Score: 0.989\n",
      "   Classifications: Model=Low, CP=High\n",
      "   Net Amount: ₹5,000,000\n",
      "   Account: Cash in Bank - CITIBANK INR 8007\n",
      "   Triggered CPs: CP_07 (84), CP_21 (69), CP_24 (78)\n",
      "\n",
      "   📝 EXPLANATION:\n",
      "      Document Imbalance - Document entries do not balance to zero, indicating posting errors requiring correction (CP_07)\n",
      "      Period-End Timing - Transaction occurs on month-end date when period-end adjustments are commonly made (CP_21)\n",
      "      Multiple risk indicators: documentation and processing issues detected\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "🆔 Transaction 4:\n",
      "   Model Score: 0.924 | CP Score: 0.989\n",
      "   Classifications: Model=Medium, CP=High\n",
      "   Net Amount: ₹74,000,000\n",
      "   Account: Cash in Bank - CITIBANK INR 8007\n",
      "   Triggered CPs: CP_07 (84), CP_21 (69), CP_24 (78)\n",
      "\n",
      "   📝 EXPLANATION:\n",
      "      Document Imbalance - Document entries do not balance to zero, indicating posting errors requiring correction (CP_07)\n",
      "      Period-End Timing - Transaction occurs on month-end date when period-end adjustments are commonly made (CP_21)\n",
      "      Unusual Amount Pattern - Transaction amount ending in 000 follows rare sequential or repetitive digit patterns (CP_24)\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "🆔 Transaction 5:\n",
      "   Model Score: 0.034 | CP Score: 0.989\n",
      "   Classifications: Model=Low, CP=High\n",
      "   Net Amount: ₹-1,000\n",
      "   Account: Cash in Bank - CITIBANK INR 8007\n",
      "   Triggered CPs: CP_07 (84), CP_21 (69), CP_24 (78)\n",
      "\n",
      "   📝 EXPLANATION:\n",
      "      Document Imbalance - Document entries do not balance to zero, indicating posting errors requiring correction (CP_07)\n",
      "      Period-End Timing - Transaction occurs on month-end date when period-end adjustments are commonly made (CP_21)\n",
      "      Multiple risk indicators: documentation and processing issues detected\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "🔄 STEP 4: Analyzing Risk Drivers...\n",
      "\n",
      "🎯 Risk Driver Analysis:\n",
      "   Model-driven High Risk: 0\n",
      "   CP-driven High Risk: 394\n",
      "   Both Model & CP High: 0\n",
      "\n",
      "📊 Explanation Type Distribution:\n",
      "   Model-only explanations: 0\n",
      "   CP-only explanations: 394\n",
      "   Combined explanations: 0\n",
      "\n",
      "🔄 STEP 5: Verifying Output Schema...\n",
      "\n",
      "📋 Output Schema Verification:\n",
      "   Total columns: 68\n",
      "   Explanation-related columns: ['Explanation_Summary', 'Top_Risky_Feature_Groups']\n",
      "\n",
      "🔍 Key Column Data Types:\n",
      "   Final_Score: float64 (874/874 non-null)\n",
      "   CP_Score: float64 (874/874 non-null)\n",
      "   Final Risk Classification: object (874/874 non-null)\n",
      "   Explanation_Summary: object (874/874 non-null)\n",
      "\n",
      "💾 Sample output saved to: sample_enhanced_output.xlsx\n",
      "\n",
      "🔄 STEP 6: Testing with Enhanced Test File (if available)...\n",
      "⚠️  Enhanced test file not found: Enhanced Test File.xlsx\n",
      "   Skipping enhanced file test...\n",
      "\n",
      "======================================================================\n",
      "🏁 TESTING SUMMARY\n",
      "======================================================================\n",
      "✅ Pipeline Status: Completed Successfully\n",
      "📊 Total Transactions: 874\n",
      "🔴 High Risk Count: 394\n",
      "🟡 Medium Risk Count: 396\n",
      "🟢 Low Risk Count: 84\n",
      "📝 Explanation Coverage: 100.0% of high risk transactions\n",
      "\n",
      "🎯 Next Steps:\n",
      "   1. Review sample_enhanced_output.xlsx for quality check\n",
      "   2. Test the API endpoint with main.py\n",
      "   3. Deploy to Render when satisfied with results\n",
      "   4. Update n8n workflow if needed\n",
      "\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# === JUPYTER TESTING CODE FOR ENHANCED MODEL LOGIC ===\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the app directory to Python path\n",
    "sys.path.append('./app')\n",
    "\n",
    "# Import your enhanced model logic\n",
    "from model_logic import run_full_pipeline\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"TESTING ENHANCED MODEL LOGIC WITH EXPLANATION INTEGRATION\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# === STEP 1: Test with your old test file ===\n",
    "print(\"\\n🔄 STEP 1: Testing with old test file...\")\n",
    "try:\n",
    "    # Use your old test file\n",
    "    old_test_file = \"old_test_file.csv\"  # Adjust path if needed\n",
    "    \n",
    "    print(f\"📁 Loading test file: {old_test_file}\")\n",
    "    result_df = run_full_pipeline(old_test_file)\n",
    "    \n",
    "    print(f\"✅ Pipeline completed successfully!\")\n",
    "    print(f\"📊 Total transactions processed: {len(result_df):,}\")\n",
    "    \n",
    "    # Check key columns exist\n",
    "    required_columns = [\n",
    "        'Final_Score', 'CP_Score', 'Final Risk Classification',\n",
    "        'Model Classification', 'CP Classification', \n",
    "        'Explanation_Summary', 'Top_Risky_Feature_Groups'\n",
    "    ]\n",
    "    \n",
    "    missing_cols = [col for col in required_columns if col not in result_df.columns]\n",
    "    if missing_cols:\n",
    "        print(f\"❌ Missing columns: {missing_cols}\")\n",
    "    else:\n",
    "        print(\"✅ All required columns present\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"❌ Error in STEP 1: {str(e)}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "\n",
    "# === STEP 2: Analyze Risk Classifications ===\n",
    "print(\"\\n🔄 STEP 2: Analyzing Risk Classifications...\")\n",
    "try:\n",
    "    risk_summary = result_df['Final Risk Classification'].value_counts()\n",
    "    print(\"\\n📈 Risk Classification Distribution:\")\n",
    "    for risk_level, count in risk_summary.items():\n",
    "        percentage = (count / len(result_df)) * 100\n",
    "        print(f\"   {risk_level}: {count:,} ({percentage:.1f}%)\")\n",
    "    \n",
    "    # Focus on High Risk transactions\n",
    "    high_risk_df = result_df[result_df['Final Risk Classification'] == 'High'].copy()\n",
    "    print(f\"\\n🔍 High Risk Transactions: {len(high_risk_df):,}\")\n",
    "    \n",
    "    if len(high_risk_df) > 0:\n",
    "        # Check explanation coverage\n",
    "        explained_count = sum(1 for exp in high_risk_df['Explanation_Summary'] if str(exp).strip() != '')\n",
    "        explanation_coverage = (explained_count / len(high_risk_df)) * 100\n",
    "        print(f\"📝 Explanation Coverage: {explained_count:,}/{len(high_risk_df):,} ({explanation_coverage:.1f}%)\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Error in STEP 2: {str(e)}\")\n",
    "\n",
    "# === STEP 3: Sample High Risk Explanations ===\n",
    "print(\"\\n🔄 STEP 3: Examining High Risk Explanations...\")\n",
    "try:\n",
    "    if len(high_risk_df) > 0:\n",
    "        # Get first 5 high risk transactions with explanations\n",
    "        sample_high_risk = high_risk_df[high_risk_df['Explanation_Summary'].str.strip() != ''].head(5)\n",
    "        \n",
    "        print(f\"\\n📋 Sample High Risk Explanations (First {len(sample_high_risk)} transactions):\")\n",
    "        print(\"=\" * 100)\n",
    "        \n",
    "        for idx, row in sample_high_risk.iterrows():\n",
    "            print(f\"\\n🆔 Transaction {idx + 1}:\")\n",
    "            print(f\"   Model Score: {row['Final_Score']:.3f} | CP Score: {row['CP_Score']:.3f}\")\n",
    "            print(f\"   Classifications: Model={row['Model Classification']}, CP={row['CP Classification']}\")\n",
    "            print(f\"   Net Amount: ₹{row.get('Net Amount', 0):,.0f}\")\n",
    "            print(f\"   Account: {row.get('Account Name', 'N/A')}\")\n",
    "            print(f\"   Triggered CPs: {row.get('Triggered_CPs', 'None')}\")\n",
    "            print(f\"\\n   📝 EXPLANATION:\")\n",
    "            explanation = str(row['Explanation_Summary']).strip()\n",
    "            if explanation:\n",
    "                # Split by newlines and indent each line\n",
    "                for line in explanation.split('\\n'):\n",
    "                    if line.strip():\n",
    "                        print(f\"      {line.strip()}\")\n",
    "            else:\n",
    "                print(\"      No explanation provided\")\n",
    "            print(\"-\" * 80)\n",
    "    else:\n",
    "        print(\"   No high risk transactions found to display explanations\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"❌ Error in STEP 3: {str(e)}\")\n",
    "\n",
    "# === STEP 4: Compare Model vs CP Driven Risks ===\n",
    "print(\"\\n🔄 STEP 4: Analyzing Risk Drivers...\")\n",
    "try:\n",
    "    if len(high_risk_df) > 0:\n",
    "        model_high = len(high_risk_df[high_risk_df['Model Classification'] == 'High'])\n",
    "        cp_high = len(high_risk_df[high_risk_df['CP Classification'] == 'High'])\n",
    "        both_high = len(high_risk_df[(high_risk_df['Model Classification'] == 'High') & \n",
    "                                    (high_risk_df['CP Classification'] == 'High')])\n",
    "        \n",
    "        print(f\"\\n🎯 Risk Driver Analysis:\")\n",
    "        print(f\"   Model-driven High Risk: {model_high:,}\")\n",
    "        print(f\"   CP-driven High Risk: {cp_high:,}\")\n",
    "        print(f\"   Both Model & CP High: {both_high:,}\")\n",
    "        \n",
    "        # Show breakdown of explanation types\n",
    "        model_only = high_risk_df[(high_risk_df['Model Classification'] == 'High') & \n",
    "                                 (high_risk_df['CP Classification'] != 'High')]\n",
    "        cp_only = high_risk_df[(high_risk_df['Model Classification'] != 'High') & \n",
    "                              (high_risk_df['CP Classification'] == 'High')]\n",
    "        \n",
    "        print(f\"\\n📊 Explanation Type Distribution:\")\n",
    "        print(f\"   Model-only explanations: {len(model_only):,}\")\n",
    "        print(f\"   CP-only explanations: {len(cp_only):,}\")\n",
    "        print(f\"   Combined explanations: {both_high:,}\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"❌ Error in STEP 4: {str(e)}\")\n",
    "\n",
    "# === STEP 5: Verify Column Schema Compatibility ===\n",
    "print(\"\\n🔄 STEP 5: Verifying Output Schema...\")\n",
    "try:\n",
    "    print(f\"\\n📋 Output Schema Verification:\")\n",
    "    print(f\"   Total columns: {len(result_df.columns)}\")\n",
    "    \n",
    "    # Check for key explanation columns\n",
    "    explanation_cols = [col for col in result_df.columns if 'explanation' in col.lower() or 'risky' in col.lower()]\n",
    "    print(f\"   Explanation-related columns: {explanation_cols}\")\n",
    "    \n",
    "    # Check data types\n",
    "    print(f\"\\n🔍 Key Column Data Types:\")\n",
    "    key_columns = ['Final_Score', 'CP_Score', 'Final Risk Classification', 'Explanation_Summary']\n",
    "    for col in key_columns:\n",
    "        if col in result_df.columns:\n",
    "            dtype = result_df[col].dtype\n",
    "            non_null = result_df[col].notna().sum()\n",
    "            print(f\"   {col}: {dtype} ({non_null:,}/{len(result_df):,} non-null)\")\n",
    "    \n",
    "    # Export sample for manual review\n",
    "    sample_output = result_df.head(100)\n",
    "    sample_filename = \"sample_enhanced_output.xlsx\"\n",
    "    sample_output.to_excel(sample_filename, index=False)\n",
    "    print(f\"\\n💾 Sample output saved to: {sample_filename}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Error in STEP 5: {str(e)}\")\n",
    "\n",
    "# === STEP 6: Test with Enhanced Test File (if available) ===\n",
    "print(\"\\n🔄 STEP 6: Testing with Enhanced Test File (if available)...\")\n",
    "try:\n",
    "    enhanced_test_file = \"Enhanced Test File.xlsx\"  # Adjust path if needed\n",
    "    \n",
    "    if os.path.exists(enhanced_test_file):\n",
    "        print(f\"📁 Found enhanced test file: {enhanced_test_file}\")\n",
    "        result_enhanced = run_full_pipeline(enhanced_test_file)\n",
    "        \n",
    "        print(f\"✅ Enhanced file processed successfully!\")\n",
    "        print(f\"📊 Enhanced file transactions: {len(result_enhanced):,}\")\n",
    "        \n",
    "        # Compare results\n",
    "        enhanced_high_risk = len(result_enhanced[result_enhanced['Final Risk Classification'] == 'High'])\n",
    "        print(f\"🔍 Enhanced file high risk transactions: {enhanced_high_risk:,}\")\n",
    "        \n",
    "    else:\n",
    "        print(f\"⚠️  Enhanced test file not found: {enhanced_test_file}\")\n",
    "        print(\"   Skipping enhanced file test...\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"❌ Error in STEP 6: {str(e)}\")\n",
    "\n",
    "# === FINAL SUMMARY ===\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"🏁 TESTING SUMMARY\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "try:\n",
    "    print(f\"✅ Pipeline Status: Completed Successfully\")\n",
    "    print(f\"📊 Total Transactions: {len(result_df):,}\")\n",
    "    print(f\"🔴 High Risk Count: {len(result_df[result_df['Final Risk Classification'] == 'High']):,}\")\n",
    "    print(f\"🟡 Medium Risk Count: {len(result_df[result_df['Final Risk Classification'] == 'Medium']):,}\")\n",
    "    print(f\"🟢 Low Risk Count: {len(result_df[result_df['Final Risk Classification'] == 'Low']):,}\")\n",
    "    \n",
    "    # Check explanation quality\n",
    "    high_risk_with_explanations = len(result_df[\n",
    "        (result_df['Final Risk Classification'] == 'High') & \n",
    "        (result_df['Explanation_Summary'].str.strip() != '')\n",
    "    ])\n",
    "    \n",
    "    total_high_risk = len(result_df[result_df['Final Risk Classification'] == 'High'])\n",
    "    if total_high_risk > 0:\n",
    "        explanation_rate = (high_risk_with_explanations / total_high_risk) * 100\n",
    "        print(f\"📝 Explanation Coverage: {explanation_rate:.1f}% of high risk transactions\")\n",
    "    \n",
    "    print(f\"\\n🎯 Next Steps:\")\n",
    "    print(f\"   1. Review sample_enhanced_output.xlsx for quality check\")\n",
    "    print(f\"   2. Test the API endpoint with main.py\")\n",
    "    print(f\"   3. Deploy to Render when satisfied with results\")\n",
    "    print(f\"   4. Update n8n workflow if needed\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Error in final summary: {str(e)}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5ca96c56-c61a-4d2e-af8d-7df6cb95cd6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === DEBUG VERSION - COLUMN TRACKING MODEL LOGIC ===\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import warnings\n",
    "import logging\n",
    "from typing import Dict, List, Tuple, Optional, Union\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "def debug_columns(df, step_name):\n",
    "    \"\"\"Debug function to track column changes\"\"\"\n",
    "    print(f\"\\n=== {step_name} ===\")\n",
    "    print(f\"Total columns: {len(df.columns)}\")\n",
    "    print(f\"Column names: {list(df.columns)}\")\n",
    "    print(\"-\" * 80)\n",
    "\n",
    "def run_full_pipeline_debug(file_path: str) -> pd.DataFrame:\n",
    "    logging.debug(\"Starting DEBUG run_full_pipeline\")\n",
    "    # Lazy load heavy packages here\n",
    "    import shap\n",
    "    from catboost import CatBoostClassifier\n",
    "    from sentence_transformers import SentenceTransformer\n",
    "    from umap import UMAP\n",
    "    from sklearn.cluster import KMeans\n",
    "\n",
    "    print(\"STARTING DEBUG MODE - TRACKING ALL COLUMN CHANGES\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    # === Load Models ===\n",
    "    try:\n",
    "        model = CatBoostClassifier()\n",
    "        model.load_model(\"models/catboost_v2_model.cbm\")\n",
    "        model_bert = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "        print(\"Models loaded successfully\")\n",
    "    except Exception as e:\n",
    "        print(f\"Model loading failed: {e}\")\n",
    "        raise\n",
    "\n",
    "    # === Load Data ===\n",
    "    test_df = pd.read_csv(file_path, encoding='ISO-8859-1')\n",
    "    test_df.columns = test_df.columns.str.strip()\n",
    "    debug_columns(test_df, \"INITIAL DATA LOAD\")\n",
    "\n",
    "    # === Clean Numeric Columns ===\n",
    "    comma_cols = [\"Entered Dr SUM\", \"Entered Cr SUM\", \"Accounted Dr SUM\", \"Accounted Cr SUM\", \"Net Amount\"]\n",
    "    for col in comma_cols:\n",
    "        if col in test_df.columns:\n",
    "            test_df[col] = test_df[col].astype(str).str.replace(\",\", \"\").replace(\"nan\", np.nan).astype(float)\n",
    "    debug_columns(test_df, \"AFTER NUMERIC CLEANING\")\n",
    "\n",
    "    # === Combine Text Fields ===\n",
    "    text_fields = [\"Line Desc\", \"Source Desc\", \"Batch Name\"]\n",
    "    test_df[text_fields] = test_df[text_fields].fillna(\"\")\n",
    "    test_df[\"Combined_Text\"] = test_df[\"Line Desc\"] + \" | \" + test_df[\"Source Desc\"] + \" | \" + test_df[\"Batch Name\"]\n",
    "    debug_columns(test_df, \"AFTER ADDING Combined_Text\")\n",
    "\n",
    "    # === BERT Embeddings ===\n",
    "    embeddings = model_bert.encode(test_df[\"Combined_Text\"].tolist(), show_progress_bar=False)\n",
    "    embedding_df = pd.DataFrame(embeddings, columns=[f\"text_emb_{i}\" for i in range(embeddings.shape[1])])\n",
    "    print(f\"BERT embedding columns created: {len(embedding_df.columns)}\")\n",
    "    \n",
    "    test_df = pd.concat([test_df.reset_index(drop=True), embedding_df], axis=1)\n",
    "    debug_columns(test_df, \"AFTER ADDING BERT EMBEDDINGS\")\n",
    "\n",
    "    # === Clustering ===\n",
    "    umap_model = UMAP(n_neighbors=15, min_dist=0.1, n_components=2, random_state=42)\n",
    "    reduced = umap_model.fit_transform(embeddings)\n",
    "    kmeans = KMeans(n_clusters=10, random_state=42)\n",
    "    test_df[\"Narration_Cluster\"] = kmeans.fit_predict(reduced)\n",
    "    debug_columns(test_df, \"AFTER ADDING Narration_Cluster\")\n",
    "\n",
    "    cluster_summary = (\n",
    "        test_df.groupby(\"Narration_Cluster\")[\"Combined_Text\"]\n",
    "        .apply(lambda x: \"; \".join(x.head(3)))\n",
    "        .reset_index(name=\"Narration_Cluster_Label\")\n",
    "    )\n",
    "    test_df = test_df.merge(cluster_summary, on=\"Narration_Cluster\", how=\"left\")\n",
    "    debug_columns(test_df, \"AFTER ADDING Narration_Cluster_Label\")\n",
    "\n",
    "    # === Date Features ===\n",
    "    date_cols = [\"Accounting Date\", \"Invoice Date\", \"Posted Date\"]\n",
    "    for col in date_cols:\n",
    "        test_df[col] = pd.to_datetime(test_df[col], errors=\"coerce\")\n",
    "\n",
    "    test_df[\"Accounting_Month\"] = test_df[\"Accounting Date\"].dt.month\n",
    "    test_df[\"Accounting_Weekday\"] = test_df[\"Accounting Date\"].dt.weekday\n",
    "    test_df[\"Invoice_Month\"] = test_df[\"Invoice Date\"].dt.month\n",
    "    test_df[\"Invoice_Weekday\"] = test_df[\"Invoice Date\"].dt.weekday\n",
    "    test_df[\"Posted_Month\"] = test_df[\"Posted Date\"].dt.month\n",
    "    test_df[\"Posted_Weekday\"] = test_df[\"Posted Date\"].dt.weekday\n",
    "    debug_columns(test_df, \"AFTER ADDING DATE FEATURES\")\n",
    "\n",
    "    # === Feature Preparation ===\n",
    "    exclude_cols = [\"S. No\", \"Combined_Text\", \"Accounting Date\", \"Invoice Date\", \"Posted Date\"]\n",
    "    model_feature_names = model.feature_names_\n",
    "    feature_cols = [col for col in test_df.columns if col in model_feature_names and col not in exclude_cols and not col.startswith(\"Unnamed\")]\n",
    "\n",
    "    for col in feature_cols:\n",
    "        if test_df[col].dtype == object or test_df[col].isnull().any():\n",
    "            test_df[col] = test_df[col].astype(str).fillna(\"Missing\")\n",
    "\n",
    "    X_final = test_df[feature_cols].copy()\n",
    "    print(f\"Feature columns for model: {len(feature_cols)}\")\n",
    "\n",
    "    # === Model Predictions ===\n",
    "    test_df[\"Model_Score\"] = model.predict_proba(X_final)[:, 1]\n",
    "    test_df[\"Final_Score\"] = test_df[\"Model_Score\"].round(3)\n",
    "    debug_columns(test_df, \"AFTER ADDING MODEL SCORES\")\n",
    "\n",
    "    # === SHAP Analysis ===\n",
    "    explainer = shap.TreeExplainer(model)\n",
    "    shap_values = explainer.shap_values(X_final)\n",
    "    print(f\"SHAP values calculated for {len(shap_values)} transactions\")\n",
    "\n",
    "    # === Control Points Setup ===\n",
    "    cp_score_dict = {\n",
    "        \"CP_01\": 83, \"CP_02\": 86, \"CP_03\": 78, \"CP_04\": 81, \"CP_07\": 84, \"CP_08\": 80,\n",
    "        \"CP_09\": 76, \"CP_15\": 88, \"CP_16\": 73, \"CP_17\": 75, \"CP_19\": 60,\n",
    "        \"CP_21\": 69, \"CP_22\": 66, \"CP_23\": 87, \"CP_24\": 78, \"CP_26\": 0,\n",
    "        \"CP_30\": 72, \"CP_32\": 72\n",
    "    }\n",
    "    valid_cps = list(cp_score_dict.keys())\n",
    "\n",
    "    pl_net_total = test_df[test_df[\"PL/ BS\"] == \"PL\"][\"Net Amount\"].abs().sum()\n",
    "    pl_net_threshold = 0.10 * pl_net_total\n",
    "    total_net = test_df[\"Net Amount\"].abs().sum()\n",
    "\n",
    "    # Control Point Functions (simplified for debug)\n",
    "    def cp_01(row):\n",
    "        keywords = ['fraud','bribe','kickback','suspicious','fake','dummy']\n",
    "        text = f\"{str(row.get('Line Desc', '')).lower()} {str(row.get('Source Desc', '')).lower()}\"\n",
    "        return int(any(k in text for k in keywords))\n",
    "\n",
    "    def cp_02(row):\n",
    "        return int(row.get(\"PL/ BS\") == \"PL\" and abs(row.get(\"Net Amount\", 0)) > pl_net_threshold)\n",
    "\n",
    "    def cp_19(row):\n",
    "        try: \n",
    "            return int(pd.to_datetime(row[\"Accounting Date\"]).weekday() == 6)\n",
    "        except: \n",
    "            return 0\n",
    "\n",
    "    # Apply a few CPs for testing\n",
    "    print(\"Applying Control Points...\")\n",
    "    test_df[\"CP_01\"] = test_df.apply(cp_01, axis=1)\n",
    "    debug_columns(test_df, \"AFTER ADDING CP_01\")\n",
    "\n",
    "    test_df[\"CP_02\"] = test_df.apply(cp_02, axis=1)\n",
    "    debug_columns(test_df, \"AFTER ADDING CP_02\")\n",
    "\n",
    "    test_df[\"CP_19\"] = test_df.apply(cp_19, axis=1)\n",
    "    debug_columns(test_df, \"AFTER ADDING CP_19\")\n",
    "\n",
    "    # Add Currency column explicitly\n",
    "    print(\"Adding Currency column...\")\n",
    "    if \"Currency\" not in test_df.columns:\n",
    "        test_df[\"Currency\"] = \"INR\"\n",
    "    debug_columns(test_df, \"AFTER ADDING Currency\")\n",
    "\n",
    "    # Simple CP Score calculation\n",
    "    def compute_cp_score_simple(row):\n",
    "        triggered = [\"CP_01\", \"CP_02\", \"CP_19\"]\n",
    "        triggered_count = sum(1 for cp in triggered if row.get(cp, 0) == 1)\n",
    "        return round(triggered_count * 0.3, 3)  # Simple scoring\n",
    "\n",
    "    test_df[\"CP_Score\"] = test_df.apply(compute_cp_score_simple, axis=1)\n",
    "    debug_columns(test_df, \"AFTER ADDING CP_Score\")\n",
    "\n",
    "    test_df[\"Triggered_CPs\"] = \"CP_01 (83)\"  # Placeholder\n",
    "    debug_columns(test_df, \"AFTER ADDING Triggered_CPs\")\n",
    "\n",
    "    # === Risk Classifications (INTERNAL VARIABLES ONLY) ===\n",
    "    print(\"Creating internal risk classifications...\")\n",
    "    \n",
    "    # Create these as separate lists, NOT dataframe columns\n",
    "    model_classifications = []\n",
    "    cp_classifications = []\n",
    "    final_risk_classifications = []\n",
    "    \n",
    "    for i in range(len(test_df)):\n",
    "        score = test_df.iloc[i][\"Final_Score\"]\n",
    "        cp_score = test_df.iloc[i][\"CP_Score\"]\n",
    "        \n",
    "        model_class = \"High\" if score >= 0.995 else (\"Medium\" if score >= 0.5 else \"Low\")\n",
    "        cp_class = \"High\" if cp_score >= 0.95 else (\"Medium\" if cp_score > 0.8 else \"Low\")\n",
    "        final_risk = \"High\" if model_class == \"High\" or cp_class == \"High\" else (\"Medium\" if model_class == \"Medium\" or cp_class == \"Medium\" else \"Low\")\n",
    "        \n",
    "        model_classifications.append(model_class)\n",
    "        cp_classifications.append(cp_class)\n",
    "        final_risk_classifications.append(final_risk)\n",
    "    \n",
    "    print(f\"Created {len(model_classifications)} internal classifications\")\n",
    "    print(f\"Risk distribution: High={final_risk_classifications.count('High')}, Medium={final_risk_classifications.count('Medium')}, Low={final_risk_classifications.count('Low')}\")\n",
    "    \n",
    "    # Check if these accidentally became columns\n",
    "    debug_columns(test_df, \"AFTER INTERNAL CLASSIFICATIONS\")\n",
    "\n",
    "    # === Create Explanation Columns ===\n",
    "    print(\"Creating explanation columns...\")\n",
    "    \n",
    "    # Simple placeholder explanations for testing\n",
    "    top_risky_texts = []\n",
    "    top_safe_texts = []\n",
    "    explanation_summaries = []\n",
    "    \n",
    "    for i in range(len(test_df)):\n",
    "        final_risk = final_risk_classifications[i]\n",
    "        if final_risk == \"High\":\n",
    "            explanation_summaries.append(\"High risk transaction requiring enhanced review\")\n",
    "        else:\n",
    "            explanation_summaries.append(\"\")\n",
    "        \n",
    "        top_risky_texts.append(\"Risk factor 1\\nRisk factor 2\")\n",
    "        top_safe_texts.append(\"Safe factor 1\\nSafe factor 2\")\n",
    "\n",
    "    test_df[\"Top_Risky_Feature_Groups\"] = top_risky_texts\n",
    "    debug_columns(test_df, \"AFTER ADDING Top_Risky_Feature_Groups\")\n",
    "\n",
    "    test_df[\"Top_Safe_Feature_Groups\"] = top_safe_texts\n",
    "    debug_columns(test_df, \"AFTER ADDING Top_Safe_Feature_Groups\")\n",
    "\n",
    "    test_df[\"Explanation_Summary\"] = explanation_summaries\n",
    "    debug_columns(test_df, \"AFTER ADDING Explanation_Summary\")\n",
    "\n",
    "    # === Remove BERT Columns ===\n",
    "    print(\"Removing BERT embedding columns...\")\n",
    "    bert_columns = [col for col in test_df.columns if col.startswith(\"text_emb_\")]\n",
    "    print(f\"Removing {len(bert_columns)} BERT columns\")\n",
    "    test_df = test_df.drop(columns=bert_columns)\n",
    "    debug_columns(test_df, \"AFTER REMOVING BERT COLUMNS\")\n",
    "\n",
    "    # === Expected Columns Check ===\n",
    "    expected_columns = [\n",
    "        # Original 30 columns\n",
    "        \"S. No\", \"Entity Name\", \"Accounting Date\", \"Approval Type\", \"Document Type\", \"Invoice Date\", \"Day\", \"Nature\",\n",
    "        \"Account Code\", \"PL/ BS\", \"Report Group\", \"Account Name\", \"Nature in balance sheet\", \"Document Number\", \"Je Line Num\",\n",
    "        \"Source Number\", \"Source Name\", \"Source Voucher Name\", \"Source Desc\", \"Line Desc\", \"Project Code\", \"Internal Reference\", \n",
    "        \"Posted Date\", \"Branch\", \"Batch Name\", \"Entered Dr SUM\", \"Entered Cr SUM\", \"Accounted Dr SUM\", \"Accounted Cr SUM\", \"Net Amount\",\n",
    "        \n",
    "        # Generated analysis columns\n",
    "        \"Combined_Text\", \"Narration_Cluster\", \"Narration_Cluster_Label\",\n",
    "        \"Accounting_Month\", \"Accounting_Weekday\", \"Invoice_Month\", \"Invoice_Weekday\", \"Posted_Month\", \"Posted_Weekday\",\n",
    "        \"Model_Score\", \"Final_Score\",\n",
    "        \n",
    "        # Explanation columns\n",
    "        \"Top_Risky_Feature_Groups\", \"Top_Safe_Feature_Groups\", \"Explanation_Summary\",\n",
    "        \n",
    "        # Control points columns (simplified for debug)\n",
    "        \"CP_01\", \"CP_02\", \"CP_19\", \"Currency\", \"Triggered_CPs\", \"CP_Score\"\n",
    "    ]\n",
    "\n",
    "    print(f\"\\nEXPECTED COLUMNS ({len(expected_columns)}): {expected_columns}\")\n",
    "    print(f\"\\nACTUAL COLUMNS ({len(test_df.columns)}): {list(test_df.columns)}\")\n",
    "\n",
    "    # Find differences\n",
    "    missing_from_actual = [col for col in expected_columns if col not in test_df.columns]\n",
    "    extra_in_actual = [col for col in test_df.columns if col not in expected_columns]\n",
    "\n",
    "    print(f\"\\nMISSING FROM ACTUAL: {missing_from_actual}\")\n",
    "    print(f\"EXTRA IN ACTUAL: {extra_in_actual}\")\n",
    "\n",
    "    # Try final column selection\n",
    "    print(\"\\nAttempting final column selection...\")\n",
    "    try:\n",
    "        available_expected = [col for col in expected_columns if col in test_df.columns]\n",
    "        final_df = test_df[available_expected].copy()\n",
    "        print(f\"SUCCESS: Final dataframe has {len(final_df.columns)} columns\")\n",
    "        debug_columns(final_df, \"FINAL OUTPUT\")\n",
    "        return final_df\n",
    "    except Exception as e:\n",
    "        print(f\"FAILED: Column selection error: {e}\")\n",
    "        return test_df\n",
    "\n",
    "# Test function\n",
    "def test_debug_pipeline(file_path: str):\n",
    "    try:\n",
    "        result = run_full_pipeline_debug(file_path)\n",
    "        print(f\"\\nFINAL RESULT: {len(result.columns)} columns\")\n",
    "        print(\"Debug completed successfully\")\n",
    "        return result\n",
    "    except Exception as e:\n",
    "        print(f\"DEBUG PIPELINE FAILED: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dede0b51-de76-473b-befd-307407a1db6c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-20 12:41:04,330 - INFO - Use pytorch device_name: cpu\n",
      "2025-09-20 12:41:04,331 - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STARTING DEBUG MODE - TRACKING ALL COLUMN CHANGES\n",
      "================================================================================\n",
      "Models loaded successfully\n",
      "\n",
      "=== INITIAL DATA LOAD ===\n",
      "Total columns: 30\n",
      "Column names: ['S. No', 'Entity Name', 'Accounting Date', 'Approval Type', 'Document Type', 'Invoice Date', 'Day', 'Nature', 'Account Code', 'PL/ BS', 'Report Group', 'Account Name', 'Nature in balance sheet', 'Document Number', 'Je Line Num', 'Source Number', 'Source Name', 'Source Voucher Name', 'Source Desc', 'Line Desc', 'Project Code', 'Internal Reference', 'Posted Date', 'Branch', 'Batch Name', 'Entered Dr SUM', 'Entered Cr SUM', 'Accounted Dr SUM', 'Accounted Cr SUM', 'Net Amount']\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "=== AFTER NUMERIC CLEANING ===\n",
      "Total columns: 30\n",
      "Column names: ['S. No', 'Entity Name', 'Accounting Date', 'Approval Type', 'Document Type', 'Invoice Date', 'Day', 'Nature', 'Account Code', 'PL/ BS', 'Report Group', 'Account Name', 'Nature in balance sheet', 'Document Number', 'Je Line Num', 'Source Number', 'Source Name', 'Source Voucher Name', 'Source Desc', 'Line Desc', 'Project Code', 'Internal Reference', 'Posted Date', 'Branch', 'Batch Name', 'Entered Dr SUM', 'Entered Cr SUM', 'Accounted Dr SUM', 'Accounted Cr SUM', 'Net Amount']\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "=== AFTER ADDING Combined_Text ===\n",
      "Total columns: 31\n",
      "Column names: ['S. No', 'Entity Name', 'Accounting Date', 'Approval Type', 'Document Type', 'Invoice Date', 'Day', 'Nature', 'Account Code', 'PL/ BS', 'Report Group', 'Account Name', 'Nature in balance sheet', 'Document Number', 'Je Line Num', 'Source Number', 'Source Name', 'Source Voucher Name', 'Source Desc', 'Line Desc', 'Project Code', 'Internal Reference', 'Posted Date', 'Branch', 'Batch Name', 'Entered Dr SUM', 'Entered Cr SUM', 'Accounted Dr SUM', 'Accounted Cr SUM', 'Net Amount', 'Combined_Text']\n",
      "--------------------------------------------------------------------------------\n",
      "BERT embedding columns created: 384\n",
      "\n",
      "=== AFTER ADDING BERT EMBEDDINGS ===\n",
      "Total columns: 415\n",
      "Column names: ['S. No', 'Entity Name', 'Accounting Date', 'Approval Type', 'Document Type', 'Invoice Date', 'Day', 'Nature', 'Account Code', 'PL/ BS', 'Report Group', 'Account Name', 'Nature in balance sheet', 'Document Number', 'Je Line Num', 'Source Number', 'Source Name', 'Source Voucher Name', 'Source Desc', 'Line Desc', 'Project Code', 'Internal Reference', 'Posted Date', 'Branch', 'Batch Name', 'Entered Dr SUM', 'Entered Cr SUM', 'Accounted Dr SUM', 'Accounted Cr SUM', 'Net Amount', 'Combined_Text', 'text_emb_0', 'text_emb_1', 'text_emb_2', 'text_emb_3', 'text_emb_4', 'text_emb_5', 'text_emb_6', 'text_emb_7', 'text_emb_8', 'text_emb_9', 'text_emb_10', 'text_emb_11', 'text_emb_12', 'text_emb_13', 'text_emb_14', 'text_emb_15', 'text_emb_16', 'text_emb_17', 'text_emb_18', 'text_emb_19', 'text_emb_20', 'text_emb_21', 'text_emb_22', 'text_emb_23', 'text_emb_24', 'text_emb_25', 'text_emb_26', 'text_emb_27', 'text_emb_28', 'text_emb_29', 'text_emb_30', 'text_emb_31', 'text_emb_32', 'text_emb_33', 'text_emb_34', 'text_emb_35', 'text_emb_36', 'text_emb_37', 'text_emb_38', 'text_emb_39', 'text_emb_40', 'text_emb_41', 'text_emb_42', 'text_emb_43', 'text_emb_44', 'text_emb_45', 'text_emb_46', 'text_emb_47', 'text_emb_48', 'text_emb_49', 'text_emb_50', 'text_emb_51', 'text_emb_52', 'text_emb_53', 'text_emb_54', 'text_emb_55', 'text_emb_56', 'text_emb_57', 'text_emb_58', 'text_emb_59', 'text_emb_60', 'text_emb_61', 'text_emb_62', 'text_emb_63', 'text_emb_64', 'text_emb_65', 'text_emb_66', 'text_emb_67', 'text_emb_68', 'text_emb_69', 'text_emb_70', 'text_emb_71', 'text_emb_72', 'text_emb_73', 'text_emb_74', 'text_emb_75', 'text_emb_76', 'text_emb_77', 'text_emb_78', 'text_emb_79', 'text_emb_80', 'text_emb_81', 'text_emb_82', 'text_emb_83', 'text_emb_84', 'text_emb_85', 'text_emb_86', 'text_emb_87', 'text_emb_88', 'text_emb_89', 'text_emb_90', 'text_emb_91', 'text_emb_92', 'text_emb_93', 'text_emb_94', 'text_emb_95', 'text_emb_96', 'text_emb_97', 'text_emb_98', 'text_emb_99', 'text_emb_100', 'text_emb_101', 'text_emb_102', 'text_emb_103', 'text_emb_104', 'text_emb_105', 'text_emb_106', 'text_emb_107', 'text_emb_108', 'text_emb_109', 'text_emb_110', 'text_emb_111', 'text_emb_112', 'text_emb_113', 'text_emb_114', 'text_emb_115', 'text_emb_116', 'text_emb_117', 'text_emb_118', 'text_emb_119', 'text_emb_120', 'text_emb_121', 'text_emb_122', 'text_emb_123', 'text_emb_124', 'text_emb_125', 'text_emb_126', 'text_emb_127', 'text_emb_128', 'text_emb_129', 'text_emb_130', 'text_emb_131', 'text_emb_132', 'text_emb_133', 'text_emb_134', 'text_emb_135', 'text_emb_136', 'text_emb_137', 'text_emb_138', 'text_emb_139', 'text_emb_140', 'text_emb_141', 'text_emb_142', 'text_emb_143', 'text_emb_144', 'text_emb_145', 'text_emb_146', 'text_emb_147', 'text_emb_148', 'text_emb_149', 'text_emb_150', 'text_emb_151', 'text_emb_152', 'text_emb_153', 'text_emb_154', 'text_emb_155', 'text_emb_156', 'text_emb_157', 'text_emb_158', 'text_emb_159', 'text_emb_160', 'text_emb_161', 'text_emb_162', 'text_emb_163', 'text_emb_164', 'text_emb_165', 'text_emb_166', 'text_emb_167', 'text_emb_168', 'text_emb_169', 'text_emb_170', 'text_emb_171', 'text_emb_172', 'text_emb_173', 'text_emb_174', 'text_emb_175', 'text_emb_176', 'text_emb_177', 'text_emb_178', 'text_emb_179', 'text_emb_180', 'text_emb_181', 'text_emb_182', 'text_emb_183', 'text_emb_184', 'text_emb_185', 'text_emb_186', 'text_emb_187', 'text_emb_188', 'text_emb_189', 'text_emb_190', 'text_emb_191', 'text_emb_192', 'text_emb_193', 'text_emb_194', 'text_emb_195', 'text_emb_196', 'text_emb_197', 'text_emb_198', 'text_emb_199', 'text_emb_200', 'text_emb_201', 'text_emb_202', 'text_emb_203', 'text_emb_204', 'text_emb_205', 'text_emb_206', 'text_emb_207', 'text_emb_208', 'text_emb_209', 'text_emb_210', 'text_emb_211', 'text_emb_212', 'text_emb_213', 'text_emb_214', 'text_emb_215', 'text_emb_216', 'text_emb_217', 'text_emb_218', 'text_emb_219', 'text_emb_220', 'text_emb_221', 'text_emb_222', 'text_emb_223', 'text_emb_224', 'text_emb_225', 'text_emb_226', 'text_emb_227', 'text_emb_228', 'text_emb_229', 'text_emb_230', 'text_emb_231', 'text_emb_232', 'text_emb_233', 'text_emb_234', 'text_emb_235', 'text_emb_236', 'text_emb_237', 'text_emb_238', 'text_emb_239', 'text_emb_240', 'text_emb_241', 'text_emb_242', 'text_emb_243', 'text_emb_244', 'text_emb_245', 'text_emb_246', 'text_emb_247', 'text_emb_248', 'text_emb_249', 'text_emb_250', 'text_emb_251', 'text_emb_252', 'text_emb_253', 'text_emb_254', 'text_emb_255', 'text_emb_256', 'text_emb_257', 'text_emb_258', 'text_emb_259', 'text_emb_260', 'text_emb_261', 'text_emb_262', 'text_emb_263', 'text_emb_264', 'text_emb_265', 'text_emb_266', 'text_emb_267', 'text_emb_268', 'text_emb_269', 'text_emb_270', 'text_emb_271', 'text_emb_272', 'text_emb_273', 'text_emb_274', 'text_emb_275', 'text_emb_276', 'text_emb_277', 'text_emb_278', 'text_emb_279', 'text_emb_280', 'text_emb_281', 'text_emb_282', 'text_emb_283', 'text_emb_284', 'text_emb_285', 'text_emb_286', 'text_emb_287', 'text_emb_288', 'text_emb_289', 'text_emb_290', 'text_emb_291', 'text_emb_292', 'text_emb_293', 'text_emb_294', 'text_emb_295', 'text_emb_296', 'text_emb_297', 'text_emb_298', 'text_emb_299', 'text_emb_300', 'text_emb_301', 'text_emb_302', 'text_emb_303', 'text_emb_304', 'text_emb_305', 'text_emb_306', 'text_emb_307', 'text_emb_308', 'text_emb_309', 'text_emb_310', 'text_emb_311', 'text_emb_312', 'text_emb_313', 'text_emb_314', 'text_emb_315', 'text_emb_316', 'text_emb_317', 'text_emb_318', 'text_emb_319', 'text_emb_320', 'text_emb_321', 'text_emb_322', 'text_emb_323', 'text_emb_324', 'text_emb_325', 'text_emb_326', 'text_emb_327', 'text_emb_328', 'text_emb_329', 'text_emb_330', 'text_emb_331', 'text_emb_332', 'text_emb_333', 'text_emb_334', 'text_emb_335', 'text_emb_336', 'text_emb_337', 'text_emb_338', 'text_emb_339', 'text_emb_340', 'text_emb_341', 'text_emb_342', 'text_emb_343', 'text_emb_344', 'text_emb_345', 'text_emb_346', 'text_emb_347', 'text_emb_348', 'text_emb_349', 'text_emb_350', 'text_emb_351', 'text_emb_352', 'text_emb_353', 'text_emb_354', 'text_emb_355', 'text_emb_356', 'text_emb_357', 'text_emb_358', 'text_emb_359', 'text_emb_360', 'text_emb_361', 'text_emb_362', 'text_emb_363', 'text_emb_364', 'text_emb_365', 'text_emb_366', 'text_emb_367', 'text_emb_368', 'text_emb_369', 'text_emb_370', 'text_emb_371', 'text_emb_372', 'text_emb_373', 'text_emb_374', 'text_emb_375', 'text_emb_376', 'text_emb_377', 'text_emb_378', 'text_emb_379', 'text_emb_380', 'text_emb_381', 'text_emb_382', 'text_emb_383']\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "=== AFTER ADDING Narration_Cluster ===\n",
      "Total columns: 416\n",
      "Column names: ['S. No', 'Entity Name', 'Accounting Date', 'Approval Type', 'Document Type', 'Invoice Date', 'Day', 'Nature', 'Account Code', 'PL/ BS', 'Report Group', 'Account Name', 'Nature in balance sheet', 'Document Number', 'Je Line Num', 'Source Number', 'Source Name', 'Source Voucher Name', 'Source Desc', 'Line Desc', 'Project Code', 'Internal Reference', 'Posted Date', 'Branch', 'Batch Name', 'Entered Dr SUM', 'Entered Cr SUM', 'Accounted Dr SUM', 'Accounted Cr SUM', 'Net Amount', 'Combined_Text', 'text_emb_0', 'text_emb_1', 'text_emb_2', 'text_emb_3', 'text_emb_4', 'text_emb_5', 'text_emb_6', 'text_emb_7', 'text_emb_8', 'text_emb_9', 'text_emb_10', 'text_emb_11', 'text_emb_12', 'text_emb_13', 'text_emb_14', 'text_emb_15', 'text_emb_16', 'text_emb_17', 'text_emb_18', 'text_emb_19', 'text_emb_20', 'text_emb_21', 'text_emb_22', 'text_emb_23', 'text_emb_24', 'text_emb_25', 'text_emb_26', 'text_emb_27', 'text_emb_28', 'text_emb_29', 'text_emb_30', 'text_emb_31', 'text_emb_32', 'text_emb_33', 'text_emb_34', 'text_emb_35', 'text_emb_36', 'text_emb_37', 'text_emb_38', 'text_emb_39', 'text_emb_40', 'text_emb_41', 'text_emb_42', 'text_emb_43', 'text_emb_44', 'text_emb_45', 'text_emb_46', 'text_emb_47', 'text_emb_48', 'text_emb_49', 'text_emb_50', 'text_emb_51', 'text_emb_52', 'text_emb_53', 'text_emb_54', 'text_emb_55', 'text_emb_56', 'text_emb_57', 'text_emb_58', 'text_emb_59', 'text_emb_60', 'text_emb_61', 'text_emb_62', 'text_emb_63', 'text_emb_64', 'text_emb_65', 'text_emb_66', 'text_emb_67', 'text_emb_68', 'text_emb_69', 'text_emb_70', 'text_emb_71', 'text_emb_72', 'text_emb_73', 'text_emb_74', 'text_emb_75', 'text_emb_76', 'text_emb_77', 'text_emb_78', 'text_emb_79', 'text_emb_80', 'text_emb_81', 'text_emb_82', 'text_emb_83', 'text_emb_84', 'text_emb_85', 'text_emb_86', 'text_emb_87', 'text_emb_88', 'text_emb_89', 'text_emb_90', 'text_emb_91', 'text_emb_92', 'text_emb_93', 'text_emb_94', 'text_emb_95', 'text_emb_96', 'text_emb_97', 'text_emb_98', 'text_emb_99', 'text_emb_100', 'text_emb_101', 'text_emb_102', 'text_emb_103', 'text_emb_104', 'text_emb_105', 'text_emb_106', 'text_emb_107', 'text_emb_108', 'text_emb_109', 'text_emb_110', 'text_emb_111', 'text_emb_112', 'text_emb_113', 'text_emb_114', 'text_emb_115', 'text_emb_116', 'text_emb_117', 'text_emb_118', 'text_emb_119', 'text_emb_120', 'text_emb_121', 'text_emb_122', 'text_emb_123', 'text_emb_124', 'text_emb_125', 'text_emb_126', 'text_emb_127', 'text_emb_128', 'text_emb_129', 'text_emb_130', 'text_emb_131', 'text_emb_132', 'text_emb_133', 'text_emb_134', 'text_emb_135', 'text_emb_136', 'text_emb_137', 'text_emb_138', 'text_emb_139', 'text_emb_140', 'text_emb_141', 'text_emb_142', 'text_emb_143', 'text_emb_144', 'text_emb_145', 'text_emb_146', 'text_emb_147', 'text_emb_148', 'text_emb_149', 'text_emb_150', 'text_emb_151', 'text_emb_152', 'text_emb_153', 'text_emb_154', 'text_emb_155', 'text_emb_156', 'text_emb_157', 'text_emb_158', 'text_emb_159', 'text_emb_160', 'text_emb_161', 'text_emb_162', 'text_emb_163', 'text_emb_164', 'text_emb_165', 'text_emb_166', 'text_emb_167', 'text_emb_168', 'text_emb_169', 'text_emb_170', 'text_emb_171', 'text_emb_172', 'text_emb_173', 'text_emb_174', 'text_emb_175', 'text_emb_176', 'text_emb_177', 'text_emb_178', 'text_emb_179', 'text_emb_180', 'text_emb_181', 'text_emb_182', 'text_emb_183', 'text_emb_184', 'text_emb_185', 'text_emb_186', 'text_emb_187', 'text_emb_188', 'text_emb_189', 'text_emb_190', 'text_emb_191', 'text_emb_192', 'text_emb_193', 'text_emb_194', 'text_emb_195', 'text_emb_196', 'text_emb_197', 'text_emb_198', 'text_emb_199', 'text_emb_200', 'text_emb_201', 'text_emb_202', 'text_emb_203', 'text_emb_204', 'text_emb_205', 'text_emb_206', 'text_emb_207', 'text_emb_208', 'text_emb_209', 'text_emb_210', 'text_emb_211', 'text_emb_212', 'text_emb_213', 'text_emb_214', 'text_emb_215', 'text_emb_216', 'text_emb_217', 'text_emb_218', 'text_emb_219', 'text_emb_220', 'text_emb_221', 'text_emb_222', 'text_emb_223', 'text_emb_224', 'text_emb_225', 'text_emb_226', 'text_emb_227', 'text_emb_228', 'text_emb_229', 'text_emb_230', 'text_emb_231', 'text_emb_232', 'text_emb_233', 'text_emb_234', 'text_emb_235', 'text_emb_236', 'text_emb_237', 'text_emb_238', 'text_emb_239', 'text_emb_240', 'text_emb_241', 'text_emb_242', 'text_emb_243', 'text_emb_244', 'text_emb_245', 'text_emb_246', 'text_emb_247', 'text_emb_248', 'text_emb_249', 'text_emb_250', 'text_emb_251', 'text_emb_252', 'text_emb_253', 'text_emb_254', 'text_emb_255', 'text_emb_256', 'text_emb_257', 'text_emb_258', 'text_emb_259', 'text_emb_260', 'text_emb_261', 'text_emb_262', 'text_emb_263', 'text_emb_264', 'text_emb_265', 'text_emb_266', 'text_emb_267', 'text_emb_268', 'text_emb_269', 'text_emb_270', 'text_emb_271', 'text_emb_272', 'text_emb_273', 'text_emb_274', 'text_emb_275', 'text_emb_276', 'text_emb_277', 'text_emb_278', 'text_emb_279', 'text_emb_280', 'text_emb_281', 'text_emb_282', 'text_emb_283', 'text_emb_284', 'text_emb_285', 'text_emb_286', 'text_emb_287', 'text_emb_288', 'text_emb_289', 'text_emb_290', 'text_emb_291', 'text_emb_292', 'text_emb_293', 'text_emb_294', 'text_emb_295', 'text_emb_296', 'text_emb_297', 'text_emb_298', 'text_emb_299', 'text_emb_300', 'text_emb_301', 'text_emb_302', 'text_emb_303', 'text_emb_304', 'text_emb_305', 'text_emb_306', 'text_emb_307', 'text_emb_308', 'text_emb_309', 'text_emb_310', 'text_emb_311', 'text_emb_312', 'text_emb_313', 'text_emb_314', 'text_emb_315', 'text_emb_316', 'text_emb_317', 'text_emb_318', 'text_emb_319', 'text_emb_320', 'text_emb_321', 'text_emb_322', 'text_emb_323', 'text_emb_324', 'text_emb_325', 'text_emb_326', 'text_emb_327', 'text_emb_328', 'text_emb_329', 'text_emb_330', 'text_emb_331', 'text_emb_332', 'text_emb_333', 'text_emb_334', 'text_emb_335', 'text_emb_336', 'text_emb_337', 'text_emb_338', 'text_emb_339', 'text_emb_340', 'text_emb_341', 'text_emb_342', 'text_emb_343', 'text_emb_344', 'text_emb_345', 'text_emb_346', 'text_emb_347', 'text_emb_348', 'text_emb_349', 'text_emb_350', 'text_emb_351', 'text_emb_352', 'text_emb_353', 'text_emb_354', 'text_emb_355', 'text_emb_356', 'text_emb_357', 'text_emb_358', 'text_emb_359', 'text_emb_360', 'text_emb_361', 'text_emb_362', 'text_emb_363', 'text_emb_364', 'text_emb_365', 'text_emb_366', 'text_emb_367', 'text_emb_368', 'text_emb_369', 'text_emb_370', 'text_emb_371', 'text_emb_372', 'text_emb_373', 'text_emb_374', 'text_emb_375', 'text_emb_376', 'text_emb_377', 'text_emb_378', 'text_emb_379', 'text_emb_380', 'text_emb_381', 'text_emb_382', 'text_emb_383', 'Narration_Cluster']\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "=== AFTER ADDING Narration_Cluster_Label ===\n",
      "Total columns: 417\n",
      "Column names: ['S. No', 'Entity Name', 'Accounting Date', 'Approval Type', 'Document Type', 'Invoice Date', 'Day', 'Nature', 'Account Code', 'PL/ BS', 'Report Group', 'Account Name', 'Nature in balance sheet', 'Document Number', 'Je Line Num', 'Source Number', 'Source Name', 'Source Voucher Name', 'Source Desc', 'Line Desc', 'Project Code', 'Internal Reference', 'Posted Date', 'Branch', 'Batch Name', 'Entered Dr SUM', 'Entered Cr SUM', 'Accounted Dr SUM', 'Accounted Cr SUM', 'Net Amount', 'Combined_Text', 'text_emb_0', 'text_emb_1', 'text_emb_2', 'text_emb_3', 'text_emb_4', 'text_emb_5', 'text_emb_6', 'text_emb_7', 'text_emb_8', 'text_emb_9', 'text_emb_10', 'text_emb_11', 'text_emb_12', 'text_emb_13', 'text_emb_14', 'text_emb_15', 'text_emb_16', 'text_emb_17', 'text_emb_18', 'text_emb_19', 'text_emb_20', 'text_emb_21', 'text_emb_22', 'text_emb_23', 'text_emb_24', 'text_emb_25', 'text_emb_26', 'text_emb_27', 'text_emb_28', 'text_emb_29', 'text_emb_30', 'text_emb_31', 'text_emb_32', 'text_emb_33', 'text_emb_34', 'text_emb_35', 'text_emb_36', 'text_emb_37', 'text_emb_38', 'text_emb_39', 'text_emb_40', 'text_emb_41', 'text_emb_42', 'text_emb_43', 'text_emb_44', 'text_emb_45', 'text_emb_46', 'text_emb_47', 'text_emb_48', 'text_emb_49', 'text_emb_50', 'text_emb_51', 'text_emb_52', 'text_emb_53', 'text_emb_54', 'text_emb_55', 'text_emb_56', 'text_emb_57', 'text_emb_58', 'text_emb_59', 'text_emb_60', 'text_emb_61', 'text_emb_62', 'text_emb_63', 'text_emb_64', 'text_emb_65', 'text_emb_66', 'text_emb_67', 'text_emb_68', 'text_emb_69', 'text_emb_70', 'text_emb_71', 'text_emb_72', 'text_emb_73', 'text_emb_74', 'text_emb_75', 'text_emb_76', 'text_emb_77', 'text_emb_78', 'text_emb_79', 'text_emb_80', 'text_emb_81', 'text_emb_82', 'text_emb_83', 'text_emb_84', 'text_emb_85', 'text_emb_86', 'text_emb_87', 'text_emb_88', 'text_emb_89', 'text_emb_90', 'text_emb_91', 'text_emb_92', 'text_emb_93', 'text_emb_94', 'text_emb_95', 'text_emb_96', 'text_emb_97', 'text_emb_98', 'text_emb_99', 'text_emb_100', 'text_emb_101', 'text_emb_102', 'text_emb_103', 'text_emb_104', 'text_emb_105', 'text_emb_106', 'text_emb_107', 'text_emb_108', 'text_emb_109', 'text_emb_110', 'text_emb_111', 'text_emb_112', 'text_emb_113', 'text_emb_114', 'text_emb_115', 'text_emb_116', 'text_emb_117', 'text_emb_118', 'text_emb_119', 'text_emb_120', 'text_emb_121', 'text_emb_122', 'text_emb_123', 'text_emb_124', 'text_emb_125', 'text_emb_126', 'text_emb_127', 'text_emb_128', 'text_emb_129', 'text_emb_130', 'text_emb_131', 'text_emb_132', 'text_emb_133', 'text_emb_134', 'text_emb_135', 'text_emb_136', 'text_emb_137', 'text_emb_138', 'text_emb_139', 'text_emb_140', 'text_emb_141', 'text_emb_142', 'text_emb_143', 'text_emb_144', 'text_emb_145', 'text_emb_146', 'text_emb_147', 'text_emb_148', 'text_emb_149', 'text_emb_150', 'text_emb_151', 'text_emb_152', 'text_emb_153', 'text_emb_154', 'text_emb_155', 'text_emb_156', 'text_emb_157', 'text_emb_158', 'text_emb_159', 'text_emb_160', 'text_emb_161', 'text_emb_162', 'text_emb_163', 'text_emb_164', 'text_emb_165', 'text_emb_166', 'text_emb_167', 'text_emb_168', 'text_emb_169', 'text_emb_170', 'text_emb_171', 'text_emb_172', 'text_emb_173', 'text_emb_174', 'text_emb_175', 'text_emb_176', 'text_emb_177', 'text_emb_178', 'text_emb_179', 'text_emb_180', 'text_emb_181', 'text_emb_182', 'text_emb_183', 'text_emb_184', 'text_emb_185', 'text_emb_186', 'text_emb_187', 'text_emb_188', 'text_emb_189', 'text_emb_190', 'text_emb_191', 'text_emb_192', 'text_emb_193', 'text_emb_194', 'text_emb_195', 'text_emb_196', 'text_emb_197', 'text_emb_198', 'text_emb_199', 'text_emb_200', 'text_emb_201', 'text_emb_202', 'text_emb_203', 'text_emb_204', 'text_emb_205', 'text_emb_206', 'text_emb_207', 'text_emb_208', 'text_emb_209', 'text_emb_210', 'text_emb_211', 'text_emb_212', 'text_emb_213', 'text_emb_214', 'text_emb_215', 'text_emb_216', 'text_emb_217', 'text_emb_218', 'text_emb_219', 'text_emb_220', 'text_emb_221', 'text_emb_222', 'text_emb_223', 'text_emb_224', 'text_emb_225', 'text_emb_226', 'text_emb_227', 'text_emb_228', 'text_emb_229', 'text_emb_230', 'text_emb_231', 'text_emb_232', 'text_emb_233', 'text_emb_234', 'text_emb_235', 'text_emb_236', 'text_emb_237', 'text_emb_238', 'text_emb_239', 'text_emb_240', 'text_emb_241', 'text_emb_242', 'text_emb_243', 'text_emb_244', 'text_emb_245', 'text_emb_246', 'text_emb_247', 'text_emb_248', 'text_emb_249', 'text_emb_250', 'text_emb_251', 'text_emb_252', 'text_emb_253', 'text_emb_254', 'text_emb_255', 'text_emb_256', 'text_emb_257', 'text_emb_258', 'text_emb_259', 'text_emb_260', 'text_emb_261', 'text_emb_262', 'text_emb_263', 'text_emb_264', 'text_emb_265', 'text_emb_266', 'text_emb_267', 'text_emb_268', 'text_emb_269', 'text_emb_270', 'text_emb_271', 'text_emb_272', 'text_emb_273', 'text_emb_274', 'text_emb_275', 'text_emb_276', 'text_emb_277', 'text_emb_278', 'text_emb_279', 'text_emb_280', 'text_emb_281', 'text_emb_282', 'text_emb_283', 'text_emb_284', 'text_emb_285', 'text_emb_286', 'text_emb_287', 'text_emb_288', 'text_emb_289', 'text_emb_290', 'text_emb_291', 'text_emb_292', 'text_emb_293', 'text_emb_294', 'text_emb_295', 'text_emb_296', 'text_emb_297', 'text_emb_298', 'text_emb_299', 'text_emb_300', 'text_emb_301', 'text_emb_302', 'text_emb_303', 'text_emb_304', 'text_emb_305', 'text_emb_306', 'text_emb_307', 'text_emb_308', 'text_emb_309', 'text_emb_310', 'text_emb_311', 'text_emb_312', 'text_emb_313', 'text_emb_314', 'text_emb_315', 'text_emb_316', 'text_emb_317', 'text_emb_318', 'text_emb_319', 'text_emb_320', 'text_emb_321', 'text_emb_322', 'text_emb_323', 'text_emb_324', 'text_emb_325', 'text_emb_326', 'text_emb_327', 'text_emb_328', 'text_emb_329', 'text_emb_330', 'text_emb_331', 'text_emb_332', 'text_emb_333', 'text_emb_334', 'text_emb_335', 'text_emb_336', 'text_emb_337', 'text_emb_338', 'text_emb_339', 'text_emb_340', 'text_emb_341', 'text_emb_342', 'text_emb_343', 'text_emb_344', 'text_emb_345', 'text_emb_346', 'text_emb_347', 'text_emb_348', 'text_emb_349', 'text_emb_350', 'text_emb_351', 'text_emb_352', 'text_emb_353', 'text_emb_354', 'text_emb_355', 'text_emb_356', 'text_emb_357', 'text_emb_358', 'text_emb_359', 'text_emb_360', 'text_emb_361', 'text_emb_362', 'text_emb_363', 'text_emb_364', 'text_emb_365', 'text_emb_366', 'text_emb_367', 'text_emb_368', 'text_emb_369', 'text_emb_370', 'text_emb_371', 'text_emb_372', 'text_emb_373', 'text_emb_374', 'text_emb_375', 'text_emb_376', 'text_emb_377', 'text_emb_378', 'text_emb_379', 'text_emb_380', 'text_emb_381', 'text_emb_382', 'text_emb_383', 'Narration_Cluster', 'Narration_Cluster_Label']\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "=== AFTER ADDING DATE FEATURES ===\n",
      "Total columns: 423\n",
      "Column names: ['S. No', 'Entity Name', 'Accounting Date', 'Approval Type', 'Document Type', 'Invoice Date', 'Day', 'Nature', 'Account Code', 'PL/ BS', 'Report Group', 'Account Name', 'Nature in balance sheet', 'Document Number', 'Je Line Num', 'Source Number', 'Source Name', 'Source Voucher Name', 'Source Desc', 'Line Desc', 'Project Code', 'Internal Reference', 'Posted Date', 'Branch', 'Batch Name', 'Entered Dr SUM', 'Entered Cr SUM', 'Accounted Dr SUM', 'Accounted Cr SUM', 'Net Amount', 'Combined_Text', 'text_emb_0', 'text_emb_1', 'text_emb_2', 'text_emb_3', 'text_emb_4', 'text_emb_5', 'text_emb_6', 'text_emb_7', 'text_emb_8', 'text_emb_9', 'text_emb_10', 'text_emb_11', 'text_emb_12', 'text_emb_13', 'text_emb_14', 'text_emb_15', 'text_emb_16', 'text_emb_17', 'text_emb_18', 'text_emb_19', 'text_emb_20', 'text_emb_21', 'text_emb_22', 'text_emb_23', 'text_emb_24', 'text_emb_25', 'text_emb_26', 'text_emb_27', 'text_emb_28', 'text_emb_29', 'text_emb_30', 'text_emb_31', 'text_emb_32', 'text_emb_33', 'text_emb_34', 'text_emb_35', 'text_emb_36', 'text_emb_37', 'text_emb_38', 'text_emb_39', 'text_emb_40', 'text_emb_41', 'text_emb_42', 'text_emb_43', 'text_emb_44', 'text_emb_45', 'text_emb_46', 'text_emb_47', 'text_emb_48', 'text_emb_49', 'text_emb_50', 'text_emb_51', 'text_emb_52', 'text_emb_53', 'text_emb_54', 'text_emb_55', 'text_emb_56', 'text_emb_57', 'text_emb_58', 'text_emb_59', 'text_emb_60', 'text_emb_61', 'text_emb_62', 'text_emb_63', 'text_emb_64', 'text_emb_65', 'text_emb_66', 'text_emb_67', 'text_emb_68', 'text_emb_69', 'text_emb_70', 'text_emb_71', 'text_emb_72', 'text_emb_73', 'text_emb_74', 'text_emb_75', 'text_emb_76', 'text_emb_77', 'text_emb_78', 'text_emb_79', 'text_emb_80', 'text_emb_81', 'text_emb_82', 'text_emb_83', 'text_emb_84', 'text_emb_85', 'text_emb_86', 'text_emb_87', 'text_emb_88', 'text_emb_89', 'text_emb_90', 'text_emb_91', 'text_emb_92', 'text_emb_93', 'text_emb_94', 'text_emb_95', 'text_emb_96', 'text_emb_97', 'text_emb_98', 'text_emb_99', 'text_emb_100', 'text_emb_101', 'text_emb_102', 'text_emb_103', 'text_emb_104', 'text_emb_105', 'text_emb_106', 'text_emb_107', 'text_emb_108', 'text_emb_109', 'text_emb_110', 'text_emb_111', 'text_emb_112', 'text_emb_113', 'text_emb_114', 'text_emb_115', 'text_emb_116', 'text_emb_117', 'text_emb_118', 'text_emb_119', 'text_emb_120', 'text_emb_121', 'text_emb_122', 'text_emb_123', 'text_emb_124', 'text_emb_125', 'text_emb_126', 'text_emb_127', 'text_emb_128', 'text_emb_129', 'text_emb_130', 'text_emb_131', 'text_emb_132', 'text_emb_133', 'text_emb_134', 'text_emb_135', 'text_emb_136', 'text_emb_137', 'text_emb_138', 'text_emb_139', 'text_emb_140', 'text_emb_141', 'text_emb_142', 'text_emb_143', 'text_emb_144', 'text_emb_145', 'text_emb_146', 'text_emb_147', 'text_emb_148', 'text_emb_149', 'text_emb_150', 'text_emb_151', 'text_emb_152', 'text_emb_153', 'text_emb_154', 'text_emb_155', 'text_emb_156', 'text_emb_157', 'text_emb_158', 'text_emb_159', 'text_emb_160', 'text_emb_161', 'text_emb_162', 'text_emb_163', 'text_emb_164', 'text_emb_165', 'text_emb_166', 'text_emb_167', 'text_emb_168', 'text_emb_169', 'text_emb_170', 'text_emb_171', 'text_emb_172', 'text_emb_173', 'text_emb_174', 'text_emb_175', 'text_emb_176', 'text_emb_177', 'text_emb_178', 'text_emb_179', 'text_emb_180', 'text_emb_181', 'text_emb_182', 'text_emb_183', 'text_emb_184', 'text_emb_185', 'text_emb_186', 'text_emb_187', 'text_emb_188', 'text_emb_189', 'text_emb_190', 'text_emb_191', 'text_emb_192', 'text_emb_193', 'text_emb_194', 'text_emb_195', 'text_emb_196', 'text_emb_197', 'text_emb_198', 'text_emb_199', 'text_emb_200', 'text_emb_201', 'text_emb_202', 'text_emb_203', 'text_emb_204', 'text_emb_205', 'text_emb_206', 'text_emb_207', 'text_emb_208', 'text_emb_209', 'text_emb_210', 'text_emb_211', 'text_emb_212', 'text_emb_213', 'text_emb_214', 'text_emb_215', 'text_emb_216', 'text_emb_217', 'text_emb_218', 'text_emb_219', 'text_emb_220', 'text_emb_221', 'text_emb_222', 'text_emb_223', 'text_emb_224', 'text_emb_225', 'text_emb_226', 'text_emb_227', 'text_emb_228', 'text_emb_229', 'text_emb_230', 'text_emb_231', 'text_emb_232', 'text_emb_233', 'text_emb_234', 'text_emb_235', 'text_emb_236', 'text_emb_237', 'text_emb_238', 'text_emb_239', 'text_emb_240', 'text_emb_241', 'text_emb_242', 'text_emb_243', 'text_emb_244', 'text_emb_245', 'text_emb_246', 'text_emb_247', 'text_emb_248', 'text_emb_249', 'text_emb_250', 'text_emb_251', 'text_emb_252', 'text_emb_253', 'text_emb_254', 'text_emb_255', 'text_emb_256', 'text_emb_257', 'text_emb_258', 'text_emb_259', 'text_emb_260', 'text_emb_261', 'text_emb_262', 'text_emb_263', 'text_emb_264', 'text_emb_265', 'text_emb_266', 'text_emb_267', 'text_emb_268', 'text_emb_269', 'text_emb_270', 'text_emb_271', 'text_emb_272', 'text_emb_273', 'text_emb_274', 'text_emb_275', 'text_emb_276', 'text_emb_277', 'text_emb_278', 'text_emb_279', 'text_emb_280', 'text_emb_281', 'text_emb_282', 'text_emb_283', 'text_emb_284', 'text_emb_285', 'text_emb_286', 'text_emb_287', 'text_emb_288', 'text_emb_289', 'text_emb_290', 'text_emb_291', 'text_emb_292', 'text_emb_293', 'text_emb_294', 'text_emb_295', 'text_emb_296', 'text_emb_297', 'text_emb_298', 'text_emb_299', 'text_emb_300', 'text_emb_301', 'text_emb_302', 'text_emb_303', 'text_emb_304', 'text_emb_305', 'text_emb_306', 'text_emb_307', 'text_emb_308', 'text_emb_309', 'text_emb_310', 'text_emb_311', 'text_emb_312', 'text_emb_313', 'text_emb_314', 'text_emb_315', 'text_emb_316', 'text_emb_317', 'text_emb_318', 'text_emb_319', 'text_emb_320', 'text_emb_321', 'text_emb_322', 'text_emb_323', 'text_emb_324', 'text_emb_325', 'text_emb_326', 'text_emb_327', 'text_emb_328', 'text_emb_329', 'text_emb_330', 'text_emb_331', 'text_emb_332', 'text_emb_333', 'text_emb_334', 'text_emb_335', 'text_emb_336', 'text_emb_337', 'text_emb_338', 'text_emb_339', 'text_emb_340', 'text_emb_341', 'text_emb_342', 'text_emb_343', 'text_emb_344', 'text_emb_345', 'text_emb_346', 'text_emb_347', 'text_emb_348', 'text_emb_349', 'text_emb_350', 'text_emb_351', 'text_emb_352', 'text_emb_353', 'text_emb_354', 'text_emb_355', 'text_emb_356', 'text_emb_357', 'text_emb_358', 'text_emb_359', 'text_emb_360', 'text_emb_361', 'text_emb_362', 'text_emb_363', 'text_emb_364', 'text_emb_365', 'text_emb_366', 'text_emb_367', 'text_emb_368', 'text_emb_369', 'text_emb_370', 'text_emb_371', 'text_emb_372', 'text_emb_373', 'text_emb_374', 'text_emb_375', 'text_emb_376', 'text_emb_377', 'text_emb_378', 'text_emb_379', 'text_emb_380', 'text_emb_381', 'text_emb_382', 'text_emb_383', 'Narration_Cluster', 'Narration_Cluster_Label', 'Accounting_Month', 'Accounting_Weekday', 'Invoice_Month', 'Invoice_Weekday', 'Posted_Month', 'Posted_Weekday']\n",
      "--------------------------------------------------------------------------------\n",
      "Feature columns for model: 416\n",
      "\n",
      "=== AFTER ADDING MODEL SCORES ===\n",
      "Total columns: 425\n",
      "Column names: ['S. No', 'Entity Name', 'Accounting Date', 'Approval Type', 'Document Type', 'Invoice Date', 'Day', 'Nature', 'Account Code', 'PL/ BS', 'Report Group', 'Account Name', 'Nature in balance sheet', 'Document Number', 'Je Line Num', 'Source Number', 'Source Name', 'Source Voucher Name', 'Source Desc', 'Line Desc', 'Project Code', 'Internal Reference', 'Posted Date', 'Branch', 'Batch Name', 'Entered Dr SUM', 'Entered Cr SUM', 'Accounted Dr SUM', 'Accounted Cr SUM', 'Net Amount', 'Combined_Text', 'text_emb_0', 'text_emb_1', 'text_emb_2', 'text_emb_3', 'text_emb_4', 'text_emb_5', 'text_emb_6', 'text_emb_7', 'text_emb_8', 'text_emb_9', 'text_emb_10', 'text_emb_11', 'text_emb_12', 'text_emb_13', 'text_emb_14', 'text_emb_15', 'text_emb_16', 'text_emb_17', 'text_emb_18', 'text_emb_19', 'text_emb_20', 'text_emb_21', 'text_emb_22', 'text_emb_23', 'text_emb_24', 'text_emb_25', 'text_emb_26', 'text_emb_27', 'text_emb_28', 'text_emb_29', 'text_emb_30', 'text_emb_31', 'text_emb_32', 'text_emb_33', 'text_emb_34', 'text_emb_35', 'text_emb_36', 'text_emb_37', 'text_emb_38', 'text_emb_39', 'text_emb_40', 'text_emb_41', 'text_emb_42', 'text_emb_43', 'text_emb_44', 'text_emb_45', 'text_emb_46', 'text_emb_47', 'text_emb_48', 'text_emb_49', 'text_emb_50', 'text_emb_51', 'text_emb_52', 'text_emb_53', 'text_emb_54', 'text_emb_55', 'text_emb_56', 'text_emb_57', 'text_emb_58', 'text_emb_59', 'text_emb_60', 'text_emb_61', 'text_emb_62', 'text_emb_63', 'text_emb_64', 'text_emb_65', 'text_emb_66', 'text_emb_67', 'text_emb_68', 'text_emb_69', 'text_emb_70', 'text_emb_71', 'text_emb_72', 'text_emb_73', 'text_emb_74', 'text_emb_75', 'text_emb_76', 'text_emb_77', 'text_emb_78', 'text_emb_79', 'text_emb_80', 'text_emb_81', 'text_emb_82', 'text_emb_83', 'text_emb_84', 'text_emb_85', 'text_emb_86', 'text_emb_87', 'text_emb_88', 'text_emb_89', 'text_emb_90', 'text_emb_91', 'text_emb_92', 'text_emb_93', 'text_emb_94', 'text_emb_95', 'text_emb_96', 'text_emb_97', 'text_emb_98', 'text_emb_99', 'text_emb_100', 'text_emb_101', 'text_emb_102', 'text_emb_103', 'text_emb_104', 'text_emb_105', 'text_emb_106', 'text_emb_107', 'text_emb_108', 'text_emb_109', 'text_emb_110', 'text_emb_111', 'text_emb_112', 'text_emb_113', 'text_emb_114', 'text_emb_115', 'text_emb_116', 'text_emb_117', 'text_emb_118', 'text_emb_119', 'text_emb_120', 'text_emb_121', 'text_emb_122', 'text_emb_123', 'text_emb_124', 'text_emb_125', 'text_emb_126', 'text_emb_127', 'text_emb_128', 'text_emb_129', 'text_emb_130', 'text_emb_131', 'text_emb_132', 'text_emb_133', 'text_emb_134', 'text_emb_135', 'text_emb_136', 'text_emb_137', 'text_emb_138', 'text_emb_139', 'text_emb_140', 'text_emb_141', 'text_emb_142', 'text_emb_143', 'text_emb_144', 'text_emb_145', 'text_emb_146', 'text_emb_147', 'text_emb_148', 'text_emb_149', 'text_emb_150', 'text_emb_151', 'text_emb_152', 'text_emb_153', 'text_emb_154', 'text_emb_155', 'text_emb_156', 'text_emb_157', 'text_emb_158', 'text_emb_159', 'text_emb_160', 'text_emb_161', 'text_emb_162', 'text_emb_163', 'text_emb_164', 'text_emb_165', 'text_emb_166', 'text_emb_167', 'text_emb_168', 'text_emb_169', 'text_emb_170', 'text_emb_171', 'text_emb_172', 'text_emb_173', 'text_emb_174', 'text_emb_175', 'text_emb_176', 'text_emb_177', 'text_emb_178', 'text_emb_179', 'text_emb_180', 'text_emb_181', 'text_emb_182', 'text_emb_183', 'text_emb_184', 'text_emb_185', 'text_emb_186', 'text_emb_187', 'text_emb_188', 'text_emb_189', 'text_emb_190', 'text_emb_191', 'text_emb_192', 'text_emb_193', 'text_emb_194', 'text_emb_195', 'text_emb_196', 'text_emb_197', 'text_emb_198', 'text_emb_199', 'text_emb_200', 'text_emb_201', 'text_emb_202', 'text_emb_203', 'text_emb_204', 'text_emb_205', 'text_emb_206', 'text_emb_207', 'text_emb_208', 'text_emb_209', 'text_emb_210', 'text_emb_211', 'text_emb_212', 'text_emb_213', 'text_emb_214', 'text_emb_215', 'text_emb_216', 'text_emb_217', 'text_emb_218', 'text_emb_219', 'text_emb_220', 'text_emb_221', 'text_emb_222', 'text_emb_223', 'text_emb_224', 'text_emb_225', 'text_emb_226', 'text_emb_227', 'text_emb_228', 'text_emb_229', 'text_emb_230', 'text_emb_231', 'text_emb_232', 'text_emb_233', 'text_emb_234', 'text_emb_235', 'text_emb_236', 'text_emb_237', 'text_emb_238', 'text_emb_239', 'text_emb_240', 'text_emb_241', 'text_emb_242', 'text_emb_243', 'text_emb_244', 'text_emb_245', 'text_emb_246', 'text_emb_247', 'text_emb_248', 'text_emb_249', 'text_emb_250', 'text_emb_251', 'text_emb_252', 'text_emb_253', 'text_emb_254', 'text_emb_255', 'text_emb_256', 'text_emb_257', 'text_emb_258', 'text_emb_259', 'text_emb_260', 'text_emb_261', 'text_emb_262', 'text_emb_263', 'text_emb_264', 'text_emb_265', 'text_emb_266', 'text_emb_267', 'text_emb_268', 'text_emb_269', 'text_emb_270', 'text_emb_271', 'text_emb_272', 'text_emb_273', 'text_emb_274', 'text_emb_275', 'text_emb_276', 'text_emb_277', 'text_emb_278', 'text_emb_279', 'text_emb_280', 'text_emb_281', 'text_emb_282', 'text_emb_283', 'text_emb_284', 'text_emb_285', 'text_emb_286', 'text_emb_287', 'text_emb_288', 'text_emb_289', 'text_emb_290', 'text_emb_291', 'text_emb_292', 'text_emb_293', 'text_emb_294', 'text_emb_295', 'text_emb_296', 'text_emb_297', 'text_emb_298', 'text_emb_299', 'text_emb_300', 'text_emb_301', 'text_emb_302', 'text_emb_303', 'text_emb_304', 'text_emb_305', 'text_emb_306', 'text_emb_307', 'text_emb_308', 'text_emb_309', 'text_emb_310', 'text_emb_311', 'text_emb_312', 'text_emb_313', 'text_emb_314', 'text_emb_315', 'text_emb_316', 'text_emb_317', 'text_emb_318', 'text_emb_319', 'text_emb_320', 'text_emb_321', 'text_emb_322', 'text_emb_323', 'text_emb_324', 'text_emb_325', 'text_emb_326', 'text_emb_327', 'text_emb_328', 'text_emb_329', 'text_emb_330', 'text_emb_331', 'text_emb_332', 'text_emb_333', 'text_emb_334', 'text_emb_335', 'text_emb_336', 'text_emb_337', 'text_emb_338', 'text_emb_339', 'text_emb_340', 'text_emb_341', 'text_emb_342', 'text_emb_343', 'text_emb_344', 'text_emb_345', 'text_emb_346', 'text_emb_347', 'text_emb_348', 'text_emb_349', 'text_emb_350', 'text_emb_351', 'text_emb_352', 'text_emb_353', 'text_emb_354', 'text_emb_355', 'text_emb_356', 'text_emb_357', 'text_emb_358', 'text_emb_359', 'text_emb_360', 'text_emb_361', 'text_emb_362', 'text_emb_363', 'text_emb_364', 'text_emb_365', 'text_emb_366', 'text_emb_367', 'text_emb_368', 'text_emb_369', 'text_emb_370', 'text_emb_371', 'text_emb_372', 'text_emb_373', 'text_emb_374', 'text_emb_375', 'text_emb_376', 'text_emb_377', 'text_emb_378', 'text_emb_379', 'text_emb_380', 'text_emb_381', 'text_emb_382', 'text_emb_383', 'Narration_Cluster', 'Narration_Cluster_Label', 'Accounting_Month', 'Accounting_Weekday', 'Invoice_Month', 'Invoice_Weekday', 'Posted_Month', 'Posted_Weekday', 'Model_Score', 'Final_Score']\n",
      "--------------------------------------------------------------------------------\n",
      "SHAP values calculated for 874 transactions\n",
      "Applying Control Points...\n",
      "\n",
      "=== AFTER ADDING CP_01 ===\n",
      "Total columns: 426\n",
      "Column names: ['S. No', 'Entity Name', 'Accounting Date', 'Approval Type', 'Document Type', 'Invoice Date', 'Day', 'Nature', 'Account Code', 'PL/ BS', 'Report Group', 'Account Name', 'Nature in balance sheet', 'Document Number', 'Je Line Num', 'Source Number', 'Source Name', 'Source Voucher Name', 'Source Desc', 'Line Desc', 'Project Code', 'Internal Reference', 'Posted Date', 'Branch', 'Batch Name', 'Entered Dr SUM', 'Entered Cr SUM', 'Accounted Dr SUM', 'Accounted Cr SUM', 'Net Amount', 'Combined_Text', 'text_emb_0', 'text_emb_1', 'text_emb_2', 'text_emb_3', 'text_emb_4', 'text_emb_5', 'text_emb_6', 'text_emb_7', 'text_emb_8', 'text_emb_9', 'text_emb_10', 'text_emb_11', 'text_emb_12', 'text_emb_13', 'text_emb_14', 'text_emb_15', 'text_emb_16', 'text_emb_17', 'text_emb_18', 'text_emb_19', 'text_emb_20', 'text_emb_21', 'text_emb_22', 'text_emb_23', 'text_emb_24', 'text_emb_25', 'text_emb_26', 'text_emb_27', 'text_emb_28', 'text_emb_29', 'text_emb_30', 'text_emb_31', 'text_emb_32', 'text_emb_33', 'text_emb_34', 'text_emb_35', 'text_emb_36', 'text_emb_37', 'text_emb_38', 'text_emb_39', 'text_emb_40', 'text_emb_41', 'text_emb_42', 'text_emb_43', 'text_emb_44', 'text_emb_45', 'text_emb_46', 'text_emb_47', 'text_emb_48', 'text_emb_49', 'text_emb_50', 'text_emb_51', 'text_emb_52', 'text_emb_53', 'text_emb_54', 'text_emb_55', 'text_emb_56', 'text_emb_57', 'text_emb_58', 'text_emb_59', 'text_emb_60', 'text_emb_61', 'text_emb_62', 'text_emb_63', 'text_emb_64', 'text_emb_65', 'text_emb_66', 'text_emb_67', 'text_emb_68', 'text_emb_69', 'text_emb_70', 'text_emb_71', 'text_emb_72', 'text_emb_73', 'text_emb_74', 'text_emb_75', 'text_emb_76', 'text_emb_77', 'text_emb_78', 'text_emb_79', 'text_emb_80', 'text_emb_81', 'text_emb_82', 'text_emb_83', 'text_emb_84', 'text_emb_85', 'text_emb_86', 'text_emb_87', 'text_emb_88', 'text_emb_89', 'text_emb_90', 'text_emb_91', 'text_emb_92', 'text_emb_93', 'text_emb_94', 'text_emb_95', 'text_emb_96', 'text_emb_97', 'text_emb_98', 'text_emb_99', 'text_emb_100', 'text_emb_101', 'text_emb_102', 'text_emb_103', 'text_emb_104', 'text_emb_105', 'text_emb_106', 'text_emb_107', 'text_emb_108', 'text_emb_109', 'text_emb_110', 'text_emb_111', 'text_emb_112', 'text_emb_113', 'text_emb_114', 'text_emb_115', 'text_emb_116', 'text_emb_117', 'text_emb_118', 'text_emb_119', 'text_emb_120', 'text_emb_121', 'text_emb_122', 'text_emb_123', 'text_emb_124', 'text_emb_125', 'text_emb_126', 'text_emb_127', 'text_emb_128', 'text_emb_129', 'text_emb_130', 'text_emb_131', 'text_emb_132', 'text_emb_133', 'text_emb_134', 'text_emb_135', 'text_emb_136', 'text_emb_137', 'text_emb_138', 'text_emb_139', 'text_emb_140', 'text_emb_141', 'text_emb_142', 'text_emb_143', 'text_emb_144', 'text_emb_145', 'text_emb_146', 'text_emb_147', 'text_emb_148', 'text_emb_149', 'text_emb_150', 'text_emb_151', 'text_emb_152', 'text_emb_153', 'text_emb_154', 'text_emb_155', 'text_emb_156', 'text_emb_157', 'text_emb_158', 'text_emb_159', 'text_emb_160', 'text_emb_161', 'text_emb_162', 'text_emb_163', 'text_emb_164', 'text_emb_165', 'text_emb_166', 'text_emb_167', 'text_emb_168', 'text_emb_169', 'text_emb_170', 'text_emb_171', 'text_emb_172', 'text_emb_173', 'text_emb_174', 'text_emb_175', 'text_emb_176', 'text_emb_177', 'text_emb_178', 'text_emb_179', 'text_emb_180', 'text_emb_181', 'text_emb_182', 'text_emb_183', 'text_emb_184', 'text_emb_185', 'text_emb_186', 'text_emb_187', 'text_emb_188', 'text_emb_189', 'text_emb_190', 'text_emb_191', 'text_emb_192', 'text_emb_193', 'text_emb_194', 'text_emb_195', 'text_emb_196', 'text_emb_197', 'text_emb_198', 'text_emb_199', 'text_emb_200', 'text_emb_201', 'text_emb_202', 'text_emb_203', 'text_emb_204', 'text_emb_205', 'text_emb_206', 'text_emb_207', 'text_emb_208', 'text_emb_209', 'text_emb_210', 'text_emb_211', 'text_emb_212', 'text_emb_213', 'text_emb_214', 'text_emb_215', 'text_emb_216', 'text_emb_217', 'text_emb_218', 'text_emb_219', 'text_emb_220', 'text_emb_221', 'text_emb_222', 'text_emb_223', 'text_emb_224', 'text_emb_225', 'text_emb_226', 'text_emb_227', 'text_emb_228', 'text_emb_229', 'text_emb_230', 'text_emb_231', 'text_emb_232', 'text_emb_233', 'text_emb_234', 'text_emb_235', 'text_emb_236', 'text_emb_237', 'text_emb_238', 'text_emb_239', 'text_emb_240', 'text_emb_241', 'text_emb_242', 'text_emb_243', 'text_emb_244', 'text_emb_245', 'text_emb_246', 'text_emb_247', 'text_emb_248', 'text_emb_249', 'text_emb_250', 'text_emb_251', 'text_emb_252', 'text_emb_253', 'text_emb_254', 'text_emb_255', 'text_emb_256', 'text_emb_257', 'text_emb_258', 'text_emb_259', 'text_emb_260', 'text_emb_261', 'text_emb_262', 'text_emb_263', 'text_emb_264', 'text_emb_265', 'text_emb_266', 'text_emb_267', 'text_emb_268', 'text_emb_269', 'text_emb_270', 'text_emb_271', 'text_emb_272', 'text_emb_273', 'text_emb_274', 'text_emb_275', 'text_emb_276', 'text_emb_277', 'text_emb_278', 'text_emb_279', 'text_emb_280', 'text_emb_281', 'text_emb_282', 'text_emb_283', 'text_emb_284', 'text_emb_285', 'text_emb_286', 'text_emb_287', 'text_emb_288', 'text_emb_289', 'text_emb_290', 'text_emb_291', 'text_emb_292', 'text_emb_293', 'text_emb_294', 'text_emb_295', 'text_emb_296', 'text_emb_297', 'text_emb_298', 'text_emb_299', 'text_emb_300', 'text_emb_301', 'text_emb_302', 'text_emb_303', 'text_emb_304', 'text_emb_305', 'text_emb_306', 'text_emb_307', 'text_emb_308', 'text_emb_309', 'text_emb_310', 'text_emb_311', 'text_emb_312', 'text_emb_313', 'text_emb_314', 'text_emb_315', 'text_emb_316', 'text_emb_317', 'text_emb_318', 'text_emb_319', 'text_emb_320', 'text_emb_321', 'text_emb_322', 'text_emb_323', 'text_emb_324', 'text_emb_325', 'text_emb_326', 'text_emb_327', 'text_emb_328', 'text_emb_329', 'text_emb_330', 'text_emb_331', 'text_emb_332', 'text_emb_333', 'text_emb_334', 'text_emb_335', 'text_emb_336', 'text_emb_337', 'text_emb_338', 'text_emb_339', 'text_emb_340', 'text_emb_341', 'text_emb_342', 'text_emb_343', 'text_emb_344', 'text_emb_345', 'text_emb_346', 'text_emb_347', 'text_emb_348', 'text_emb_349', 'text_emb_350', 'text_emb_351', 'text_emb_352', 'text_emb_353', 'text_emb_354', 'text_emb_355', 'text_emb_356', 'text_emb_357', 'text_emb_358', 'text_emb_359', 'text_emb_360', 'text_emb_361', 'text_emb_362', 'text_emb_363', 'text_emb_364', 'text_emb_365', 'text_emb_366', 'text_emb_367', 'text_emb_368', 'text_emb_369', 'text_emb_370', 'text_emb_371', 'text_emb_372', 'text_emb_373', 'text_emb_374', 'text_emb_375', 'text_emb_376', 'text_emb_377', 'text_emb_378', 'text_emb_379', 'text_emb_380', 'text_emb_381', 'text_emb_382', 'text_emb_383', 'Narration_Cluster', 'Narration_Cluster_Label', 'Accounting_Month', 'Accounting_Weekday', 'Invoice_Month', 'Invoice_Weekday', 'Posted_Month', 'Posted_Weekday', 'Model_Score', 'Final_Score', 'CP_01']\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "=== AFTER ADDING CP_02 ===\n",
      "Total columns: 427\n",
      "Column names: ['S. No', 'Entity Name', 'Accounting Date', 'Approval Type', 'Document Type', 'Invoice Date', 'Day', 'Nature', 'Account Code', 'PL/ BS', 'Report Group', 'Account Name', 'Nature in balance sheet', 'Document Number', 'Je Line Num', 'Source Number', 'Source Name', 'Source Voucher Name', 'Source Desc', 'Line Desc', 'Project Code', 'Internal Reference', 'Posted Date', 'Branch', 'Batch Name', 'Entered Dr SUM', 'Entered Cr SUM', 'Accounted Dr SUM', 'Accounted Cr SUM', 'Net Amount', 'Combined_Text', 'text_emb_0', 'text_emb_1', 'text_emb_2', 'text_emb_3', 'text_emb_4', 'text_emb_5', 'text_emb_6', 'text_emb_7', 'text_emb_8', 'text_emb_9', 'text_emb_10', 'text_emb_11', 'text_emb_12', 'text_emb_13', 'text_emb_14', 'text_emb_15', 'text_emb_16', 'text_emb_17', 'text_emb_18', 'text_emb_19', 'text_emb_20', 'text_emb_21', 'text_emb_22', 'text_emb_23', 'text_emb_24', 'text_emb_25', 'text_emb_26', 'text_emb_27', 'text_emb_28', 'text_emb_29', 'text_emb_30', 'text_emb_31', 'text_emb_32', 'text_emb_33', 'text_emb_34', 'text_emb_35', 'text_emb_36', 'text_emb_37', 'text_emb_38', 'text_emb_39', 'text_emb_40', 'text_emb_41', 'text_emb_42', 'text_emb_43', 'text_emb_44', 'text_emb_45', 'text_emb_46', 'text_emb_47', 'text_emb_48', 'text_emb_49', 'text_emb_50', 'text_emb_51', 'text_emb_52', 'text_emb_53', 'text_emb_54', 'text_emb_55', 'text_emb_56', 'text_emb_57', 'text_emb_58', 'text_emb_59', 'text_emb_60', 'text_emb_61', 'text_emb_62', 'text_emb_63', 'text_emb_64', 'text_emb_65', 'text_emb_66', 'text_emb_67', 'text_emb_68', 'text_emb_69', 'text_emb_70', 'text_emb_71', 'text_emb_72', 'text_emb_73', 'text_emb_74', 'text_emb_75', 'text_emb_76', 'text_emb_77', 'text_emb_78', 'text_emb_79', 'text_emb_80', 'text_emb_81', 'text_emb_82', 'text_emb_83', 'text_emb_84', 'text_emb_85', 'text_emb_86', 'text_emb_87', 'text_emb_88', 'text_emb_89', 'text_emb_90', 'text_emb_91', 'text_emb_92', 'text_emb_93', 'text_emb_94', 'text_emb_95', 'text_emb_96', 'text_emb_97', 'text_emb_98', 'text_emb_99', 'text_emb_100', 'text_emb_101', 'text_emb_102', 'text_emb_103', 'text_emb_104', 'text_emb_105', 'text_emb_106', 'text_emb_107', 'text_emb_108', 'text_emb_109', 'text_emb_110', 'text_emb_111', 'text_emb_112', 'text_emb_113', 'text_emb_114', 'text_emb_115', 'text_emb_116', 'text_emb_117', 'text_emb_118', 'text_emb_119', 'text_emb_120', 'text_emb_121', 'text_emb_122', 'text_emb_123', 'text_emb_124', 'text_emb_125', 'text_emb_126', 'text_emb_127', 'text_emb_128', 'text_emb_129', 'text_emb_130', 'text_emb_131', 'text_emb_132', 'text_emb_133', 'text_emb_134', 'text_emb_135', 'text_emb_136', 'text_emb_137', 'text_emb_138', 'text_emb_139', 'text_emb_140', 'text_emb_141', 'text_emb_142', 'text_emb_143', 'text_emb_144', 'text_emb_145', 'text_emb_146', 'text_emb_147', 'text_emb_148', 'text_emb_149', 'text_emb_150', 'text_emb_151', 'text_emb_152', 'text_emb_153', 'text_emb_154', 'text_emb_155', 'text_emb_156', 'text_emb_157', 'text_emb_158', 'text_emb_159', 'text_emb_160', 'text_emb_161', 'text_emb_162', 'text_emb_163', 'text_emb_164', 'text_emb_165', 'text_emb_166', 'text_emb_167', 'text_emb_168', 'text_emb_169', 'text_emb_170', 'text_emb_171', 'text_emb_172', 'text_emb_173', 'text_emb_174', 'text_emb_175', 'text_emb_176', 'text_emb_177', 'text_emb_178', 'text_emb_179', 'text_emb_180', 'text_emb_181', 'text_emb_182', 'text_emb_183', 'text_emb_184', 'text_emb_185', 'text_emb_186', 'text_emb_187', 'text_emb_188', 'text_emb_189', 'text_emb_190', 'text_emb_191', 'text_emb_192', 'text_emb_193', 'text_emb_194', 'text_emb_195', 'text_emb_196', 'text_emb_197', 'text_emb_198', 'text_emb_199', 'text_emb_200', 'text_emb_201', 'text_emb_202', 'text_emb_203', 'text_emb_204', 'text_emb_205', 'text_emb_206', 'text_emb_207', 'text_emb_208', 'text_emb_209', 'text_emb_210', 'text_emb_211', 'text_emb_212', 'text_emb_213', 'text_emb_214', 'text_emb_215', 'text_emb_216', 'text_emb_217', 'text_emb_218', 'text_emb_219', 'text_emb_220', 'text_emb_221', 'text_emb_222', 'text_emb_223', 'text_emb_224', 'text_emb_225', 'text_emb_226', 'text_emb_227', 'text_emb_228', 'text_emb_229', 'text_emb_230', 'text_emb_231', 'text_emb_232', 'text_emb_233', 'text_emb_234', 'text_emb_235', 'text_emb_236', 'text_emb_237', 'text_emb_238', 'text_emb_239', 'text_emb_240', 'text_emb_241', 'text_emb_242', 'text_emb_243', 'text_emb_244', 'text_emb_245', 'text_emb_246', 'text_emb_247', 'text_emb_248', 'text_emb_249', 'text_emb_250', 'text_emb_251', 'text_emb_252', 'text_emb_253', 'text_emb_254', 'text_emb_255', 'text_emb_256', 'text_emb_257', 'text_emb_258', 'text_emb_259', 'text_emb_260', 'text_emb_261', 'text_emb_262', 'text_emb_263', 'text_emb_264', 'text_emb_265', 'text_emb_266', 'text_emb_267', 'text_emb_268', 'text_emb_269', 'text_emb_270', 'text_emb_271', 'text_emb_272', 'text_emb_273', 'text_emb_274', 'text_emb_275', 'text_emb_276', 'text_emb_277', 'text_emb_278', 'text_emb_279', 'text_emb_280', 'text_emb_281', 'text_emb_282', 'text_emb_283', 'text_emb_284', 'text_emb_285', 'text_emb_286', 'text_emb_287', 'text_emb_288', 'text_emb_289', 'text_emb_290', 'text_emb_291', 'text_emb_292', 'text_emb_293', 'text_emb_294', 'text_emb_295', 'text_emb_296', 'text_emb_297', 'text_emb_298', 'text_emb_299', 'text_emb_300', 'text_emb_301', 'text_emb_302', 'text_emb_303', 'text_emb_304', 'text_emb_305', 'text_emb_306', 'text_emb_307', 'text_emb_308', 'text_emb_309', 'text_emb_310', 'text_emb_311', 'text_emb_312', 'text_emb_313', 'text_emb_314', 'text_emb_315', 'text_emb_316', 'text_emb_317', 'text_emb_318', 'text_emb_319', 'text_emb_320', 'text_emb_321', 'text_emb_322', 'text_emb_323', 'text_emb_324', 'text_emb_325', 'text_emb_326', 'text_emb_327', 'text_emb_328', 'text_emb_329', 'text_emb_330', 'text_emb_331', 'text_emb_332', 'text_emb_333', 'text_emb_334', 'text_emb_335', 'text_emb_336', 'text_emb_337', 'text_emb_338', 'text_emb_339', 'text_emb_340', 'text_emb_341', 'text_emb_342', 'text_emb_343', 'text_emb_344', 'text_emb_345', 'text_emb_346', 'text_emb_347', 'text_emb_348', 'text_emb_349', 'text_emb_350', 'text_emb_351', 'text_emb_352', 'text_emb_353', 'text_emb_354', 'text_emb_355', 'text_emb_356', 'text_emb_357', 'text_emb_358', 'text_emb_359', 'text_emb_360', 'text_emb_361', 'text_emb_362', 'text_emb_363', 'text_emb_364', 'text_emb_365', 'text_emb_366', 'text_emb_367', 'text_emb_368', 'text_emb_369', 'text_emb_370', 'text_emb_371', 'text_emb_372', 'text_emb_373', 'text_emb_374', 'text_emb_375', 'text_emb_376', 'text_emb_377', 'text_emb_378', 'text_emb_379', 'text_emb_380', 'text_emb_381', 'text_emb_382', 'text_emb_383', 'Narration_Cluster', 'Narration_Cluster_Label', 'Accounting_Month', 'Accounting_Weekday', 'Invoice_Month', 'Invoice_Weekday', 'Posted_Month', 'Posted_Weekday', 'Model_Score', 'Final_Score', 'CP_01', 'CP_02']\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "=== AFTER ADDING CP_19 ===\n",
      "Total columns: 428\n",
      "Column names: ['S. No', 'Entity Name', 'Accounting Date', 'Approval Type', 'Document Type', 'Invoice Date', 'Day', 'Nature', 'Account Code', 'PL/ BS', 'Report Group', 'Account Name', 'Nature in balance sheet', 'Document Number', 'Je Line Num', 'Source Number', 'Source Name', 'Source Voucher Name', 'Source Desc', 'Line Desc', 'Project Code', 'Internal Reference', 'Posted Date', 'Branch', 'Batch Name', 'Entered Dr SUM', 'Entered Cr SUM', 'Accounted Dr SUM', 'Accounted Cr SUM', 'Net Amount', 'Combined_Text', 'text_emb_0', 'text_emb_1', 'text_emb_2', 'text_emb_3', 'text_emb_4', 'text_emb_5', 'text_emb_6', 'text_emb_7', 'text_emb_8', 'text_emb_9', 'text_emb_10', 'text_emb_11', 'text_emb_12', 'text_emb_13', 'text_emb_14', 'text_emb_15', 'text_emb_16', 'text_emb_17', 'text_emb_18', 'text_emb_19', 'text_emb_20', 'text_emb_21', 'text_emb_22', 'text_emb_23', 'text_emb_24', 'text_emb_25', 'text_emb_26', 'text_emb_27', 'text_emb_28', 'text_emb_29', 'text_emb_30', 'text_emb_31', 'text_emb_32', 'text_emb_33', 'text_emb_34', 'text_emb_35', 'text_emb_36', 'text_emb_37', 'text_emb_38', 'text_emb_39', 'text_emb_40', 'text_emb_41', 'text_emb_42', 'text_emb_43', 'text_emb_44', 'text_emb_45', 'text_emb_46', 'text_emb_47', 'text_emb_48', 'text_emb_49', 'text_emb_50', 'text_emb_51', 'text_emb_52', 'text_emb_53', 'text_emb_54', 'text_emb_55', 'text_emb_56', 'text_emb_57', 'text_emb_58', 'text_emb_59', 'text_emb_60', 'text_emb_61', 'text_emb_62', 'text_emb_63', 'text_emb_64', 'text_emb_65', 'text_emb_66', 'text_emb_67', 'text_emb_68', 'text_emb_69', 'text_emb_70', 'text_emb_71', 'text_emb_72', 'text_emb_73', 'text_emb_74', 'text_emb_75', 'text_emb_76', 'text_emb_77', 'text_emb_78', 'text_emb_79', 'text_emb_80', 'text_emb_81', 'text_emb_82', 'text_emb_83', 'text_emb_84', 'text_emb_85', 'text_emb_86', 'text_emb_87', 'text_emb_88', 'text_emb_89', 'text_emb_90', 'text_emb_91', 'text_emb_92', 'text_emb_93', 'text_emb_94', 'text_emb_95', 'text_emb_96', 'text_emb_97', 'text_emb_98', 'text_emb_99', 'text_emb_100', 'text_emb_101', 'text_emb_102', 'text_emb_103', 'text_emb_104', 'text_emb_105', 'text_emb_106', 'text_emb_107', 'text_emb_108', 'text_emb_109', 'text_emb_110', 'text_emb_111', 'text_emb_112', 'text_emb_113', 'text_emb_114', 'text_emb_115', 'text_emb_116', 'text_emb_117', 'text_emb_118', 'text_emb_119', 'text_emb_120', 'text_emb_121', 'text_emb_122', 'text_emb_123', 'text_emb_124', 'text_emb_125', 'text_emb_126', 'text_emb_127', 'text_emb_128', 'text_emb_129', 'text_emb_130', 'text_emb_131', 'text_emb_132', 'text_emb_133', 'text_emb_134', 'text_emb_135', 'text_emb_136', 'text_emb_137', 'text_emb_138', 'text_emb_139', 'text_emb_140', 'text_emb_141', 'text_emb_142', 'text_emb_143', 'text_emb_144', 'text_emb_145', 'text_emb_146', 'text_emb_147', 'text_emb_148', 'text_emb_149', 'text_emb_150', 'text_emb_151', 'text_emb_152', 'text_emb_153', 'text_emb_154', 'text_emb_155', 'text_emb_156', 'text_emb_157', 'text_emb_158', 'text_emb_159', 'text_emb_160', 'text_emb_161', 'text_emb_162', 'text_emb_163', 'text_emb_164', 'text_emb_165', 'text_emb_166', 'text_emb_167', 'text_emb_168', 'text_emb_169', 'text_emb_170', 'text_emb_171', 'text_emb_172', 'text_emb_173', 'text_emb_174', 'text_emb_175', 'text_emb_176', 'text_emb_177', 'text_emb_178', 'text_emb_179', 'text_emb_180', 'text_emb_181', 'text_emb_182', 'text_emb_183', 'text_emb_184', 'text_emb_185', 'text_emb_186', 'text_emb_187', 'text_emb_188', 'text_emb_189', 'text_emb_190', 'text_emb_191', 'text_emb_192', 'text_emb_193', 'text_emb_194', 'text_emb_195', 'text_emb_196', 'text_emb_197', 'text_emb_198', 'text_emb_199', 'text_emb_200', 'text_emb_201', 'text_emb_202', 'text_emb_203', 'text_emb_204', 'text_emb_205', 'text_emb_206', 'text_emb_207', 'text_emb_208', 'text_emb_209', 'text_emb_210', 'text_emb_211', 'text_emb_212', 'text_emb_213', 'text_emb_214', 'text_emb_215', 'text_emb_216', 'text_emb_217', 'text_emb_218', 'text_emb_219', 'text_emb_220', 'text_emb_221', 'text_emb_222', 'text_emb_223', 'text_emb_224', 'text_emb_225', 'text_emb_226', 'text_emb_227', 'text_emb_228', 'text_emb_229', 'text_emb_230', 'text_emb_231', 'text_emb_232', 'text_emb_233', 'text_emb_234', 'text_emb_235', 'text_emb_236', 'text_emb_237', 'text_emb_238', 'text_emb_239', 'text_emb_240', 'text_emb_241', 'text_emb_242', 'text_emb_243', 'text_emb_244', 'text_emb_245', 'text_emb_246', 'text_emb_247', 'text_emb_248', 'text_emb_249', 'text_emb_250', 'text_emb_251', 'text_emb_252', 'text_emb_253', 'text_emb_254', 'text_emb_255', 'text_emb_256', 'text_emb_257', 'text_emb_258', 'text_emb_259', 'text_emb_260', 'text_emb_261', 'text_emb_262', 'text_emb_263', 'text_emb_264', 'text_emb_265', 'text_emb_266', 'text_emb_267', 'text_emb_268', 'text_emb_269', 'text_emb_270', 'text_emb_271', 'text_emb_272', 'text_emb_273', 'text_emb_274', 'text_emb_275', 'text_emb_276', 'text_emb_277', 'text_emb_278', 'text_emb_279', 'text_emb_280', 'text_emb_281', 'text_emb_282', 'text_emb_283', 'text_emb_284', 'text_emb_285', 'text_emb_286', 'text_emb_287', 'text_emb_288', 'text_emb_289', 'text_emb_290', 'text_emb_291', 'text_emb_292', 'text_emb_293', 'text_emb_294', 'text_emb_295', 'text_emb_296', 'text_emb_297', 'text_emb_298', 'text_emb_299', 'text_emb_300', 'text_emb_301', 'text_emb_302', 'text_emb_303', 'text_emb_304', 'text_emb_305', 'text_emb_306', 'text_emb_307', 'text_emb_308', 'text_emb_309', 'text_emb_310', 'text_emb_311', 'text_emb_312', 'text_emb_313', 'text_emb_314', 'text_emb_315', 'text_emb_316', 'text_emb_317', 'text_emb_318', 'text_emb_319', 'text_emb_320', 'text_emb_321', 'text_emb_322', 'text_emb_323', 'text_emb_324', 'text_emb_325', 'text_emb_326', 'text_emb_327', 'text_emb_328', 'text_emb_329', 'text_emb_330', 'text_emb_331', 'text_emb_332', 'text_emb_333', 'text_emb_334', 'text_emb_335', 'text_emb_336', 'text_emb_337', 'text_emb_338', 'text_emb_339', 'text_emb_340', 'text_emb_341', 'text_emb_342', 'text_emb_343', 'text_emb_344', 'text_emb_345', 'text_emb_346', 'text_emb_347', 'text_emb_348', 'text_emb_349', 'text_emb_350', 'text_emb_351', 'text_emb_352', 'text_emb_353', 'text_emb_354', 'text_emb_355', 'text_emb_356', 'text_emb_357', 'text_emb_358', 'text_emb_359', 'text_emb_360', 'text_emb_361', 'text_emb_362', 'text_emb_363', 'text_emb_364', 'text_emb_365', 'text_emb_366', 'text_emb_367', 'text_emb_368', 'text_emb_369', 'text_emb_370', 'text_emb_371', 'text_emb_372', 'text_emb_373', 'text_emb_374', 'text_emb_375', 'text_emb_376', 'text_emb_377', 'text_emb_378', 'text_emb_379', 'text_emb_380', 'text_emb_381', 'text_emb_382', 'text_emb_383', 'Narration_Cluster', 'Narration_Cluster_Label', 'Accounting_Month', 'Accounting_Weekday', 'Invoice_Month', 'Invoice_Weekday', 'Posted_Month', 'Posted_Weekday', 'Model_Score', 'Final_Score', 'CP_01', 'CP_02', 'CP_19']\n",
      "--------------------------------------------------------------------------------\n",
      "Adding Currency column...\n",
      "\n",
      "=== AFTER ADDING Currency ===\n",
      "Total columns: 429\n",
      "Column names: ['S. No', 'Entity Name', 'Accounting Date', 'Approval Type', 'Document Type', 'Invoice Date', 'Day', 'Nature', 'Account Code', 'PL/ BS', 'Report Group', 'Account Name', 'Nature in balance sheet', 'Document Number', 'Je Line Num', 'Source Number', 'Source Name', 'Source Voucher Name', 'Source Desc', 'Line Desc', 'Project Code', 'Internal Reference', 'Posted Date', 'Branch', 'Batch Name', 'Entered Dr SUM', 'Entered Cr SUM', 'Accounted Dr SUM', 'Accounted Cr SUM', 'Net Amount', 'Combined_Text', 'text_emb_0', 'text_emb_1', 'text_emb_2', 'text_emb_3', 'text_emb_4', 'text_emb_5', 'text_emb_6', 'text_emb_7', 'text_emb_8', 'text_emb_9', 'text_emb_10', 'text_emb_11', 'text_emb_12', 'text_emb_13', 'text_emb_14', 'text_emb_15', 'text_emb_16', 'text_emb_17', 'text_emb_18', 'text_emb_19', 'text_emb_20', 'text_emb_21', 'text_emb_22', 'text_emb_23', 'text_emb_24', 'text_emb_25', 'text_emb_26', 'text_emb_27', 'text_emb_28', 'text_emb_29', 'text_emb_30', 'text_emb_31', 'text_emb_32', 'text_emb_33', 'text_emb_34', 'text_emb_35', 'text_emb_36', 'text_emb_37', 'text_emb_38', 'text_emb_39', 'text_emb_40', 'text_emb_41', 'text_emb_42', 'text_emb_43', 'text_emb_44', 'text_emb_45', 'text_emb_46', 'text_emb_47', 'text_emb_48', 'text_emb_49', 'text_emb_50', 'text_emb_51', 'text_emb_52', 'text_emb_53', 'text_emb_54', 'text_emb_55', 'text_emb_56', 'text_emb_57', 'text_emb_58', 'text_emb_59', 'text_emb_60', 'text_emb_61', 'text_emb_62', 'text_emb_63', 'text_emb_64', 'text_emb_65', 'text_emb_66', 'text_emb_67', 'text_emb_68', 'text_emb_69', 'text_emb_70', 'text_emb_71', 'text_emb_72', 'text_emb_73', 'text_emb_74', 'text_emb_75', 'text_emb_76', 'text_emb_77', 'text_emb_78', 'text_emb_79', 'text_emb_80', 'text_emb_81', 'text_emb_82', 'text_emb_83', 'text_emb_84', 'text_emb_85', 'text_emb_86', 'text_emb_87', 'text_emb_88', 'text_emb_89', 'text_emb_90', 'text_emb_91', 'text_emb_92', 'text_emb_93', 'text_emb_94', 'text_emb_95', 'text_emb_96', 'text_emb_97', 'text_emb_98', 'text_emb_99', 'text_emb_100', 'text_emb_101', 'text_emb_102', 'text_emb_103', 'text_emb_104', 'text_emb_105', 'text_emb_106', 'text_emb_107', 'text_emb_108', 'text_emb_109', 'text_emb_110', 'text_emb_111', 'text_emb_112', 'text_emb_113', 'text_emb_114', 'text_emb_115', 'text_emb_116', 'text_emb_117', 'text_emb_118', 'text_emb_119', 'text_emb_120', 'text_emb_121', 'text_emb_122', 'text_emb_123', 'text_emb_124', 'text_emb_125', 'text_emb_126', 'text_emb_127', 'text_emb_128', 'text_emb_129', 'text_emb_130', 'text_emb_131', 'text_emb_132', 'text_emb_133', 'text_emb_134', 'text_emb_135', 'text_emb_136', 'text_emb_137', 'text_emb_138', 'text_emb_139', 'text_emb_140', 'text_emb_141', 'text_emb_142', 'text_emb_143', 'text_emb_144', 'text_emb_145', 'text_emb_146', 'text_emb_147', 'text_emb_148', 'text_emb_149', 'text_emb_150', 'text_emb_151', 'text_emb_152', 'text_emb_153', 'text_emb_154', 'text_emb_155', 'text_emb_156', 'text_emb_157', 'text_emb_158', 'text_emb_159', 'text_emb_160', 'text_emb_161', 'text_emb_162', 'text_emb_163', 'text_emb_164', 'text_emb_165', 'text_emb_166', 'text_emb_167', 'text_emb_168', 'text_emb_169', 'text_emb_170', 'text_emb_171', 'text_emb_172', 'text_emb_173', 'text_emb_174', 'text_emb_175', 'text_emb_176', 'text_emb_177', 'text_emb_178', 'text_emb_179', 'text_emb_180', 'text_emb_181', 'text_emb_182', 'text_emb_183', 'text_emb_184', 'text_emb_185', 'text_emb_186', 'text_emb_187', 'text_emb_188', 'text_emb_189', 'text_emb_190', 'text_emb_191', 'text_emb_192', 'text_emb_193', 'text_emb_194', 'text_emb_195', 'text_emb_196', 'text_emb_197', 'text_emb_198', 'text_emb_199', 'text_emb_200', 'text_emb_201', 'text_emb_202', 'text_emb_203', 'text_emb_204', 'text_emb_205', 'text_emb_206', 'text_emb_207', 'text_emb_208', 'text_emb_209', 'text_emb_210', 'text_emb_211', 'text_emb_212', 'text_emb_213', 'text_emb_214', 'text_emb_215', 'text_emb_216', 'text_emb_217', 'text_emb_218', 'text_emb_219', 'text_emb_220', 'text_emb_221', 'text_emb_222', 'text_emb_223', 'text_emb_224', 'text_emb_225', 'text_emb_226', 'text_emb_227', 'text_emb_228', 'text_emb_229', 'text_emb_230', 'text_emb_231', 'text_emb_232', 'text_emb_233', 'text_emb_234', 'text_emb_235', 'text_emb_236', 'text_emb_237', 'text_emb_238', 'text_emb_239', 'text_emb_240', 'text_emb_241', 'text_emb_242', 'text_emb_243', 'text_emb_244', 'text_emb_245', 'text_emb_246', 'text_emb_247', 'text_emb_248', 'text_emb_249', 'text_emb_250', 'text_emb_251', 'text_emb_252', 'text_emb_253', 'text_emb_254', 'text_emb_255', 'text_emb_256', 'text_emb_257', 'text_emb_258', 'text_emb_259', 'text_emb_260', 'text_emb_261', 'text_emb_262', 'text_emb_263', 'text_emb_264', 'text_emb_265', 'text_emb_266', 'text_emb_267', 'text_emb_268', 'text_emb_269', 'text_emb_270', 'text_emb_271', 'text_emb_272', 'text_emb_273', 'text_emb_274', 'text_emb_275', 'text_emb_276', 'text_emb_277', 'text_emb_278', 'text_emb_279', 'text_emb_280', 'text_emb_281', 'text_emb_282', 'text_emb_283', 'text_emb_284', 'text_emb_285', 'text_emb_286', 'text_emb_287', 'text_emb_288', 'text_emb_289', 'text_emb_290', 'text_emb_291', 'text_emb_292', 'text_emb_293', 'text_emb_294', 'text_emb_295', 'text_emb_296', 'text_emb_297', 'text_emb_298', 'text_emb_299', 'text_emb_300', 'text_emb_301', 'text_emb_302', 'text_emb_303', 'text_emb_304', 'text_emb_305', 'text_emb_306', 'text_emb_307', 'text_emb_308', 'text_emb_309', 'text_emb_310', 'text_emb_311', 'text_emb_312', 'text_emb_313', 'text_emb_314', 'text_emb_315', 'text_emb_316', 'text_emb_317', 'text_emb_318', 'text_emb_319', 'text_emb_320', 'text_emb_321', 'text_emb_322', 'text_emb_323', 'text_emb_324', 'text_emb_325', 'text_emb_326', 'text_emb_327', 'text_emb_328', 'text_emb_329', 'text_emb_330', 'text_emb_331', 'text_emb_332', 'text_emb_333', 'text_emb_334', 'text_emb_335', 'text_emb_336', 'text_emb_337', 'text_emb_338', 'text_emb_339', 'text_emb_340', 'text_emb_341', 'text_emb_342', 'text_emb_343', 'text_emb_344', 'text_emb_345', 'text_emb_346', 'text_emb_347', 'text_emb_348', 'text_emb_349', 'text_emb_350', 'text_emb_351', 'text_emb_352', 'text_emb_353', 'text_emb_354', 'text_emb_355', 'text_emb_356', 'text_emb_357', 'text_emb_358', 'text_emb_359', 'text_emb_360', 'text_emb_361', 'text_emb_362', 'text_emb_363', 'text_emb_364', 'text_emb_365', 'text_emb_366', 'text_emb_367', 'text_emb_368', 'text_emb_369', 'text_emb_370', 'text_emb_371', 'text_emb_372', 'text_emb_373', 'text_emb_374', 'text_emb_375', 'text_emb_376', 'text_emb_377', 'text_emb_378', 'text_emb_379', 'text_emb_380', 'text_emb_381', 'text_emb_382', 'text_emb_383', 'Narration_Cluster', 'Narration_Cluster_Label', 'Accounting_Month', 'Accounting_Weekday', 'Invoice_Month', 'Invoice_Weekday', 'Posted_Month', 'Posted_Weekday', 'Model_Score', 'Final_Score', 'CP_01', 'CP_02', 'CP_19', 'Currency']\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "=== AFTER ADDING CP_Score ===\n",
      "Total columns: 430\n",
      "Column names: ['S. No', 'Entity Name', 'Accounting Date', 'Approval Type', 'Document Type', 'Invoice Date', 'Day', 'Nature', 'Account Code', 'PL/ BS', 'Report Group', 'Account Name', 'Nature in balance sheet', 'Document Number', 'Je Line Num', 'Source Number', 'Source Name', 'Source Voucher Name', 'Source Desc', 'Line Desc', 'Project Code', 'Internal Reference', 'Posted Date', 'Branch', 'Batch Name', 'Entered Dr SUM', 'Entered Cr SUM', 'Accounted Dr SUM', 'Accounted Cr SUM', 'Net Amount', 'Combined_Text', 'text_emb_0', 'text_emb_1', 'text_emb_2', 'text_emb_3', 'text_emb_4', 'text_emb_5', 'text_emb_6', 'text_emb_7', 'text_emb_8', 'text_emb_9', 'text_emb_10', 'text_emb_11', 'text_emb_12', 'text_emb_13', 'text_emb_14', 'text_emb_15', 'text_emb_16', 'text_emb_17', 'text_emb_18', 'text_emb_19', 'text_emb_20', 'text_emb_21', 'text_emb_22', 'text_emb_23', 'text_emb_24', 'text_emb_25', 'text_emb_26', 'text_emb_27', 'text_emb_28', 'text_emb_29', 'text_emb_30', 'text_emb_31', 'text_emb_32', 'text_emb_33', 'text_emb_34', 'text_emb_35', 'text_emb_36', 'text_emb_37', 'text_emb_38', 'text_emb_39', 'text_emb_40', 'text_emb_41', 'text_emb_42', 'text_emb_43', 'text_emb_44', 'text_emb_45', 'text_emb_46', 'text_emb_47', 'text_emb_48', 'text_emb_49', 'text_emb_50', 'text_emb_51', 'text_emb_52', 'text_emb_53', 'text_emb_54', 'text_emb_55', 'text_emb_56', 'text_emb_57', 'text_emb_58', 'text_emb_59', 'text_emb_60', 'text_emb_61', 'text_emb_62', 'text_emb_63', 'text_emb_64', 'text_emb_65', 'text_emb_66', 'text_emb_67', 'text_emb_68', 'text_emb_69', 'text_emb_70', 'text_emb_71', 'text_emb_72', 'text_emb_73', 'text_emb_74', 'text_emb_75', 'text_emb_76', 'text_emb_77', 'text_emb_78', 'text_emb_79', 'text_emb_80', 'text_emb_81', 'text_emb_82', 'text_emb_83', 'text_emb_84', 'text_emb_85', 'text_emb_86', 'text_emb_87', 'text_emb_88', 'text_emb_89', 'text_emb_90', 'text_emb_91', 'text_emb_92', 'text_emb_93', 'text_emb_94', 'text_emb_95', 'text_emb_96', 'text_emb_97', 'text_emb_98', 'text_emb_99', 'text_emb_100', 'text_emb_101', 'text_emb_102', 'text_emb_103', 'text_emb_104', 'text_emb_105', 'text_emb_106', 'text_emb_107', 'text_emb_108', 'text_emb_109', 'text_emb_110', 'text_emb_111', 'text_emb_112', 'text_emb_113', 'text_emb_114', 'text_emb_115', 'text_emb_116', 'text_emb_117', 'text_emb_118', 'text_emb_119', 'text_emb_120', 'text_emb_121', 'text_emb_122', 'text_emb_123', 'text_emb_124', 'text_emb_125', 'text_emb_126', 'text_emb_127', 'text_emb_128', 'text_emb_129', 'text_emb_130', 'text_emb_131', 'text_emb_132', 'text_emb_133', 'text_emb_134', 'text_emb_135', 'text_emb_136', 'text_emb_137', 'text_emb_138', 'text_emb_139', 'text_emb_140', 'text_emb_141', 'text_emb_142', 'text_emb_143', 'text_emb_144', 'text_emb_145', 'text_emb_146', 'text_emb_147', 'text_emb_148', 'text_emb_149', 'text_emb_150', 'text_emb_151', 'text_emb_152', 'text_emb_153', 'text_emb_154', 'text_emb_155', 'text_emb_156', 'text_emb_157', 'text_emb_158', 'text_emb_159', 'text_emb_160', 'text_emb_161', 'text_emb_162', 'text_emb_163', 'text_emb_164', 'text_emb_165', 'text_emb_166', 'text_emb_167', 'text_emb_168', 'text_emb_169', 'text_emb_170', 'text_emb_171', 'text_emb_172', 'text_emb_173', 'text_emb_174', 'text_emb_175', 'text_emb_176', 'text_emb_177', 'text_emb_178', 'text_emb_179', 'text_emb_180', 'text_emb_181', 'text_emb_182', 'text_emb_183', 'text_emb_184', 'text_emb_185', 'text_emb_186', 'text_emb_187', 'text_emb_188', 'text_emb_189', 'text_emb_190', 'text_emb_191', 'text_emb_192', 'text_emb_193', 'text_emb_194', 'text_emb_195', 'text_emb_196', 'text_emb_197', 'text_emb_198', 'text_emb_199', 'text_emb_200', 'text_emb_201', 'text_emb_202', 'text_emb_203', 'text_emb_204', 'text_emb_205', 'text_emb_206', 'text_emb_207', 'text_emb_208', 'text_emb_209', 'text_emb_210', 'text_emb_211', 'text_emb_212', 'text_emb_213', 'text_emb_214', 'text_emb_215', 'text_emb_216', 'text_emb_217', 'text_emb_218', 'text_emb_219', 'text_emb_220', 'text_emb_221', 'text_emb_222', 'text_emb_223', 'text_emb_224', 'text_emb_225', 'text_emb_226', 'text_emb_227', 'text_emb_228', 'text_emb_229', 'text_emb_230', 'text_emb_231', 'text_emb_232', 'text_emb_233', 'text_emb_234', 'text_emb_235', 'text_emb_236', 'text_emb_237', 'text_emb_238', 'text_emb_239', 'text_emb_240', 'text_emb_241', 'text_emb_242', 'text_emb_243', 'text_emb_244', 'text_emb_245', 'text_emb_246', 'text_emb_247', 'text_emb_248', 'text_emb_249', 'text_emb_250', 'text_emb_251', 'text_emb_252', 'text_emb_253', 'text_emb_254', 'text_emb_255', 'text_emb_256', 'text_emb_257', 'text_emb_258', 'text_emb_259', 'text_emb_260', 'text_emb_261', 'text_emb_262', 'text_emb_263', 'text_emb_264', 'text_emb_265', 'text_emb_266', 'text_emb_267', 'text_emb_268', 'text_emb_269', 'text_emb_270', 'text_emb_271', 'text_emb_272', 'text_emb_273', 'text_emb_274', 'text_emb_275', 'text_emb_276', 'text_emb_277', 'text_emb_278', 'text_emb_279', 'text_emb_280', 'text_emb_281', 'text_emb_282', 'text_emb_283', 'text_emb_284', 'text_emb_285', 'text_emb_286', 'text_emb_287', 'text_emb_288', 'text_emb_289', 'text_emb_290', 'text_emb_291', 'text_emb_292', 'text_emb_293', 'text_emb_294', 'text_emb_295', 'text_emb_296', 'text_emb_297', 'text_emb_298', 'text_emb_299', 'text_emb_300', 'text_emb_301', 'text_emb_302', 'text_emb_303', 'text_emb_304', 'text_emb_305', 'text_emb_306', 'text_emb_307', 'text_emb_308', 'text_emb_309', 'text_emb_310', 'text_emb_311', 'text_emb_312', 'text_emb_313', 'text_emb_314', 'text_emb_315', 'text_emb_316', 'text_emb_317', 'text_emb_318', 'text_emb_319', 'text_emb_320', 'text_emb_321', 'text_emb_322', 'text_emb_323', 'text_emb_324', 'text_emb_325', 'text_emb_326', 'text_emb_327', 'text_emb_328', 'text_emb_329', 'text_emb_330', 'text_emb_331', 'text_emb_332', 'text_emb_333', 'text_emb_334', 'text_emb_335', 'text_emb_336', 'text_emb_337', 'text_emb_338', 'text_emb_339', 'text_emb_340', 'text_emb_341', 'text_emb_342', 'text_emb_343', 'text_emb_344', 'text_emb_345', 'text_emb_346', 'text_emb_347', 'text_emb_348', 'text_emb_349', 'text_emb_350', 'text_emb_351', 'text_emb_352', 'text_emb_353', 'text_emb_354', 'text_emb_355', 'text_emb_356', 'text_emb_357', 'text_emb_358', 'text_emb_359', 'text_emb_360', 'text_emb_361', 'text_emb_362', 'text_emb_363', 'text_emb_364', 'text_emb_365', 'text_emb_366', 'text_emb_367', 'text_emb_368', 'text_emb_369', 'text_emb_370', 'text_emb_371', 'text_emb_372', 'text_emb_373', 'text_emb_374', 'text_emb_375', 'text_emb_376', 'text_emb_377', 'text_emb_378', 'text_emb_379', 'text_emb_380', 'text_emb_381', 'text_emb_382', 'text_emb_383', 'Narration_Cluster', 'Narration_Cluster_Label', 'Accounting_Month', 'Accounting_Weekday', 'Invoice_Month', 'Invoice_Weekday', 'Posted_Month', 'Posted_Weekday', 'Model_Score', 'Final_Score', 'CP_01', 'CP_02', 'CP_19', 'Currency', 'CP_Score']\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "=== AFTER ADDING Triggered_CPs ===\n",
      "Total columns: 431\n",
      "Column names: ['S. No', 'Entity Name', 'Accounting Date', 'Approval Type', 'Document Type', 'Invoice Date', 'Day', 'Nature', 'Account Code', 'PL/ BS', 'Report Group', 'Account Name', 'Nature in balance sheet', 'Document Number', 'Je Line Num', 'Source Number', 'Source Name', 'Source Voucher Name', 'Source Desc', 'Line Desc', 'Project Code', 'Internal Reference', 'Posted Date', 'Branch', 'Batch Name', 'Entered Dr SUM', 'Entered Cr SUM', 'Accounted Dr SUM', 'Accounted Cr SUM', 'Net Amount', 'Combined_Text', 'text_emb_0', 'text_emb_1', 'text_emb_2', 'text_emb_3', 'text_emb_4', 'text_emb_5', 'text_emb_6', 'text_emb_7', 'text_emb_8', 'text_emb_9', 'text_emb_10', 'text_emb_11', 'text_emb_12', 'text_emb_13', 'text_emb_14', 'text_emb_15', 'text_emb_16', 'text_emb_17', 'text_emb_18', 'text_emb_19', 'text_emb_20', 'text_emb_21', 'text_emb_22', 'text_emb_23', 'text_emb_24', 'text_emb_25', 'text_emb_26', 'text_emb_27', 'text_emb_28', 'text_emb_29', 'text_emb_30', 'text_emb_31', 'text_emb_32', 'text_emb_33', 'text_emb_34', 'text_emb_35', 'text_emb_36', 'text_emb_37', 'text_emb_38', 'text_emb_39', 'text_emb_40', 'text_emb_41', 'text_emb_42', 'text_emb_43', 'text_emb_44', 'text_emb_45', 'text_emb_46', 'text_emb_47', 'text_emb_48', 'text_emb_49', 'text_emb_50', 'text_emb_51', 'text_emb_52', 'text_emb_53', 'text_emb_54', 'text_emb_55', 'text_emb_56', 'text_emb_57', 'text_emb_58', 'text_emb_59', 'text_emb_60', 'text_emb_61', 'text_emb_62', 'text_emb_63', 'text_emb_64', 'text_emb_65', 'text_emb_66', 'text_emb_67', 'text_emb_68', 'text_emb_69', 'text_emb_70', 'text_emb_71', 'text_emb_72', 'text_emb_73', 'text_emb_74', 'text_emb_75', 'text_emb_76', 'text_emb_77', 'text_emb_78', 'text_emb_79', 'text_emb_80', 'text_emb_81', 'text_emb_82', 'text_emb_83', 'text_emb_84', 'text_emb_85', 'text_emb_86', 'text_emb_87', 'text_emb_88', 'text_emb_89', 'text_emb_90', 'text_emb_91', 'text_emb_92', 'text_emb_93', 'text_emb_94', 'text_emb_95', 'text_emb_96', 'text_emb_97', 'text_emb_98', 'text_emb_99', 'text_emb_100', 'text_emb_101', 'text_emb_102', 'text_emb_103', 'text_emb_104', 'text_emb_105', 'text_emb_106', 'text_emb_107', 'text_emb_108', 'text_emb_109', 'text_emb_110', 'text_emb_111', 'text_emb_112', 'text_emb_113', 'text_emb_114', 'text_emb_115', 'text_emb_116', 'text_emb_117', 'text_emb_118', 'text_emb_119', 'text_emb_120', 'text_emb_121', 'text_emb_122', 'text_emb_123', 'text_emb_124', 'text_emb_125', 'text_emb_126', 'text_emb_127', 'text_emb_128', 'text_emb_129', 'text_emb_130', 'text_emb_131', 'text_emb_132', 'text_emb_133', 'text_emb_134', 'text_emb_135', 'text_emb_136', 'text_emb_137', 'text_emb_138', 'text_emb_139', 'text_emb_140', 'text_emb_141', 'text_emb_142', 'text_emb_143', 'text_emb_144', 'text_emb_145', 'text_emb_146', 'text_emb_147', 'text_emb_148', 'text_emb_149', 'text_emb_150', 'text_emb_151', 'text_emb_152', 'text_emb_153', 'text_emb_154', 'text_emb_155', 'text_emb_156', 'text_emb_157', 'text_emb_158', 'text_emb_159', 'text_emb_160', 'text_emb_161', 'text_emb_162', 'text_emb_163', 'text_emb_164', 'text_emb_165', 'text_emb_166', 'text_emb_167', 'text_emb_168', 'text_emb_169', 'text_emb_170', 'text_emb_171', 'text_emb_172', 'text_emb_173', 'text_emb_174', 'text_emb_175', 'text_emb_176', 'text_emb_177', 'text_emb_178', 'text_emb_179', 'text_emb_180', 'text_emb_181', 'text_emb_182', 'text_emb_183', 'text_emb_184', 'text_emb_185', 'text_emb_186', 'text_emb_187', 'text_emb_188', 'text_emb_189', 'text_emb_190', 'text_emb_191', 'text_emb_192', 'text_emb_193', 'text_emb_194', 'text_emb_195', 'text_emb_196', 'text_emb_197', 'text_emb_198', 'text_emb_199', 'text_emb_200', 'text_emb_201', 'text_emb_202', 'text_emb_203', 'text_emb_204', 'text_emb_205', 'text_emb_206', 'text_emb_207', 'text_emb_208', 'text_emb_209', 'text_emb_210', 'text_emb_211', 'text_emb_212', 'text_emb_213', 'text_emb_214', 'text_emb_215', 'text_emb_216', 'text_emb_217', 'text_emb_218', 'text_emb_219', 'text_emb_220', 'text_emb_221', 'text_emb_222', 'text_emb_223', 'text_emb_224', 'text_emb_225', 'text_emb_226', 'text_emb_227', 'text_emb_228', 'text_emb_229', 'text_emb_230', 'text_emb_231', 'text_emb_232', 'text_emb_233', 'text_emb_234', 'text_emb_235', 'text_emb_236', 'text_emb_237', 'text_emb_238', 'text_emb_239', 'text_emb_240', 'text_emb_241', 'text_emb_242', 'text_emb_243', 'text_emb_244', 'text_emb_245', 'text_emb_246', 'text_emb_247', 'text_emb_248', 'text_emb_249', 'text_emb_250', 'text_emb_251', 'text_emb_252', 'text_emb_253', 'text_emb_254', 'text_emb_255', 'text_emb_256', 'text_emb_257', 'text_emb_258', 'text_emb_259', 'text_emb_260', 'text_emb_261', 'text_emb_262', 'text_emb_263', 'text_emb_264', 'text_emb_265', 'text_emb_266', 'text_emb_267', 'text_emb_268', 'text_emb_269', 'text_emb_270', 'text_emb_271', 'text_emb_272', 'text_emb_273', 'text_emb_274', 'text_emb_275', 'text_emb_276', 'text_emb_277', 'text_emb_278', 'text_emb_279', 'text_emb_280', 'text_emb_281', 'text_emb_282', 'text_emb_283', 'text_emb_284', 'text_emb_285', 'text_emb_286', 'text_emb_287', 'text_emb_288', 'text_emb_289', 'text_emb_290', 'text_emb_291', 'text_emb_292', 'text_emb_293', 'text_emb_294', 'text_emb_295', 'text_emb_296', 'text_emb_297', 'text_emb_298', 'text_emb_299', 'text_emb_300', 'text_emb_301', 'text_emb_302', 'text_emb_303', 'text_emb_304', 'text_emb_305', 'text_emb_306', 'text_emb_307', 'text_emb_308', 'text_emb_309', 'text_emb_310', 'text_emb_311', 'text_emb_312', 'text_emb_313', 'text_emb_314', 'text_emb_315', 'text_emb_316', 'text_emb_317', 'text_emb_318', 'text_emb_319', 'text_emb_320', 'text_emb_321', 'text_emb_322', 'text_emb_323', 'text_emb_324', 'text_emb_325', 'text_emb_326', 'text_emb_327', 'text_emb_328', 'text_emb_329', 'text_emb_330', 'text_emb_331', 'text_emb_332', 'text_emb_333', 'text_emb_334', 'text_emb_335', 'text_emb_336', 'text_emb_337', 'text_emb_338', 'text_emb_339', 'text_emb_340', 'text_emb_341', 'text_emb_342', 'text_emb_343', 'text_emb_344', 'text_emb_345', 'text_emb_346', 'text_emb_347', 'text_emb_348', 'text_emb_349', 'text_emb_350', 'text_emb_351', 'text_emb_352', 'text_emb_353', 'text_emb_354', 'text_emb_355', 'text_emb_356', 'text_emb_357', 'text_emb_358', 'text_emb_359', 'text_emb_360', 'text_emb_361', 'text_emb_362', 'text_emb_363', 'text_emb_364', 'text_emb_365', 'text_emb_366', 'text_emb_367', 'text_emb_368', 'text_emb_369', 'text_emb_370', 'text_emb_371', 'text_emb_372', 'text_emb_373', 'text_emb_374', 'text_emb_375', 'text_emb_376', 'text_emb_377', 'text_emb_378', 'text_emb_379', 'text_emb_380', 'text_emb_381', 'text_emb_382', 'text_emb_383', 'Narration_Cluster', 'Narration_Cluster_Label', 'Accounting_Month', 'Accounting_Weekday', 'Invoice_Month', 'Invoice_Weekday', 'Posted_Month', 'Posted_Weekday', 'Model_Score', 'Final_Score', 'CP_01', 'CP_02', 'CP_19', 'Currency', 'CP_Score', 'Triggered_CPs']\n",
      "--------------------------------------------------------------------------------\n",
      "Creating internal risk classifications...\n",
      "Created 874 internal classifications\n",
      "Risk distribution: High=0, Medium=167, Low=707\n",
      "\n",
      "=== AFTER INTERNAL CLASSIFICATIONS ===\n",
      "Total columns: 431\n",
      "Column names: ['S. No', 'Entity Name', 'Accounting Date', 'Approval Type', 'Document Type', 'Invoice Date', 'Day', 'Nature', 'Account Code', 'PL/ BS', 'Report Group', 'Account Name', 'Nature in balance sheet', 'Document Number', 'Je Line Num', 'Source Number', 'Source Name', 'Source Voucher Name', 'Source Desc', 'Line Desc', 'Project Code', 'Internal Reference', 'Posted Date', 'Branch', 'Batch Name', 'Entered Dr SUM', 'Entered Cr SUM', 'Accounted Dr SUM', 'Accounted Cr SUM', 'Net Amount', 'Combined_Text', 'text_emb_0', 'text_emb_1', 'text_emb_2', 'text_emb_3', 'text_emb_4', 'text_emb_5', 'text_emb_6', 'text_emb_7', 'text_emb_8', 'text_emb_9', 'text_emb_10', 'text_emb_11', 'text_emb_12', 'text_emb_13', 'text_emb_14', 'text_emb_15', 'text_emb_16', 'text_emb_17', 'text_emb_18', 'text_emb_19', 'text_emb_20', 'text_emb_21', 'text_emb_22', 'text_emb_23', 'text_emb_24', 'text_emb_25', 'text_emb_26', 'text_emb_27', 'text_emb_28', 'text_emb_29', 'text_emb_30', 'text_emb_31', 'text_emb_32', 'text_emb_33', 'text_emb_34', 'text_emb_35', 'text_emb_36', 'text_emb_37', 'text_emb_38', 'text_emb_39', 'text_emb_40', 'text_emb_41', 'text_emb_42', 'text_emb_43', 'text_emb_44', 'text_emb_45', 'text_emb_46', 'text_emb_47', 'text_emb_48', 'text_emb_49', 'text_emb_50', 'text_emb_51', 'text_emb_52', 'text_emb_53', 'text_emb_54', 'text_emb_55', 'text_emb_56', 'text_emb_57', 'text_emb_58', 'text_emb_59', 'text_emb_60', 'text_emb_61', 'text_emb_62', 'text_emb_63', 'text_emb_64', 'text_emb_65', 'text_emb_66', 'text_emb_67', 'text_emb_68', 'text_emb_69', 'text_emb_70', 'text_emb_71', 'text_emb_72', 'text_emb_73', 'text_emb_74', 'text_emb_75', 'text_emb_76', 'text_emb_77', 'text_emb_78', 'text_emb_79', 'text_emb_80', 'text_emb_81', 'text_emb_82', 'text_emb_83', 'text_emb_84', 'text_emb_85', 'text_emb_86', 'text_emb_87', 'text_emb_88', 'text_emb_89', 'text_emb_90', 'text_emb_91', 'text_emb_92', 'text_emb_93', 'text_emb_94', 'text_emb_95', 'text_emb_96', 'text_emb_97', 'text_emb_98', 'text_emb_99', 'text_emb_100', 'text_emb_101', 'text_emb_102', 'text_emb_103', 'text_emb_104', 'text_emb_105', 'text_emb_106', 'text_emb_107', 'text_emb_108', 'text_emb_109', 'text_emb_110', 'text_emb_111', 'text_emb_112', 'text_emb_113', 'text_emb_114', 'text_emb_115', 'text_emb_116', 'text_emb_117', 'text_emb_118', 'text_emb_119', 'text_emb_120', 'text_emb_121', 'text_emb_122', 'text_emb_123', 'text_emb_124', 'text_emb_125', 'text_emb_126', 'text_emb_127', 'text_emb_128', 'text_emb_129', 'text_emb_130', 'text_emb_131', 'text_emb_132', 'text_emb_133', 'text_emb_134', 'text_emb_135', 'text_emb_136', 'text_emb_137', 'text_emb_138', 'text_emb_139', 'text_emb_140', 'text_emb_141', 'text_emb_142', 'text_emb_143', 'text_emb_144', 'text_emb_145', 'text_emb_146', 'text_emb_147', 'text_emb_148', 'text_emb_149', 'text_emb_150', 'text_emb_151', 'text_emb_152', 'text_emb_153', 'text_emb_154', 'text_emb_155', 'text_emb_156', 'text_emb_157', 'text_emb_158', 'text_emb_159', 'text_emb_160', 'text_emb_161', 'text_emb_162', 'text_emb_163', 'text_emb_164', 'text_emb_165', 'text_emb_166', 'text_emb_167', 'text_emb_168', 'text_emb_169', 'text_emb_170', 'text_emb_171', 'text_emb_172', 'text_emb_173', 'text_emb_174', 'text_emb_175', 'text_emb_176', 'text_emb_177', 'text_emb_178', 'text_emb_179', 'text_emb_180', 'text_emb_181', 'text_emb_182', 'text_emb_183', 'text_emb_184', 'text_emb_185', 'text_emb_186', 'text_emb_187', 'text_emb_188', 'text_emb_189', 'text_emb_190', 'text_emb_191', 'text_emb_192', 'text_emb_193', 'text_emb_194', 'text_emb_195', 'text_emb_196', 'text_emb_197', 'text_emb_198', 'text_emb_199', 'text_emb_200', 'text_emb_201', 'text_emb_202', 'text_emb_203', 'text_emb_204', 'text_emb_205', 'text_emb_206', 'text_emb_207', 'text_emb_208', 'text_emb_209', 'text_emb_210', 'text_emb_211', 'text_emb_212', 'text_emb_213', 'text_emb_214', 'text_emb_215', 'text_emb_216', 'text_emb_217', 'text_emb_218', 'text_emb_219', 'text_emb_220', 'text_emb_221', 'text_emb_222', 'text_emb_223', 'text_emb_224', 'text_emb_225', 'text_emb_226', 'text_emb_227', 'text_emb_228', 'text_emb_229', 'text_emb_230', 'text_emb_231', 'text_emb_232', 'text_emb_233', 'text_emb_234', 'text_emb_235', 'text_emb_236', 'text_emb_237', 'text_emb_238', 'text_emb_239', 'text_emb_240', 'text_emb_241', 'text_emb_242', 'text_emb_243', 'text_emb_244', 'text_emb_245', 'text_emb_246', 'text_emb_247', 'text_emb_248', 'text_emb_249', 'text_emb_250', 'text_emb_251', 'text_emb_252', 'text_emb_253', 'text_emb_254', 'text_emb_255', 'text_emb_256', 'text_emb_257', 'text_emb_258', 'text_emb_259', 'text_emb_260', 'text_emb_261', 'text_emb_262', 'text_emb_263', 'text_emb_264', 'text_emb_265', 'text_emb_266', 'text_emb_267', 'text_emb_268', 'text_emb_269', 'text_emb_270', 'text_emb_271', 'text_emb_272', 'text_emb_273', 'text_emb_274', 'text_emb_275', 'text_emb_276', 'text_emb_277', 'text_emb_278', 'text_emb_279', 'text_emb_280', 'text_emb_281', 'text_emb_282', 'text_emb_283', 'text_emb_284', 'text_emb_285', 'text_emb_286', 'text_emb_287', 'text_emb_288', 'text_emb_289', 'text_emb_290', 'text_emb_291', 'text_emb_292', 'text_emb_293', 'text_emb_294', 'text_emb_295', 'text_emb_296', 'text_emb_297', 'text_emb_298', 'text_emb_299', 'text_emb_300', 'text_emb_301', 'text_emb_302', 'text_emb_303', 'text_emb_304', 'text_emb_305', 'text_emb_306', 'text_emb_307', 'text_emb_308', 'text_emb_309', 'text_emb_310', 'text_emb_311', 'text_emb_312', 'text_emb_313', 'text_emb_314', 'text_emb_315', 'text_emb_316', 'text_emb_317', 'text_emb_318', 'text_emb_319', 'text_emb_320', 'text_emb_321', 'text_emb_322', 'text_emb_323', 'text_emb_324', 'text_emb_325', 'text_emb_326', 'text_emb_327', 'text_emb_328', 'text_emb_329', 'text_emb_330', 'text_emb_331', 'text_emb_332', 'text_emb_333', 'text_emb_334', 'text_emb_335', 'text_emb_336', 'text_emb_337', 'text_emb_338', 'text_emb_339', 'text_emb_340', 'text_emb_341', 'text_emb_342', 'text_emb_343', 'text_emb_344', 'text_emb_345', 'text_emb_346', 'text_emb_347', 'text_emb_348', 'text_emb_349', 'text_emb_350', 'text_emb_351', 'text_emb_352', 'text_emb_353', 'text_emb_354', 'text_emb_355', 'text_emb_356', 'text_emb_357', 'text_emb_358', 'text_emb_359', 'text_emb_360', 'text_emb_361', 'text_emb_362', 'text_emb_363', 'text_emb_364', 'text_emb_365', 'text_emb_366', 'text_emb_367', 'text_emb_368', 'text_emb_369', 'text_emb_370', 'text_emb_371', 'text_emb_372', 'text_emb_373', 'text_emb_374', 'text_emb_375', 'text_emb_376', 'text_emb_377', 'text_emb_378', 'text_emb_379', 'text_emb_380', 'text_emb_381', 'text_emb_382', 'text_emb_383', 'Narration_Cluster', 'Narration_Cluster_Label', 'Accounting_Month', 'Accounting_Weekday', 'Invoice_Month', 'Invoice_Weekday', 'Posted_Month', 'Posted_Weekday', 'Model_Score', 'Final_Score', 'CP_01', 'CP_02', 'CP_19', 'Currency', 'CP_Score', 'Triggered_CPs']\n",
      "--------------------------------------------------------------------------------\n",
      "Creating explanation columns...\n",
      "\n",
      "=== AFTER ADDING Top_Risky_Feature_Groups ===\n",
      "Total columns: 432\n",
      "Column names: ['S. No', 'Entity Name', 'Accounting Date', 'Approval Type', 'Document Type', 'Invoice Date', 'Day', 'Nature', 'Account Code', 'PL/ BS', 'Report Group', 'Account Name', 'Nature in balance sheet', 'Document Number', 'Je Line Num', 'Source Number', 'Source Name', 'Source Voucher Name', 'Source Desc', 'Line Desc', 'Project Code', 'Internal Reference', 'Posted Date', 'Branch', 'Batch Name', 'Entered Dr SUM', 'Entered Cr SUM', 'Accounted Dr SUM', 'Accounted Cr SUM', 'Net Amount', 'Combined_Text', 'text_emb_0', 'text_emb_1', 'text_emb_2', 'text_emb_3', 'text_emb_4', 'text_emb_5', 'text_emb_6', 'text_emb_7', 'text_emb_8', 'text_emb_9', 'text_emb_10', 'text_emb_11', 'text_emb_12', 'text_emb_13', 'text_emb_14', 'text_emb_15', 'text_emb_16', 'text_emb_17', 'text_emb_18', 'text_emb_19', 'text_emb_20', 'text_emb_21', 'text_emb_22', 'text_emb_23', 'text_emb_24', 'text_emb_25', 'text_emb_26', 'text_emb_27', 'text_emb_28', 'text_emb_29', 'text_emb_30', 'text_emb_31', 'text_emb_32', 'text_emb_33', 'text_emb_34', 'text_emb_35', 'text_emb_36', 'text_emb_37', 'text_emb_38', 'text_emb_39', 'text_emb_40', 'text_emb_41', 'text_emb_42', 'text_emb_43', 'text_emb_44', 'text_emb_45', 'text_emb_46', 'text_emb_47', 'text_emb_48', 'text_emb_49', 'text_emb_50', 'text_emb_51', 'text_emb_52', 'text_emb_53', 'text_emb_54', 'text_emb_55', 'text_emb_56', 'text_emb_57', 'text_emb_58', 'text_emb_59', 'text_emb_60', 'text_emb_61', 'text_emb_62', 'text_emb_63', 'text_emb_64', 'text_emb_65', 'text_emb_66', 'text_emb_67', 'text_emb_68', 'text_emb_69', 'text_emb_70', 'text_emb_71', 'text_emb_72', 'text_emb_73', 'text_emb_74', 'text_emb_75', 'text_emb_76', 'text_emb_77', 'text_emb_78', 'text_emb_79', 'text_emb_80', 'text_emb_81', 'text_emb_82', 'text_emb_83', 'text_emb_84', 'text_emb_85', 'text_emb_86', 'text_emb_87', 'text_emb_88', 'text_emb_89', 'text_emb_90', 'text_emb_91', 'text_emb_92', 'text_emb_93', 'text_emb_94', 'text_emb_95', 'text_emb_96', 'text_emb_97', 'text_emb_98', 'text_emb_99', 'text_emb_100', 'text_emb_101', 'text_emb_102', 'text_emb_103', 'text_emb_104', 'text_emb_105', 'text_emb_106', 'text_emb_107', 'text_emb_108', 'text_emb_109', 'text_emb_110', 'text_emb_111', 'text_emb_112', 'text_emb_113', 'text_emb_114', 'text_emb_115', 'text_emb_116', 'text_emb_117', 'text_emb_118', 'text_emb_119', 'text_emb_120', 'text_emb_121', 'text_emb_122', 'text_emb_123', 'text_emb_124', 'text_emb_125', 'text_emb_126', 'text_emb_127', 'text_emb_128', 'text_emb_129', 'text_emb_130', 'text_emb_131', 'text_emb_132', 'text_emb_133', 'text_emb_134', 'text_emb_135', 'text_emb_136', 'text_emb_137', 'text_emb_138', 'text_emb_139', 'text_emb_140', 'text_emb_141', 'text_emb_142', 'text_emb_143', 'text_emb_144', 'text_emb_145', 'text_emb_146', 'text_emb_147', 'text_emb_148', 'text_emb_149', 'text_emb_150', 'text_emb_151', 'text_emb_152', 'text_emb_153', 'text_emb_154', 'text_emb_155', 'text_emb_156', 'text_emb_157', 'text_emb_158', 'text_emb_159', 'text_emb_160', 'text_emb_161', 'text_emb_162', 'text_emb_163', 'text_emb_164', 'text_emb_165', 'text_emb_166', 'text_emb_167', 'text_emb_168', 'text_emb_169', 'text_emb_170', 'text_emb_171', 'text_emb_172', 'text_emb_173', 'text_emb_174', 'text_emb_175', 'text_emb_176', 'text_emb_177', 'text_emb_178', 'text_emb_179', 'text_emb_180', 'text_emb_181', 'text_emb_182', 'text_emb_183', 'text_emb_184', 'text_emb_185', 'text_emb_186', 'text_emb_187', 'text_emb_188', 'text_emb_189', 'text_emb_190', 'text_emb_191', 'text_emb_192', 'text_emb_193', 'text_emb_194', 'text_emb_195', 'text_emb_196', 'text_emb_197', 'text_emb_198', 'text_emb_199', 'text_emb_200', 'text_emb_201', 'text_emb_202', 'text_emb_203', 'text_emb_204', 'text_emb_205', 'text_emb_206', 'text_emb_207', 'text_emb_208', 'text_emb_209', 'text_emb_210', 'text_emb_211', 'text_emb_212', 'text_emb_213', 'text_emb_214', 'text_emb_215', 'text_emb_216', 'text_emb_217', 'text_emb_218', 'text_emb_219', 'text_emb_220', 'text_emb_221', 'text_emb_222', 'text_emb_223', 'text_emb_224', 'text_emb_225', 'text_emb_226', 'text_emb_227', 'text_emb_228', 'text_emb_229', 'text_emb_230', 'text_emb_231', 'text_emb_232', 'text_emb_233', 'text_emb_234', 'text_emb_235', 'text_emb_236', 'text_emb_237', 'text_emb_238', 'text_emb_239', 'text_emb_240', 'text_emb_241', 'text_emb_242', 'text_emb_243', 'text_emb_244', 'text_emb_245', 'text_emb_246', 'text_emb_247', 'text_emb_248', 'text_emb_249', 'text_emb_250', 'text_emb_251', 'text_emb_252', 'text_emb_253', 'text_emb_254', 'text_emb_255', 'text_emb_256', 'text_emb_257', 'text_emb_258', 'text_emb_259', 'text_emb_260', 'text_emb_261', 'text_emb_262', 'text_emb_263', 'text_emb_264', 'text_emb_265', 'text_emb_266', 'text_emb_267', 'text_emb_268', 'text_emb_269', 'text_emb_270', 'text_emb_271', 'text_emb_272', 'text_emb_273', 'text_emb_274', 'text_emb_275', 'text_emb_276', 'text_emb_277', 'text_emb_278', 'text_emb_279', 'text_emb_280', 'text_emb_281', 'text_emb_282', 'text_emb_283', 'text_emb_284', 'text_emb_285', 'text_emb_286', 'text_emb_287', 'text_emb_288', 'text_emb_289', 'text_emb_290', 'text_emb_291', 'text_emb_292', 'text_emb_293', 'text_emb_294', 'text_emb_295', 'text_emb_296', 'text_emb_297', 'text_emb_298', 'text_emb_299', 'text_emb_300', 'text_emb_301', 'text_emb_302', 'text_emb_303', 'text_emb_304', 'text_emb_305', 'text_emb_306', 'text_emb_307', 'text_emb_308', 'text_emb_309', 'text_emb_310', 'text_emb_311', 'text_emb_312', 'text_emb_313', 'text_emb_314', 'text_emb_315', 'text_emb_316', 'text_emb_317', 'text_emb_318', 'text_emb_319', 'text_emb_320', 'text_emb_321', 'text_emb_322', 'text_emb_323', 'text_emb_324', 'text_emb_325', 'text_emb_326', 'text_emb_327', 'text_emb_328', 'text_emb_329', 'text_emb_330', 'text_emb_331', 'text_emb_332', 'text_emb_333', 'text_emb_334', 'text_emb_335', 'text_emb_336', 'text_emb_337', 'text_emb_338', 'text_emb_339', 'text_emb_340', 'text_emb_341', 'text_emb_342', 'text_emb_343', 'text_emb_344', 'text_emb_345', 'text_emb_346', 'text_emb_347', 'text_emb_348', 'text_emb_349', 'text_emb_350', 'text_emb_351', 'text_emb_352', 'text_emb_353', 'text_emb_354', 'text_emb_355', 'text_emb_356', 'text_emb_357', 'text_emb_358', 'text_emb_359', 'text_emb_360', 'text_emb_361', 'text_emb_362', 'text_emb_363', 'text_emb_364', 'text_emb_365', 'text_emb_366', 'text_emb_367', 'text_emb_368', 'text_emb_369', 'text_emb_370', 'text_emb_371', 'text_emb_372', 'text_emb_373', 'text_emb_374', 'text_emb_375', 'text_emb_376', 'text_emb_377', 'text_emb_378', 'text_emb_379', 'text_emb_380', 'text_emb_381', 'text_emb_382', 'text_emb_383', 'Narration_Cluster', 'Narration_Cluster_Label', 'Accounting_Month', 'Accounting_Weekday', 'Invoice_Month', 'Invoice_Weekday', 'Posted_Month', 'Posted_Weekday', 'Model_Score', 'Final_Score', 'CP_01', 'CP_02', 'CP_19', 'Currency', 'CP_Score', 'Triggered_CPs', 'Top_Risky_Feature_Groups']\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "=== AFTER ADDING Top_Safe_Feature_Groups ===\n",
      "Total columns: 433\n",
      "Column names: ['S. No', 'Entity Name', 'Accounting Date', 'Approval Type', 'Document Type', 'Invoice Date', 'Day', 'Nature', 'Account Code', 'PL/ BS', 'Report Group', 'Account Name', 'Nature in balance sheet', 'Document Number', 'Je Line Num', 'Source Number', 'Source Name', 'Source Voucher Name', 'Source Desc', 'Line Desc', 'Project Code', 'Internal Reference', 'Posted Date', 'Branch', 'Batch Name', 'Entered Dr SUM', 'Entered Cr SUM', 'Accounted Dr SUM', 'Accounted Cr SUM', 'Net Amount', 'Combined_Text', 'text_emb_0', 'text_emb_1', 'text_emb_2', 'text_emb_3', 'text_emb_4', 'text_emb_5', 'text_emb_6', 'text_emb_7', 'text_emb_8', 'text_emb_9', 'text_emb_10', 'text_emb_11', 'text_emb_12', 'text_emb_13', 'text_emb_14', 'text_emb_15', 'text_emb_16', 'text_emb_17', 'text_emb_18', 'text_emb_19', 'text_emb_20', 'text_emb_21', 'text_emb_22', 'text_emb_23', 'text_emb_24', 'text_emb_25', 'text_emb_26', 'text_emb_27', 'text_emb_28', 'text_emb_29', 'text_emb_30', 'text_emb_31', 'text_emb_32', 'text_emb_33', 'text_emb_34', 'text_emb_35', 'text_emb_36', 'text_emb_37', 'text_emb_38', 'text_emb_39', 'text_emb_40', 'text_emb_41', 'text_emb_42', 'text_emb_43', 'text_emb_44', 'text_emb_45', 'text_emb_46', 'text_emb_47', 'text_emb_48', 'text_emb_49', 'text_emb_50', 'text_emb_51', 'text_emb_52', 'text_emb_53', 'text_emb_54', 'text_emb_55', 'text_emb_56', 'text_emb_57', 'text_emb_58', 'text_emb_59', 'text_emb_60', 'text_emb_61', 'text_emb_62', 'text_emb_63', 'text_emb_64', 'text_emb_65', 'text_emb_66', 'text_emb_67', 'text_emb_68', 'text_emb_69', 'text_emb_70', 'text_emb_71', 'text_emb_72', 'text_emb_73', 'text_emb_74', 'text_emb_75', 'text_emb_76', 'text_emb_77', 'text_emb_78', 'text_emb_79', 'text_emb_80', 'text_emb_81', 'text_emb_82', 'text_emb_83', 'text_emb_84', 'text_emb_85', 'text_emb_86', 'text_emb_87', 'text_emb_88', 'text_emb_89', 'text_emb_90', 'text_emb_91', 'text_emb_92', 'text_emb_93', 'text_emb_94', 'text_emb_95', 'text_emb_96', 'text_emb_97', 'text_emb_98', 'text_emb_99', 'text_emb_100', 'text_emb_101', 'text_emb_102', 'text_emb_103', 'text_emb_104', 'text_emb_105', 'text_emb_106', 'text_emb_107', 'text_emb_108', 'text_emb_109', 'text_emb_110', 'text_emb_111', 'text_emb_112', 'text_emb_113', 'text_emb_114', 'text_emb_115', 'text_emb_116', 'text_emb_117', 'text_emb_118', 'text_emb_119', 'text_emb_120', 'text_emb_121', 'text_emb_122', 'text_emb_123', 'text_emb_124', 'text_emb_125', 'text_emb_126', 'text_emb_127', 'text_emb_128', 'text_emb_129', 'text_emb_130', 'text_emb_131', 'text_emb_132', 'text_emb_133', 'text_emb_134', 'text_emb_135', 'text_emb_136', 'text_emb_137', 'text_emb_138', 'text_emb_139', 'text_emb_140', 'text_emb_141', 'text_emb_142', 'text_emb_143', 'text_emb_144', 'text_emb_145', 'text_emb_146', 'text_emb_147', 'text_emb_148', 'text_emb_149', 'text_emb_150', 'text_emb_151', 'text_emb_152', 'text_emb_153', 'text_emb_154', 'text_emb_155', 'text_emb_156', 'text_emb_157', 'text_emb_158', 'text_emb_159', 'text_emb_160', 'text_emb_161', 'text_emb_162', 'text_emb_163', 'text_emb_164', 'text_emb_165', 'text_emb_166', 'text_emb_167', 'text_emb_168', 'text_emb_169', 'text_emb_170', 'text_emb_171', 'text_emb_172', 'text_emb_173', 'text_emb_174', 'text_emb_175', 'text_emb_176', 'text_emb_177', 'text_emb_178', 'text_emb_179', 'text_emb_180', 'text_emb_181', 'text_emb_182', 'text_emb_183', 'text_emb_184', 'text_emb_185', 'text_emb_186', 'text_emb_187', 'text_emb_188', 'text_emb_189', 'text_emb_190', 'text_emb_191', 'text_emb_192', 'text_emb_193', 'text_emb_194', 'text_emb_195', 'text_emb_196', 'text_emb_197', 'text_emb_198', 'text_emb_199', 'text_emb_200', 'text_emb_201', 'text_emb_202', 'text_emb_203', 'text_emb_204', 'text_emb_205', 'text_emb_206', 'text_emb_207', 'text_emb_208', 'text_emb_209', 'text_emb_210', 'text_emb_211', 'text_emb_212', 'text_emb_213', 'text_emb_214', 'text_emb_215', 'text_emb_216', 'text_emb_217', 'text_emb_218', 'text_emb_219', 'text_emb_220', 'text_emb_221', 'text_emb_222', 'text_emb_223', 'text_emb_224', 'text_emb_225', 'text_emb_226', 'text_emb_227', 'text_emb_228', 'text_emb_229', 'text_emb_230', 'text_emb_231', 'text_emb_232', 'text_emb_233', 'text_emb_234', 'text_emb_235', 'text_emb_236', 'text_emb_237', 'text_emb_238', 'text_emb_239', 'text_emb_240', 'text_emb_241', 'text_emb_242', 'text_emb_243', 'text_emb_244', 'text_emb_245', 'text_emb_246', 'text_emb_247', 'text_emb_248', 'text_emb_249', 'text_emb_250', 'text_emb_251', 'text_emb_252', 'text_emb_253', 'text_emb_254', 'text_emb_255', 'text_emb_256', 'text_emb_257', 'text_emb_258', 'text_emb_259', 'text_emb_260', 'text_emb_261', 'text_emb_262', 'text_emb_263', 'text_emb_264', 'text_emb_265', 'text_emb_266', 'text_emb_267', 'text_emb_268', 'text_emb_269', 'text_emb_270', 'text_emb_271', 'text_emb_272', 'text_emb_273', 'text_emb_274', 'text_emb_275', 'text_emb_276', 'text_emb_277', 'text_emb_278', 'text_emb_279', 'text_emb_280', 'text_emb_281', 'text_emb_282', 'text_emb_283', 'text_emb_284', 'text_emb_285', 'text_emb_286', 'text_emb_287', 'text_emb_288', 'text_emb_289', 'text_emb_290', 'text_emb_291', 'text_emb_292', 'text_emb_293', 'text_emb_294', 'text_emb_295', 'text_emb_296', 'text_emb_297', 'text_emb_298', 'text_emb_299', 'text_emb_300', 'text_emb_301', 'text_emb_302', 'text_emb_303', 'text_emb_304', 'text_emb_305', 'text_emb_306', 'text_emb_307', 'text_emb_308', 'text_emb_309', 'text_emb_310', 'text_emb_311', 'text_emb_312', 'text_emb_313', 'text_emb_314', 'text_emb_315', 'text_emb_316', 'text_emb_317', 'text_emb_318', 'text_emb_319', 'text_emb_320', 'text_emb_321', 'text_emb_322', 'text_emb_323', 'text_emb_324', 'text_emb_325', 'text_emb_326', 'text_emb_327', 'text_emb_328', 'text_emb_329', 'text_emb_330', 'text_emb_331', 'text_emb_332', 'text_emb_333', 'text_emb_334', 'text_emb_335', 'text_emb_336', 'text_emb_337', 'text_emb_338', 'text_emb_339', 'text_emb_340', 'text_emb_341', 'text_emb_342', 'text_emb_343', 'text_emb_344', 'text_emb_345', 'text_emb_346', 'text_emb_347', 'text_emb_348', 'text_emb_349', 'text_emb_350', 'text_emb_351', 'text_emb_352', 'text_emb_353', 'text_emb_354', 'text_emb_355', 'text_emb_356', 'text_emb_357', 'text_emb_358', 'text_emb_359', 'text_emb_360', 'text_emb_361', 'text_emb_362', 'text_emb_363', 'text_emb_364', 'text_emb_365', 'text_emb_366', 'text_emb_367', 'text_emb_368', 'text_emb_369', 'text_emb_370', 'text_emb_371', 'text_emb_372', 'text_emb_373', 'text_emb_374', 'text_emb_375', 'text_emb_376', 'text_emb_377', 'text_emb_378', 'text_emb_379', 'text_emb_380', 'text_emb_381', 'text_emb_382', 'text_emb_383', 'Narration_Cluster', 'Narration_Cluster_Label', 'Accounting_Month', 'Accounting_Weekday', 'Invoice_Month', 'Invoice_Weekday', 'Posted_Month', 'Posted_Weekday', 'Model_Score', 'Final_Score', 'CP_01', 'CP_02', 'CP_19', 'Currency', 'CP_Score', 'Triggered_CPs', 'Top_Risky_Feature_Groups', 'Top_Safe_Feature_Groups']\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "=== AFTER ADDING Explanation_Summary ===\n",
      "Total columns: 434\n",
      "Column names: ['S. No', 'Entity Name', 'Accounting Date', 'Approval Type', 'Document Type', 'Invoice Date', 'Day', 'Nature', 'Account Code', 'PL/ BS', 'Report Group', 'Account Name', 'Nature in balance sheet', 'Document Number', 'Je Line Num', 'Source Number', 'Source Name', 'Source Voucher Name', 'Source Desc', 'Line Desc', 'Project Code', 'Internal Reference', 'Posted Date', 'Branch', 'Batch Name', 'Entered Dr SUM', 'Entered Cr SUM', 'Accounted Dr SUM', 'Accounted Cr SUM', 'Net Amount', 'Combined_Text', 'text_emb_0', 'text_emb_1', 'text_emb_2', 'text_emb_3', 'text_emb_4', 'text_emb_5', 'text_emb_6', 'text_emb_7', 'text_emb_8', 'text_emb_9', 'text_emb_10', 'text_emb_11', 'text_emb_12', 'text_emb_13', 'text_emb_14', 'text_emb_15', 'text_emb_16', 'text_emb_17', 'text_emb_18', 'text_emb_19', 'text_emb_20', 'text_emb_21', 'text_emb_22', 'text_emb_23', 'text_emb_24', 'text_emb_25', 'text_emb_26', 'text_emb_27', 'text_emb_28', 'text_emb_29', 'text_emb_30', 'text_emb_31', 'text_emb_32', 'text_emb_33', 'text_emb_34', 'text_emb_35', 'text_emb_36', 'text_emb_37', 'text_emb_38', 'text_emb_39', 'text_emb_40', 'text_emb_41', 'text_emb_42', 'text_emb_43', 'text_emb_44', 'text_emb_45', 'text_emb_46', 'text_emb_47', 'text_emb_48', 'text_emb_49', 'text_emb_50', 'text_emb_51', 'text_emb_52', 'text_emb_53', 'text_emb_54', 'text_emb_55', 'text_emb_56', 'text_emb_57', 'text_emb_58', 'text_emb_59', 'text_emb_60', 'text_emb_61', 'text_emb_62', 'text_emb_63', 'text_emb_64', 'text_emb_65', 'text_emb_66', 'text_emb_67', 'text_emb_68', 'text_emb_69', 'text_emb_70', 'text_emb_71', 'text_emb_72', 'text_emb_73', 'text_emb_74', 'text_emb_75', 'text_emb_76', 'text_emb_77', 'text_emb_78', 'text_emb_79', 'text_emb_80', 'text_emb_81', 'text_emb_82', 'text_emb_83', 'text_emb_84', 'text_emb_85', 'text_emb_86', 'text_emb_87', 'text_emb_88', 'text_emb_89', 'text_emb_90', 'text_emb_91', 'text_emb_92', 'text_emb_93', 'text_emb_94', 'text_emb_95', 'text_emb_96', 'text_emb_97', 'text_emb_98', 'text_emb_99', 'text_emb_100', 'text_emb_101', 'text_emb_102', 'text_emb_103', 'text_emb_104', 'text_emb_105', 'text_emb_106', 'text_emb_107', 'text_emb_108', 'text_emb_109', 'text_emb_110', 'text_emb_111', 'text_emb_112', 'text_emb_113', 'text_emb_114', 'text_emb_115', 'text_emb_116', 'text_emb_117', 'text_emb_118', 'text_emb_119', 'text_emb_120', 'text_emb_121', 'text_emb_122', 'text_emb_123', 'text_emb_124', 'text_emb_125', 'text_emb_126', 'text_emb_127', 'text_emb_128', 'text_emb_129', 'text_emb_130', 'text_emb_131', 'text_emb_132', 'text_emb_133', 'text_emb_134', 'text_emb_135', 'text_emb_136', 'text_emb_137', 'text_emb_138', 'text_emb_139', 'text_emb_140', 'text_emb_141', 'text_emb_142', 'text_emb_143', 'text_emb_144', 'text_emb_145', 'text_emb_146', 'text_emb_147', 'text_emb_148', 'text_emb_149', 'text_emb_150', 'text_emb_151', 'text_emb_152', 'text_emb_153', 'text_emb_154', 'text_emb_155', 'text_emb_156', 'text_emb_157', 'text_emb_158', 'text_emb_159', 'text_emb_160', 'text_emb_161', 'text_emb_162', 'text_emb_163', 'text_emb_164', 'text_emb_165', 'text_emb_166', 'text_emb_167', 'text_emb_168', 'text_emb_169', 'text_emb_170', 'text_emb_171', 'text_emb_172', 'text_emb_173', 'text_emb_174', 'text_emb_175', 'text_emb_176', 'text_emb_177', 'text_emb_178', 'text_emb_179', 'text_emb_180', 'text_emb_181', 'text_emb_182', 'text_emb_183', 'text_emb_184', 'text_emb_185', 'text_emb_186', 'text_emb_187', 'text_emb_188', 'text_emb_189', 'text_emb_190', 'text_emb_191', 'text_emb_192', 'text_emb_193', 'text_emb_194', 'text_emb_195', 'text_emb_196', 'text_emb_197', 'text_emb_198', 'text_emb_199', 'text_emb_200', 'text_emb_201', 'text_emb_202', 'text_emb_203', 'text_emb_204', 'text_emb_205', 'text_emb_206', 'text_emb_207', 'text_emb_208', 'text_emb_209', 'text_emb_210', 'text_emb_211', 'text_emb_212', 'text_emb_213', 'text_emb_214', 'text_emb_215', 'text_emb_216', 'text_emb_217', 'text_emb_218', 'text_emb_219', 'text_emb_220', 'text_emb_221', 'text_emb_222', 'text_emb_223', 'text_emb_224', 'text_emb_225', 'text_emb_226', 'text_emb_227', 'text_emb_228', 'text_emb_229', 'text_emb_230', 'text_emb_231', 'text_emb_232', 'text_emb_233', 'text_emb_234', 'text_emb_235', 'text_emb_236', 'text_emb_237', 'text_emb_238', 'text_emb_239', 'text_emb_240', 'text_emb_241', 'text_emb_242', 'text_emb_243', 'text_emb_244', 'text_emb_245', 'text_emb_246', 'text_emb_247', 'text_emb_248', 'text_emb_249', 'text_emb_250', 'text_emb_251', 'text_emb_252', 'text_emb_253', 'text_emb_254', 'text_emb_255', 'text_emb_256', 'text_emb_257', 'text_emb_258', 'text_emb_259', 'text_emb_260', 'text_emb_261', 'text_emb_262', 'text_emb_263', 'text_emb_264', 'text_emb_265', 'text_emb_266', 'text_emb_267', 'text_emb_268', 'text_emb_269', 'text_emb_270', 'text_emb_271', 'text_emb_272', 'text_emb_273', 'text_emb_274', 'text_emb_275', 'text_emb_276', 'text_emb_277', 'text_emb_278', 'text_emb_279', 'text_emb_280', 'text_emb_281', 'text_emb_282', 'text_emb_283', 'text_emb_284', 'text_emb_285', 'text_emb_286', 'text_emb_287', 'text_emb_288', 'text_emb_289', 'text_emb_290', 'text_emb_291', 'text_emb_292', 'text_emb_293', 'text_emb_294', 'text_emb_295', 'text_emb_296', 'text_emb_297', 'text_emb_298', 'text_emb_299', 'text_emb_300', 'text_emb_301', 'text_emb_302', 'text_emb_303', 'text_emb_304', 'text_emb_305', 'text_emb_306', 'text_emb_307', 'text_emb_308', 'text_emb_309', 'text_emb_310', 'text_emb_311', 'text_emb_312', 'text_emb_313', 'text_emb_314', 'text_emb_315', 'text_emb_316', 'text_emb_317', 'text_emb_318', 'text_emb_319', 'text_emb_320', 'text_emb_321', 'text_emb_322', 'text_emb_323', 'text_emb_324', 'text_emb_325', 'text_emb_326', 'text_emb_327', 'text_emb_328', 'text_emb_329', 'text_emb_330', 'text_emb_331', 'text_emb_332', 'text_emb_333', 'text_emb_334', 'text_emb_335', 'text_emb_336', 'text_emb_337', 'text_emb_338', 'text_emb_339', 'text_emb_340', 'text_emb_341', 'text_emb_342', 'text_emb_343', 'text_emb_344', 'text_emb_345', 'text_emb_346', 'text_emb_347', 'text_emb_348', 'text_emb_349', 'text_emb_350', 'text_emb_351', 'text_emb_352', 'text_emb_353', 'text_emb_354', 'text_emb_355', 'text_emb_356', 'text_emb_357', 'text_emb_358', 'text_emb_359', 'text_emb_360', 'text_emb_361', 'text_emb_362', 'text_emb_363', 'text_emb_364', 'text_emb_365', 'text_emb_366', 'text_emb_367', 'text_emb_368', 'text_emb_369', 'text_emb_370', 'text_emb_371', 'text_emb_372', 'text_emb_373', 'text_emb_374', 'text_emb_375', 'text_emb_376', 'text_emb_377', 'text_emb_378', 'text_emb_379', 'text_emb_380', 'text_emb_381', 'text_emb_382', 'text_emb_383', 'Narration_Cluster', 'Narration_Cluster_Label', 'Accounting_Month', 'Accounting_Weekday', 'Invoice_Month', 'Invoice_Weekday', 'Posted_Month', 'Posted_Weekday', 'Model_Score', 'Final_Score', 'CP_01', 'CP_02', 'CP_19', 'Currency', 'CP_Score', 'Triggered_CPs', 'Top_Risky_Feature_Groups', 'Top_Safe_Feature_Groups', 'Explanation_Summary']\n",
      "--------------------------------------------------------------------------------\n",
      "Removing BERT embedding columns...\n",
      "Removing 384 BERT columns\n",
      "\n",
      "=== AFTER REMOVING BERT COLUMNS ===\n",
      "Total columns: 50\n",
      "Column names: ['S. No', 'Entity Name', 'Accounting Date', 'Approval Type', 'Document Type', 'Invoice Date', 'Day', 'Nature', 'Account Code', 'PL/ BS', 'Report Group', 'Account Name', 'Nature in balance sheet', 'Document Number', 'Je Line Num', 'Source Number', 'Source Name', 'Source Voucher Name', 'Source Desc', 'Line Desc', 'Project Code', 'Internal Reference', 'Posted Date', 'Branch', 'Batch Name', 'Entered Dr SUM', 'Entered Cr SUM', 'Accounted Dr SUM', 'Accounted Cr SUM', 'Net Amount', 'Combined_Text', 'Narration_Cluster', 'Narration_Cluster_Label', 'Accounting_Month', 'Accounting_Weekday', 'Invoice_Month', 'Invoice_Weekday', 'Posted_Month', 'Posted_Weekday', 'Model_Score', 'Final_Score', 'CP_01', 'CP_02', 'CP_19', 'Currency', 'CP_Score', 'Triggered_CPs', 'Top_Risky_Feature_Groups', 'Top_Safe_Feature_Groups', 'Explanation_Summary']\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "EXPECTED COLUMNS (50): ['S. No', 'Entity Name', 'Accounting Date', 'Approval Type', 'Document Type', 'Invoice Date', 'Day', 'Nature', 'Account Code', 'PL/ BS', 'Report Group', 'Account Name', 'Nature in balance sheet', 'Document Number', 'Je Line Num', 'Source Number', 'Source Name', 'Source Voucher Name', 'Source Desc', 'Line Desc', 'Project Code', 'Internal Reference', 'Posted Date', 'Branch', 'Batch Name', 'Entered Dr SUM', 'Entered Cr SUM', 'Accounted Dr SUM', 'Accounted Cr SUM', 'Net Amount', 'Combined_Text', 'Narration_Cluster', 'Narration_Cluster_Label', 'Accounting_Month', 'Accounting_Weekday', 'Invoice_Month', 'Invoice_Weekday', 'Posted_Month', 'Posted_Weekday', 'Model_Score', 'Final_Score', 'Top_Risky_Feature_Groups', 'Top_Safe_Feature_Groups', 'Explanation_Summary', 'CP_01', 'CP_02', 'CP_19', 'Currency', 'Triggered_CPs', 'CP_Score']\n",
      "\n",
      "ACTUAL COLUMNS (50): ['S. No', 'Entity Name', 'Accounting Date', 'Approval Type', 'Document Type', 'Invoice Date', 'Day', 'Nature', 'Account Code', 'PL/ BS', 'Report Group', 'Account Name', 'Nature in balance sheet', 'Document Number', 'Je Line Num', 'Source Number', 'Source Name', 'Source Voucher Name', 'Source Desc', 'Line Desc', 'Project Code', 'Internal Reference', 'Posted Date', 'Branch', 'Batch Name', 'Entered Dr SUM', 'Entered Cr SUM', 'Accounted Dr SUM', 'Accounted Cr SUM', 'Net Amount', 'Combined_Text', 'Narration_Cluster', 'Narration_Cluster_Label', 'Accounting_Month', 'Accounting_Weekday', 'Invoice_Month', 'Invoice_Weekday', 'Posted_Month', 'Posted_Weekday', 'Model_Score', 'Final_Score', 'CP_01', 'CP_02', 'CP_19', 'Currency', 'CP_Score', 'Triggered_CPs', 'Top_Risky_Feature_Groups', 'Top_Safe_Feature_Groups', 'Explanation_Summary']\n",
      "\n",
      "MISSING FROM ACTUAL: []\n",
      "EXTRA IN ACTUAL: []\n",
      "\n",
      "Attempting final column selection...\n",
      "SUCCESS: Final dataframe has 50 columns\n",
      "\n",
      "=== FINAL OUTPUT ===\n",
      "Total columns: 50\n",
      "Column names: ['S. No', 'Entity Name', 'Accounting Date', 'Approval Type', 'Document Type', 'Invoice Date', 'Day', 'Nature', 'Account Code', 'PL/ BS', 'Report Group', 'Account Name', 'Nature in balance sheet', 'Document Number', 'Je Line Num', 'Source Number', 'Source Name', 'Source Voucher Name', 'Source Desc', 'Line Desc', 'Project Code', 'Internal Reference', 'Posted Date', 'Branch', 'Batch Name', 'Entered Dr SUM', 'Entered Cr SUM', 'Accounted Dr SUM', 'Accounted Cr SUM', 'Net Amount', 'Combined_Text', 'Narration_Cluster', 'Narration_Cluster_Label', 'Accounting_Month', 'Accounting_Weekday', 'Invoice_Month', 'Invoice_Weekday', 'Posted_Month', 'Posted_Weekday', 'Model_Score', 'Final_Score', 'Top_Risky_Feature_Groups', 'Top_Safe_Feature_Groups', 'Explanation_Summary', 'CP_01', 'CP_02', 'CP_19', 'Currency', 'Triggered_CPs', 'CP_Score']\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Copy the entire debug code from the artifact and paste it into a Jupyter cell\n",
    "# Then run this at the bottom:\n",
    "\n",
    "result = run_full_pipeline_debug(\"old_test_file.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fefa62b2-6f55-419a-8cf9-bf1a4482de63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === CORRECTED model_logic.py - 65 Columns with August 25 Integration ===\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import warnings\n",
    "import logging\n",
    "from typing import Dict, List, Tuple, Optional, Union\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# === BERT RISK EXPLAINER CLASS (Internal Use Only) ===\n",
    "class BERTRiskExplainer:\n",
    "    def __init__(self):\n",
    "        self.business_risk_patterns = {\n",
    "            'sunday_payment_processing': {\n",
    "                'trigger': self._check_sunday_payment_processing,\n",
    "                'explanation': \"Sunday payment processing bypassing standard authorization controls\",\n",
    "            },\n",
    "            'vague_account_classification': {\n",
    "                'trigger': self._check_vague_account_classification,\n",
    "                'explanation': \"Vague account classifications lacking transaction specificity\",\n",
    "            },\n",
    "            'high_value_escrow_processing': {\n",
    "                'trigger': self._check_high_value_escrow_processing,\n",
    "                'explanation': \"High-value escrow processing requiring enhanced fiduciary oversight\",\n",
    "            },\n",
    "            'system_integration_processing': {\n",
    "                'trigger': self._check_system_integration_processing,\n",
    "                'explanation': \"System integration processing with data integrity vulnerabilities\",\n",
    "            },\n",
    "            'manual_ecommerce_operations': {\n",
    "                'trigger': self._check_manual_ecommerce_operations,\n",
    "                'explanation': \"Manual e-commerce operations bypassing automated controls\",\n",
    "            },\n",
    "            'cod_settlement_verification': {\n",
    "                'trigger': self._check_cod_settlement_verification,\n",
    "                'explanation': \"COD settlement with logistics coordination timing differences\",\n",
    "            },\n",
    "            'payment_gateway_reconciliation': {\n",
    "                'trigger': self._check_payment_gateway_reconciliation,\n",
    "                'explanation': \"Payment gateway requiring multi-party reconciliation processes\",\n",
    "            },\n",
    "            'revenue_recognition_timing': {\n",
    "                'trigger': self._check_revenue_recognition_timing,\n",
    "                'explanation': \"Revenue recognition timing requiring compliance assessment\",\n",
    "            },\n",
    "            'adjustment_entry_documentation': {\n",
    "                'trigger': self._check_adjustment_entry_documentation,\n",
    "                'explanation': \"Manual adjustments deviating from standard processing workflows\",\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    def _check_sunday_payment_processing(self, row_data: Dict, text: str) -> bool:\n",
    "        try:\n",
    "            day = str(row_data.get('Day', '')).strip()\n",
    "            sunday_days = ['Sun', 'Sunday']\n",
    "            payment_terms = ['wallet', 'hadoop', 'payment', 'cashfree']\n",
    "            return (day in sunday_days and any(term in text.lower() for term in payment_terms))\n",
    "        except: return False\n",
    "    \n",
    "    def _check_vague_account_classification(self, row_data: Dict, text: str) -> bool:\n",
    "        try:\n",
    "            account_name = str(row_data.get('Account Name', '')).lower()\n",
    "            adjustment_terms = ['adjustment', 'settlement', 'liability']\n",
    "            return ('other' in account_name and any(term in text.lower() for term in adjustment_terms))\n",
    "        except: return False\n",
    "    \n",
    "    def _check_high_value_escrow_processing(self, row_data: Dict, text: str) -> bool:\n",
    "        try:\n",
    "            escrow_terms = ['escrow', 'wallet', 'liability']\n",
    "            account_name = str(row_data.get('Account Name', '')).lower()\n",
    "            amount = self._safe_float_conversion(row_data.get('Net Amount', 0))\n",
    "            return (any(term in text.lower() for term in escrow_terms) and\n",
    "                    'escrow' in account_name and abs(amount) > 500000)\n",
    "        except: return False\n",
    "    \n",
    "    def _check_system_integration_processing(self, row_data: Dict, text: str) -> bool:\n",
    "        try:\n",
    "            system_terms = ['hadoop', 'system', 'automated', 'verified', 'matched']\n",
    "            processing_terms = ['processing', 'settlement', 'reconciliation']\n",
    "            return (any(term in text.lower() for term in system_terms) and\n",
    "                    any(term in text.lower() for term in processing_terms))\n",
    "        except: return False\n",
    "    \n",
    "    def _check_manual_ecommerce_operations(self, row_data: Dict, text: str) -> bool:\n",
    "        try:\n",
    "            manual_terms = ['spreadsheet', 'manual']\n",
    "            ecommerce_terms = ['gmv', 'seller', 'rebate', 'voucher']\n",
    "            amount = self._safe_float_conversion(row_data.get('Net Amount', 0))\n",
    "            return (any(term in text.lower() for term in manual_terms) and\n",
    "                    any(term in text.lower() for term in ecommerce_terms) and\n",
    "                    abs(amount) > 1000000)\n",
    "        except: return False\n",
    "    \n",
    "    def _check_cod_settlement_verification(self, row_data: Dict, text: str) -> bool:\n",
    "        try:\n",
    "            cod_terms = ['cod', 'delhivery', 'delivery']\n",
    "            settlement_terms = ['settlement', 'collection', 'payment']\n",
    "            return (any(term in text.lower() for term in cod_terms) and\n",
    "                    any(term in text.lower() for term in settlement_terms))\n",
    "        except: return False\n",
    "    \n",
    "    def _check_payment_gateway_reconciliation(self, row_data: Dict, text: str) -> bool:\n",
    "        try:\n",
    "            gateway_terms = ['cashfree', 'gateway', 'payment']\n",
    "            process_terms = ['settlement', 'reconciliation', 'processing']\n",
    "            return (any(term in text.lower() for term in gateway_terms) and\n",
    "                    any(term in text.lower() for term in process_terms))\n",
    "        except: return False\n",
    "    \n",
    "    def _check_revenue_recognition_timing(self, row_data: Dict, text: str) -> bool:\n",
    "        try:\n",
    "            pl_bs = str(row_data.get('PL/ BS', '')).upper().strip()\n",
    "            revenue_terms = ['revenue', 'income', 'sales']\n",
    "            amount = self._safe_float_conversion(row_data.get('Net Amount', 0))\n",
    "            return (pl_bs == 'PL' and\n",
    "                    any(term in text.lower() for term in revenue_terms) and\n",
    "                    abs(amount) > 500000)\n",
    "        except: return False\n",
    "    \n",
    "    def _check_adjustment_entry_documentation(self, row_data: Dict, text: str) -> bool:\n",
    "        try:\n",
    "            adjustment_terms = ['adjustment', 'correction', 'manual']\n",
    "            process_terms = ['settlement', 'reconciliation', 'variance']\n",
    "            return (any(term in text.lower() for term in adjustment_terms) and\n",
    "                    any(term in text.lower() for term in process_terms))\n",
    "        except: return False\n",
    "    \n",
    "    def _safe_float_conversion(self, value: Union[str, int, float], default: float = 0.0) -> float:\n",
    "        try:\n",
    "            if pd.isna(value) or value is None: return default\n",
    "            return float(value)\n",
    "        except: return default\n",
    "    \n",
    "    def explain_bert_risk(self, transaction_data: Dict, bert_impact: float) -> Optional[str]:\n",
    "        try:\n",
    "            if pd.isna(bert_impact) or bert_impact < 0.05: return None\n",
    "            \n",
    "            text = str(transaction_data.get('Combined_Text', '')).lower().strip()\n",
    "            if not text or text == 'nan': return None\n",
    "            \n",
    "            # Check business risk patterns first\n",
    "            for pattern_name, pattern_config in self.business_risk_patterns.items():\n",
    "                try:\n",
    "                    if pattern_config['trigger'](transaction_data, text):\n",
    "                        return pattern_config['explanation']\n",
    "                except: continue\n",
    "            \n",
    "            # Fallback explanations based on text patterns\n",
    "            if any(term in text for term in ['other', 'miscellaneous', 'various', 'general']):\n",
    "                return \"Vague transaction descriptions lacking specific business purpose\"\n",
    "            elif any(term in text for term in ['spreadsheet', 'manual', 'excel']):\n",
    "                return \"Manual processing bypassing automated control frameworks\"\n",
    "            elif any(term in text for term in ['urgent', 'emergency', 'immediate']):\n",
    "                return \"Urgency indicators suggesting potential workflow bypass\"\n",
    "            else:\n",
    "                return \"Text-based risk patterns requiring enhanced verification\"\n",
    "                \n",
    "        except Exception as e:\n",
    "            return None\n",
    "\n",
    "def calculate_bert_impact(shap_values: np.ndarray, feature_names: List[str]) -> float:\n",
    "    try:\n",
    "        bert_features = [i for i, name in enumerate(feature_names) if name.startswith('text_emb_')]\n",
    "        bert_impact = sum(shap_values[i] for i in bert_features if shap_values[i] > 0)\n",
    "        return bert_impact\n",
    "    except: return 0.0\n",
    "\n",
    "def check_bert_in_top3(shap_values: np.ndarray, feature_names: List[str]) -> Tuple[bool, float]:\n",
    "    try:\n",
    "        feature_impacts = [(feature, shap_val) for feature, shap_val in zip(feature_names, shap_values) if shap_val > 0]\n",
    "        feature_impacts.sort(key=lambda x: x[1], reverse=True)\n",
    "        top_3_features = feature_impacts[:3]\n",
    "        bert_in_top3 = any(feature.startswith('text_emb_') for feature, shap_val in top_3_features)\n",
    "        bert_impact = calculate_bert_impact(shap_values, feature_names)\n",
    "        return bert_in_top3, bert_impact\n",
    "    except: return False, 0.0\n",
    "\n",
    "def get_auditor_friendly_explanation(feature, value, shap_impact):\n",
    "    try:\n",
    "        if feature == \"Day\":\n",
    "            try:\n",
    "                day_num = int(value)\n",
    "                if day_num == 6:  # Sunday\n",
    "                    return \"Transaction processed on Sunday when standard business operations are typically not active\"\n",
    "                else: return None\n",
    "            except: return None\n",
    "        \n",
    "        elif feature == \"Account Name\":\n",
    "            value_str = str(value).lower()\n",
    "            if \"other debtors\" in value_str:\n",
    "                return \"Vague account classification lacking proper transaction specificity\"\n",
    "            elif \"other professional fees\" in value_str:\n",
    "                return \"General professional fee account lacking vendor specificity\"\n",
    "            elif \"legal fee\" in value_str:\n",
    "                return \"Legal fee account susceptible to inappropriate payments\"\n",
    "            elif \"receivables from cod\" in value_str:\n",
    "                return \"Cash-on-delivery receivables requiring enhanced verification\"\n",
    "            elif \"cash in bank\" in value_str:\n",
    "                return \"Cash account requiring verification of bank reconciliations\"\n",
    "            elif \"receivables from payment gateway\" in value_str:\n",
    "                return \"Payment gateway receivables requiring verification of settlement timing\"\n",
    "            else: return None\n",
    "        \n",
    "        elif feature == \"Nature in balance sheet\":\n",
    "            value_str = str(value).lower()\n",
    "            if \"logistic\" in value_str and \"packing\" in value_str:\n",
    "                return \"Logistics expense category prone to cost inflation\"\n",
    "            elif \"legal\" in value_str and \"professional\" in value_str:\n",
    "                return \"Professional services expense susceptible to manipulation\"\n",
    "            elif \"provision\" in value_str:\n",
    "                return \"Provision account lacking detailed substantiation\"\n",
    "            else: return None\n",
    "        \n",
    "        elif feature in [\"Net Amount\", \"Entered Dr SUM\", \"Entered Cr SUM\", \"Accounted Dr SUM\", \"Accounted Cr SUM\"]:\n",
    "            try:\n",
    "                amount = float(value)\n",
    "                if amount >= 10000000000:  # 1000 Cr\n",
    "                    return \"Exceptionally high transaction value representing significant financial exposure\"\n",
    "                elif amount >= 5000000000:  # 500 Cr\n",
    "                    return \"Very high transaction value exceeding typical business thresholds\"\n",
    "                elif amount >= 1000000000:  # 100 Cr\n",
    "                    return \"High transaction value exceeding standard materiality thresholds\"\n",
    "                elif amount >= 500000000:  # 50 Cr\n",
    "                    return \"Material transaction amount warranting enhanced scrutiny\"\n",
    "                else: return None\n",
    "            except: return \"Transaction amount requiring verification due to data quality issues\"\n",
    "        \n",
    "        elif feature == \"Batch Name\":\n",
    "            value_str = str(value).lower()\n",
    "            if \"spreadsheet\" in value_str:\n",
    "                return \"Bulk spreadsheet processing bypassing individual transaction controls\"\n",
    "            else: return None\n",
    "        \n",
    "        elif feature == \"Document Type\":\n",
    "            if str(value) == \"Manual\":\n",
    "                return \"Manual entry increasing error risk and bypassing automated validation\"\n",
    "            elif str(value) == \"Spreadsheet\":\n",
    "                return \"Spreadsheet-based entry bypassing automated controls\"\n",
    "            else: return None\n",
    "        \n",
    "        elif \"Weekday\" in feature:\n",
    "            try:\n",
    "                day_num = int(value)\n",
    "                if day_num == 6:  # Sunday only\n",
    "                    return \"Transaction processed on Sunday when standard business operations are typically not active\"\n",
    "                else: return None\n",
    "            except: return None\n",
    "        \n",
    "        else: return None\n",
    "    \n",
    "    except Exception as e:\n",
    "        return None\n",
    "\n",
    "def run_full_pipeline(file_path: str) -> pd.DataFrame:\n",
    "    logging.debug(\"Starting run_full_pipeline\")\n",
    "    # Lazy load heavy packages here\n",
    "    import shap\n",
    "    from catboost import CatBoostClassifier\n",
    "    from sentence_transformers import SentenceTransformer\n",
    "    from umap import UMAP\n",
    "    from sklearn.cluster import KMeans\n",
    "    import requests\n",
    "    from io import BytesIO\n",
    "\n",
    "    # === Load Models ===\n",
    "    try:\n",
    "        print(\"Loading CatBoost model...\")\n",
    "        model = CatBoostClassifier()\n",
    "        model.load_model(\"models/catboost_v2_model.cbm\")\n",
    "        print(\"CatBoost model loaded\")\n",
    "    except Exception as e:\n",
    "        print(\"Failed to load CatBoost model:\", str(e))\n",
    "        raise\n",
    "\n",
    "    try:\n",
    "        print(\"Downloading SentenceTransformer...\")\n",
    "        model_bert = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "        print(\"SentenceTransformer loaded\")\n",
    "    except Exception as e:\n",
    "        print(\"Failed to load BERT model:\", str(e))\n",
    "        raise\n",
    "\n",
    "    # === Load and Clean Data ===\n",
    "    test_df = pd.read_csv(file_path, encoding='ISO-8859-1')\n",
    "    test_df.columns = test_df.columns.str.strip()\n",
    "\n",
    "    # Clean numeric columns\n",
    "    comma_cols = [\"Entered Dr SUM\", \"Entered Cr SUM\", \"Accounted Dr SUM\", \"Accounted Cr SUM\", \"Net Amount\"]\n",
    "    for col in comma_cols:\n",
    "        if col in test_df.columns:\n",
    "            test_df[col] = test_df[col].astype(str).str.replace(\",\", \"\").replace(\"nan\", np.nan).astype(float)\n",
    "\n",
    "    # Combine text fields\n",
    "    text_fields = [\"Line Desc\", \"Source Desc\", \"Batch Name\"]\n",
    "    test_df[text_fields] = test_df[text_fields].fillna(\"\")\n",
    "    test_df[\"Combined_Text\"] = test_df[\"Line Desc\"] + \" | \" + test_df[\"Source Desc\"] + \" | \" + test_df[\"Batch Name\"]\n",
    "\n",
    "    # === BERT Embeddings and Clustering ===\n",
    "    embeddings = model_bert.encode(test_df[\"Combined_Text\"].tolist(), show_progress_bar=False)\n",
    "    embedding_df = pd.DataFrame(embeddings, columns=[f\"text_emb_{i}\" for i in range(embeddings.shape[1])])\n",
    "    test_df = pd.concat([test_df.reset_index(drop=True), embedding_df], axis=1)\n",
    "\n",
    "    umap_model = UMAP(n_neighbors=15, min_dist=0.1, n_components=2, random_state=42)\n",
    "    reduced = umap_model.fit_transform(embeddings)\n",
    "    kmeans = KMeans(n_clusters=10, random_state=42)\n",
    "    test_df[\"Narration_Cluster\"] = kmeans.fit_predict(reduced)\n",
    "\n",
    "    cluster_summary = (\n",
    "        test_df.groupby(\"Narration_Cluster\")[\"Combined_Text\"]\n",
    "        .apply(lambda x: \"; \".join(x.head(3)))\n",
    "        .reset_index(name=\"Narration_Cluster_Label\")\n",
    "    )\n",
    "    test_df = test_df.merge(cluster_summary, on=\"Narration_Cluster\", how=\"left\")\n",
    "\n",
    "    # === Date Features ===\n",
    "    date_cols = [\"Accounting Date\", \"Invoice Date\", \"Posted Date\"]\n",
    "    for col in date_cols:\n",
    "        test_df[col] = pd.to_datetime(test_df[col], errors=\"coerce\")\n",
    "\n",
    "    test_df[\"Accounting_Month\"] = test_df[\"Accounting Date\"].dt.month\n",
    "    test_df[\"Accounting_Weekday\"] = test_df[\"Accounting Date\"].dt.weekday\n",
    "    test_df[\"Invoice_Month\"] = test_df[\"Invoice Date\"].dt.month\n",
    "    test_df[\"Invoice_Weekday\"] = test_df[\"Invoice Date\"].dt.weekday\n",
    "    test_df[\"Posted_Month\"] = test_df[\"Posted Date\"].dt.month\n",
    "    test_df[\"Posted_Weekday\"] = test_df[\"Posted Date\"].dt.weekday\n",
    "\n",
    "    # === Feature Preparation ===\n",
    "    exclude_cols = [\"S. No\", \"Combined_Text\", \"Accounting Date\", \"Invoice Date\", \"Posted Date\"]\n",
    "    model_feature_names = model.feature_names_\n",
    "    feature_cols = [col for col in test_df.columns if col in model_feature_names and col not in exclude_cols and not col.startswith(\"Unnamed\")]\n",
    "\n",
    "    for col in feature_cols:\n",
    "        if test_df[col].dtype == object or test_df[col].isnull().any():\n",
    "            test_df[col] = test_df[col].astype(str).fillna(\"Missing\")\n",
    "\n",
    "    X_final = test_df[feature_cols].copy()\n",
    "\n",
    "    # === Model Predictions ===\n",
    "    test_df[\"Model_Score\"] = model.predict_proba(X_final)[:, 1]\n",
    "    test_df[\"Final_Score\"] = test_df[\"Model_Score\"].round(3)\n",
    "\n",
    "    # === SHAP Analysis ===\n",
    "    explainer = shap.TreeExplainer(model)\n",
    "    shap_values = explainer.shap_values(X_final)\n",
    "\n",
    "    # === Initialize BERT Explainer ===\n",
    "    bert_explainer = BERTRiskExplainer()\n",
    "\n",
    "    # === Control Points Setup ===\n",
    "    cp_score_dict = {\n",
    "        \"CP_01\": 83, \"CP_02\": 86, \"CP_03\": 78, \"CP_04\": 81, \"CP_07\": 84, \"CP_08\": 80,\n",
    "        \"CP_09\": 76, \"CP_15\": 88, \"CP_16\": 73, \"CP_17\": 75, \"CP_19\": 60,\n",
    "        \"CP_21\": 69, \"CP_22\": 66, \"CP_23\": 87, \"CP_24\": 78, \"CP_26\": 0,\n",
    "        \"CP_30\": 72, \"CP_32\": 72\n",
    "    }\n",
    "    valid_cps = list(cp_score_dict.keys())\n",
    "\n",
    "    pl_net_total = test_df[test_df[\"PL/ BS\"] == \"PL\"][\"Net Amount\"].abs().sum()\n",
    "    pl_net_threshold = 0.10 * pl_net_total\n",
    "    total_net = test_df[\"Net Amount\"].abs().sum()\n",
    "\n",
    "    # === Control Point Functions ===\n",
    "    def cp_01(row):\n",
    "        keywords = ['fraud','bribe','kickback','suspicious','fake','dummy','gift','prize','token','reward','favour']\n",
    "        text = f\"{str(row.get('Line Desc', '')).lower()} {str(row.get('Source Desc', '')).lower()}\"\n",
    "        return int(any(k in text for k in keywords))\n",
    "\n",
    "    def cp_02(row):\n",
    "        return int(row.get(\"PL/ BS\") == \"PL\" and abs(row.get(\"Net Amount\", 0)) > pl_net_threshold)\n",
    "\n",
    "    def cp_03_flags(df):\n",
    "        a = df.duplicated(subset=[\"Accounting Date\", \"Line Desc\", \"Source Desc\", \"Source Name\"], keep=False)\n",
    "        b = df.duplicated(subset=[\"Accounting Date\", \"Account Code\", \"Net Amount\"], keep=False)\n",
    "        c = df.duplicated(subset=[\"Document Number\"], keep=False) & ~df.duplicated(subset=[\"Accounting Date\", \"Document Number\"], keep=False)\n",
    "        d = df.duplicated(subset=[\"Accounting Date\", \"Line Desc\", \"Account Code\"], keep=False)\n",
    "        return ((a | b | c | d).astype(int))\n",
    "\n",
    "    def cp_04(row): return cp_02(row)\n",
    "\n",
    "    def cp_07_flags(df): return (df.groupby(\"Document Number\")[\"Net Amount\"].transform(\"sum\").round(2) != 0).astype(int)\n",
    "\n",
    "    def cp_08(row):\n",
    "        text = f\"{row.get('Account Name', '')} {row.get('Line Desc', '')} {row.get('Source Desc', '')}\".lower()\n",
    "        return int(\"cash in hand\" in text)\n",
    "\n",
    "    def cp_09_flags(df):\n",
    "        result = pd.Series(0, index=df.index)\n",
    "        for doc_id, group in df.groupby(\"Document Number\"):\n",
    "            accs = group[\"Account Name\"].dropna().str.lower().tolist()\n",
    "            if any(\"cash\" in a for a in accs) and any(\"bad debt\" in a for a in accs):\n",
    "                result[group.index] = 1\n",
    "        return result\n",
    "\n",
    "    def cp_15_flags(df):\n",
    "        grp_sum = df.groupby([\"Account Code\", \"Accounting Date\"])[[\"Entered Dr SUM\", \"Entered Cr SUM\"]].sum().sum(axis=1)\n",
    "        keys = grp_sum[grp_sum > 0.03 * total_net].index\n",
    "        return df.set_index([\"Account Code\", \"Accounting Date\"]).index.isin(keys).astype(int)\n",
    "\n",
    "    def cp_16_flags(df):\n",
    "        if \"Currency\" not in df.columns:\n",
    "            df[\"Currency\"] = \"INR\"\n",
    "        docs = df.groupby(\"Document Number\")[\"Currency\"].nunique()\n",
    "        flagged = docs[docs > 1].index\n",
    "        return df[\"Document Number\"].isin(flagged).astype(int)\n",
    "\n",
    "    def cp_17_flags(df):\n",
    "        sums = df[df[\"PL/ BS\"] == \"PL\"].groupby(\"Source Name\")[\"Net Amount\"].sum().abs()\n",
    "        risky = sums[sums > 0.03 * pl_net_total].index\n",
    "        return df[\"Source Name\"].isin(risky).astype(int)\n",
    "\n",
    "    def cp_19(row):\n",
    "        try: return int(pd.to_datetime(row[\"Accounting Date\"]).weekday() == 6)\n",
    "        except: return 0\n",
    "\n",
    "    def cp_21(row):\n",
    "        try:\n",
    "            date = pd.to_datetime(row.get(\"Accounting Date\"))\n",
    "            return int(date == (date + pd.offsets.MonthEnd(0)))\n",
    "        except: return 0\n",
    "\n",
    "    def cp_22(row):\n",
    "        try:\n",
    "            date = pd.to_datetime(row.get(\"Accounting Date\"))\n",
    "            return int(date.day == 1)\n",
    "        except: return 0\n",
    "\n",
    "    def cp_23(row):\n",
    "        text = f\"{row.get('Line Desc', '')} {row.get('Account Name', '')}\".lower()\n",
    "        return int(any(t in text for t in ['derivative', 'spv', 'structured', 'note', 'swap']))\n",
    "\n",
    "    def cp_24(row):\n",
    "        try:\n",
    "            last = str(int(abs(row.get(\"Net Amount\", 0))))[-3:]\n",
    "            seqs = {'123','234','345','456','567','678','789','890','321','432','543','654','765','876','987','098'}\n",
    "            repeats = {str(i)*3 for i in range(10)} | {'000'}\n",
    "            return int(last in seqs or last in repeats and last != '901')\n",
    "        except: return 0\n",
    "\n",
    "    def cp_26_flags(df):\n",
    "        try:\n",
    "            doc_ids = sorted(df[\"Document Number\"].dropna().astype(int).unique())\n",
    "            missing = {doc_ids[i]+1 for i in range(len(doc_ids)-1) if doc_ids[i+1] - doc_ids[i] > 1}\n",
    "            flagged = set()\n",
    "            for miss in missing:\n",
    "                flagged.update([miss-1, miss+1])\n",
    "            return df[\"Document Number\"].astype(int).isin(flagged).astype(int)\n",
    "        except: return pd.Series(0, index=df.index)\n",
    "\n",
    "    def cp_30(row):\n",
    "        text = f\"{row.get('Line Desc', '')} {row.get('Account Name', '')}\".lower()\n",
    "        return int(any(t in text for t in ['derivative','option','swap','future','structured']))\n",
    "\n",
    "    def cp_32(row): return int(row.get(\"Net Amount\", 0) == 0)\n",
    "\n",
    "    # === Apply All Control Points ===\n",
    "    test_df[\"CP_01\"] = test_df.apply(cp_01, axis=1)\n",
    "    test_df[\"CP_02\"] = test_df.apply(cp_02, axis=1)\n",
    "    test_df[\"CP_03\"] = cp_03_flags(test_df)\n",
    "    test_df[\"CP_04\"] = test_df.apply(cp_04, axis=1)\n",
    "    test_df[\"CP_07\"] = cp_07_flags(test_df)\n",
    "    test_df[\"CP_08\"] = test_df.apply(cp_08, axis=1)\n",
    "    test_df[\"CP_09\"] = cp_09_flags(test_df)\n",
    "    test_df[\"CP_15\"] = cp_15_flags(test_df)\n",
    "    \n",
    "    # Ensure Currency column is created before CP_16\n",
    "    if \"Currency\" not in test_df.columns:\n",
    "        test_df[\"Currency\"] = \"INR\"\n",
    "    test_df[\"CP_16\"] = cp_16_flags(test_df)\n",
    "    \n",
    "    test_df[\"CP_17\"] = cp_17_flags(test_df)\n",
    "    test_df[\"CP_19\"] = test_df.apply(cp_19, axis=1)\n",
    "    test_df[\"CP_21\"] = test_df.apply(cp_21, axis=1)\n",
    "    test_df[\"CP_22\"] = test_df.apply(cp_22, axis=1)\n",
    "    test_df[\"CP_23\"] = test_df.apply(cp_23, axis=1)\n",
    "    test_df[\"CP_24\"] = test_df.apply(cp_24, axis=1)\n",
    "    test_df[\"CP_26\"] = cp_26_flags(test_df)\n",
    "    test_df[\"CP_30\"] = test_df.apply(cp_30, axis=1)\n",
    "    test_df[\"CP_32\"] = test_df.apply(cp_32, axis=1)\n",
    "\n",
    "    def compute_cp_score(row):\n",
    "        triggered = [cp for cp in valid_cps if row.get(cp, 0) == 1]\n",
    "        if not triggered: return 0.0\n",
    "        product = 1.0\n",
    "        for cp in triggered:\n",
    "            product *= (1 - cp_score_dict[cp] / 100)\n",
    "        return round(1 - product, 4)\n",
    "\n",
    "    def list_triggered_cps(row):\n",
    "        return \", \".join([f\"{cp} ({cp_score_dict[cp]})\" for cp in valid_cps if row.get(cp, 0) == 1])\n",
    "\n",
    "    test_df[\"Triggered_CPs\"] = test_df.apply(list_triggered_cps, axis=1)\n",
    "    test_df[\"CP_Score\"] = test_df.apply(compute_cp_score, axis=1)\n",
    "\n",
    "    # === Enhanced Risk Classifications (INTERNAL LISTS ONLY) ===\n",
    "    model_class_list = []\n",
    "    cp_class_list = []\n",
    "    final_risk_list = []\n",
    "    \n",
    "    for i in range(len(test_df)):\n",
    "        score = test_df.iloc[i][\"Final_Score\"]\n",
    "        cp_score = test_df.iloc[i][\"CP_Score\"]\n",
    "        \n",
    "        model_class = \"High\" if score >= 0.995 else (\"Medium\" if score >= 0.5 else \"Low\")\n",
    "        cp_class = \"High\" if cp_score >= 0.95 else (\"Medium\" if cp_score > 0.8 else \"Low\")\n",
    "        final_risk = \"High\" if model_class == \"High\" or cp_class == \"High\" else (\"Medium\" if model_class == \"Medium\" or cp_class == \"Medium\" else \"Low\")\n",
    "        \n",
    "        model_class_list.append(model_class)\n",
    "        cp_class_list.append(cp_class)\n",
    "        final_risk_list.append(final_risk)\n",
    "\n",
    "    # === August 25 Enhanced Explanation Generation ===\n",
    "    def get_cp_explanation(cp_code, row):\n",
    "        cp_explanations = {\n",
    "            \"CP_01\": \"Suspicious Keywords - Transaction contains high-risk terms requiring verification\",\n",
    "            \"CP_02\": f\"High Monetary Value - Amount of Rs{row.get('Net Amount', 0):,.0f} exceeds materiality threshold\",\n",
    "            \"CP_03\": \"Duplicate Patterns - Transaction matches multiple duplicate detection criteria\",\n",
    "            \"CP_07\": \"Document Imbalance - Document entries do not balance to zero\",\n",
    "            \"CP_08\": \"Cash Expenditure - Cash-in-hand transaction bypassing standard payment controls\",\n",
    "            \"CP_09\": \"Cash to Bad Debt - Transaction involves both cash and bad debt accounts\",\n",
    "            \"CP_15\": \"Split Transactions - Account activity exceeds normal volume threshold\",\n",
    "            \"CP_16\": \"Multiple Currencies - Document contains multiple currencies\",\n",
    "            \"CP_17\": \"Vendor Concentration - Source transactions exceed concentration limits\",\n",
    "            \"CP_19\": \"Weekend Processing - Transaction processed when standard approvals typically unavailable\",\n",
    "            \"CP_21\": \"Period-End Timing - Transaction occurs on month-end date\",\n",
    "            \"CP_22\": \"Period-Start Timing - Transaction occurs on first day of month\",\n",
    "            \"CP_23\": \"Complex Structure - Transaction involves derivative or structured instruments\",\n",
    "            \"CP_24\": \"Unusual Amount Pattern - Transaction amount follows rare sequential patterns\",\n",
    "            \"CP_26\": \"Document Gap - Document number is missing from sequence\",\n",
    "            \"CP_30\": \"Complex Instrument - Transaction involves sophisticated financial instruments\",\n",
    "            \"CP_32\": \"Zero Amount - Transaction recorded with zero net amount\"\n",
    "        }\n",
    "        return cp_explanations.get(cp_code, f\"Control Point {cp_code} triggered\")\n",
    "    \n",
    "    def parse_triggered_cps(triggered_cps_str):\n",
    "        try:\n",
    "            if not triggered_cps_str or triggered_cps_str.strip() == \"\": return []\n",
    "            cp_codes = []\n",
    "            for cp_part in triggered_cps_str.split(\", \"):\n",
    "                if \"CP_\" in cp_part:\n",
    "                    cp_code = cp_part.split(\" \")[0]\n",
    "                    cp_codes.append(cp_code)\n",
    "            return cp_codes\n",
    "        except: return []\n",
    "\n",
    "    # Generate Enhanced Explanations using August 25 Logic\n",
    "    explanation_summaries = []\n",
    "    \n",
    "    for i in range(len(X_final)):\n",
    "        try:\n",
    "            row_shap = shap_values[i]\n",
    "            row = test_df.iloc[i]\n",
    "            final_risk = final_risk_list[i]\n",
    "            model_class = model_class_list[i]\n",
    "            cp_class = cp_class_list[i]\n",
    "            \n",
    "            # Only generate enhanced explanations for High Risk transactions\n",
    "            if final_risk == \"High\":\n",
    "                bert_in_top3, bert_impact = check_bert_in_top3(row_shap, feature_cols)\n",
    "                explanations = []\n",
    "                \n",
    "                if model_class == \"High\" and cp_class != \"High\":\n",
    "                    # Model-driven risk\n",
    "                    feature_impacts = []\n",
    "                    for j, feature in enumerate(feature_cols):\n",
    "                        if row_shap[j] > 0 and not feature.startswith('text_emb_'):\n",
    "                            feature_value = row.get(feature, \"N/A\")\n",
    "                            feature_impacts.append((feature, feature_value, row_shap[j]))\n",
    "                    \n",
    "                    feature_impacts.sort(key=lambda x: x[2], reverse=True)\n",
    "                    num_regular = 2 if bert_in_top3 and bert_impact >= 0.05 else 3\n",
    "                    \n",
    "                    used_explanations = set()\n",
    "                    consolidated_amounts = []\n",
    "                    \n",
    "                    for feature, value, shap_val in feature_impacts:\n",
    "                        if feature in [\"Net Amount\", \"Entered Dr SUM\", \"Entered Cr SUM\", \"Accounted Dr SUM\", \"Accounted Cr SUM\"]:\n",
    "                            consolidated_amounts.append((feature, value, shap_val))\n",
    "                            continue\n",
    "                        \n",
    "                        explanation_text = get_auditor_friendly_explanation(feature, value, shap_val)\n",
    "                        if explanation_text is not None and explanation_text not in used_explanations:\n",
    "                            formatted_explanation = f'\"{feature}: {value}\" - {explanation_text}'\n",
    "                            explanations.append(formatted_explanation)\n",
    "                            used_explanations.add(explanation_text)\n",
    "                            if len(explanations) >= num_regular:\n",
    "                                break\n",
    "                    \n",
    "                    if consolidated_amounts and len(explanations) < num_regular:\n",
    "                        highest_amount = max(consolidated_amounts, key=lambda x: x[2])\n",
    "                        feature, value, shap_val = highest_amount\n",
    "                        explanation_text = get_auditor_friendly_explanation(feature, value, shap_val)\n",
    "                        if explanation_text is not None:\n",
    "                            formatted_explanation = f'\"Transaction Amount: Rs{float(value):,.0f}\" - {explanation_text}'\n",
    "                            explanations.insert(0, formatted_explanation)\n",
    "                    \n",
    "                    if bert_in_top3 and bert_impact >= 0.05:\n",
    "                        row_dict = row.to_dict()\n",
    "                        bert_exp = bert_explainer.explain_bert_risk(row_dict, bert_impact)\n",
    "                        if bert_exp:\n",
    "                            explanations.append(bert_exp)\n",
    "                    \n",
    "                elif model_class != \"High\" and cp_class == \"High\":\n",
    "                    # CP-driven risk\n",
    "                    triggered_cps = parse_triggered_cps(row.get(\"Triggered_CPs\", \"\"))\n",
    "                    num_cp = 2 if bert_in_top3 and bert_impact >= 0.05 else 3\n",
    "                    \n",
    "                    for cp_code in triggered_cps[:num_cp]:\n",
    "                        cp_explanation = get_cp_explanation(cp_code, row)\n",
    "                        explanations.append(cp_explanation)\n",
    "                    \n",
    "                    if bert_in_top3 and bert_impact >= 0.05:\n",
    "                        row_dict = row.to_dict()\n",
    "                        bert_exp = bert_explainer.explain_bert_risk(row_dict, bert_impact)\n",
    "                        if bert_exp:\n",
    "                            explanations.append(bert_exp)\n",
    "                    \n",
    "                elif model_class == \"High\" and cp_class == \"High\":\n",
    "                    # Both model and CP high risk\n",
    "                    num_model = 1 if bert_in_top3 and bert_impact >= 0.05 else 2\n",
    "                    num_cp = 1\n",
    "                    \n",
    "                    # Model features\n",
    "                    feature_impacts = []\n",
    "                    for j, feature in enumerate(feature_cols):\n",
    "                        if row_shap[j] > 0 and not feature.startswith('text_emb_'):\n",
    "                            feature_value = row.get(feature, \"N/A\")\n",
    "                            feature_impacts.append((feature, feature_value, row_shap[j]))\n",
    "                    \n",
    "                    feature_impacts.sort(key=lambda x: x[2], reverse=True)\n",
    "                    \n",
    "                    for k, (feature, value, shap_val) in enumerate(feature_impacts[:num_model]):\n",
    "                        explanation_text = get_auditor_friendly_explanation(feature, value, shap_val)\n",
    "                        if explanation_text is not None:\n",
    "                            if feature in [\"Net Amount\", \"Entered Dr SUM\", \"Entered Cr SUM\", \"Accounted Dr SUM\", \"Accounted Cr SUM\"]:\n",
    "                                formatted_explanation = f'\"Transaction Amount: Rs{float(value):,.0f}\" - {explanation_text}'\n",
    "                            else:\n",
    "                                formatted_explanation = f'\"{feature}: {value}\" - {explanation_text}'\n",
    "                            explanations.append(formatted_explanation)\n",
    "                    \n",
    "                    # CP features\n",
    "                    triggered_cps = parse_triggered_cps(row.get(\"Triggered_CPs\", \"\"))\n",
    "                    for cp_code in triggered_cps[:num_cp]:\n",
    "                        cp_explanation = get_cp_explanation(cp_code, row)\n",
    "                        explanations.append(cp_explanation)\n",
    "                    \n",
    "                    if bert_in_top3 and bert_impact >= 0.05:\n",
    "                        row_dict = row.to_dict()\n",
    "                        bert_exp = bert_explainer.explain_bert_risk(row_dict, bert_impact)\n",
    "                        if bert_exp:\n",
    "                            explanations.append(bert_exp)\n",
    "                \n",
    "                # Create enhanced explanation for High Risk\n",
    "                if explanations:\n",
    "                    final_explanation = \"\\n\".join(explanations[:3])\n",
    "                    explanation_summaries.append(final_explanation)\n",
    "                else:\n",
    "                    explanation_summaries.append(\"High risk transaction requiring enhanced review\")\n",
    "            else:\n",
    "                # For non-High risk transactions, use empty explanation\n",
    "                explanation_summaries.append(\"\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            explanation_summaries.append(\"\")\n",
    "\n",
    "    # === Generate Original Feature Group Explanations ===\n",
    "    amount_features = [\"Entered Dr SUM\", \"Entered Cr SUM\", \"Accounted Dr SUM\", \"Accounted Cr SUM\", \"Net Amount\"]\n",
    "    date_features = [\"Accounting_Month\", \"Accounting_Weekday\", \"Invoice_Month\", \"Invoice_Weekday\", \"Posted_Month\", \"Posted_Weekday\"]\n",
    "    account_info_features = [\"Account Name\", \"Nature in balance sheet\", \"Source Name\", \"Document Type\", \"Tax Rate\", \"Tax Rate Name\"]\n",
    "    other_features = [col for col in model.feature_names_ if col not in amount_features + date_features + account_info_features and not col.startswith(\"text_emb_\")]\n",
    "\n",
    "    feature_groups = {\n",
    "        \"Amount\": amount_features,\n",
    "        \"Date\": date_features,\n",
    "        \"Source Info\": account_info_features,\n",
    "        \"Batch\": other_features,\n",
    "        \"Narration\": [\"Narration_Cluster_Label\"]\n",
    "    }\n",
    "\n",
    "    explanation_templates = {\n",
    "        \"Narration\": \"Narration pattern resembles high-value or structured payouts\",\n",
    "        \"Amount\": \"High {feature} = ₹{value:,.0f}\",\n",
    "        \"Date\": \"Posted on {feature} = {value}\",\n",
    "        \"Source Info\": \"{feature} = '{value}' is missing or looks suspicious\",\n",
    "        \"Batch\": \"Batch reference '{value}' appears frequently in vendor payments\"\n",
    "    }\n",
    "\n",
    "    top_risky_texts, top_safe_texts = [], []\n",
    "    for i in range(len(X_final)):\n",
    "        row_shap = shap_values[i]\n",
    "        row = test_df.iloc[i]\n",
    "\n",
    "        impact_by_group = {}\n",
    "        feature_info = {}\n",
    "        for group, features in feature_groups.items():\n",
    "            valid_feats = [f for f in features if f in feature_cols]\n",
    "            if not valid_feats:\n",
    "                continue\n",
    "            group_shap_sum = sum(row_shap[feature_cols.index(f)] for f in valid_feats)\n",
    "            impact_by_group[group] = group_shap_sum\n",
    "            top_feat = max(valid_feats, key=lambda f: abs(row_shap[feature_cols.index(f)]))\n",
    "            value = row.get(top_feat, \"N/A\")\n",
    "            feature_info[group] = (top_feat, value)\n",
    "\n",
    "        sorted_risk = sorted(impact_by_group.items(), key=lambda x: x[1], reverse=True)\n",
    "        sorted_safe = sorted(impact_by_group.items(), key=lambda x: x[1])\n",
    "\n",
    "        def render(group, feature, value):\n",
    "            if group == \"Narration\":\n",
    "                return explanation_templates[group]\n",
    "            elif group in explanation_templates:\n",
    "                return explanation_templates[group].format(feature=feature, value=value)\n",
    "            else:\n",
    "                return f\"{group}: {feature} = {value}\"\n",
    "\n",
    "        top_risk = [render(g, *feature_info[g]) for g, _ in sorted_risk[:3]]\n",
    "        top_safe = [render(g, *feature_info[g]) for g, _ in sorted_safe if g not in [r[0] for r in sorted_risk[:3]][:2]]\n",
    "\n",
    "        top_risky_texts.append(\"\\n\".join(f\"- {t}\" for t in top_risk))\n",
    "        top_safe_texts.append(\"\\n\".join(f\"- {t}\" for t in top_safe[:2]))\n",
    "\n",
    "    # Remove BERT embedding columns before creating final columns\n",
    "    test_df = test_df.drop(columns=[col for col in test_df.columns if col.startswith(\"text_emb_\")])\n",
    "\n",
    "    # === Create Explanation Columns (Positions 42-44) ===\n",
    "    test_df[\"Top_Risky_Feature_Groups\"] = top_risky_texts\n",
    "    test_df[\"Top_Safe_Feature_Groups\"] = top_safe_texts\n",
    "    test_df[\"Explanation_Summary\"] = explanation_summaries\n",
    "\n",
    "    # === Final Column Order (65 columns) ===\n",
    "    expected_columns = [\n",
    "        # Original 30 columns\n",
    "        \"S. No\", \"Entity Name\", \"Accounting Date\", \"Approval Type\", \"Document Type\", \"Invoice Date\", \"Day\", \"Nature\",\n",
    "        \"Account Code\", \"PL/ BS\", \"Report Group\", \"Account Name\", \"Nature in balance sheet\", \"Document Number\", \"Je Line Num\",\n",
    "        \"Source Number\", \"Source Name\", \"Source Voucher Name\", \"Source Desc\", \"Line Desc\", \"Project Code\", \"Internal Reference\", \n",
    "        \"Posted Date\", \"Branch\", \"Batch Name\", \"Entered Dr SUM\", \"Entered Cr SUM\", \"Accounted Dr SUM\", \"Accounted Cr SUM\", \"Net Amount\",\n",
    "        \n",
    "        # Generated analysis columns\n",
    "        \"Combined_Text\", \"Narration_Cluster\", \"Narration_Cluster_Label\",\n",
    "        \"Accounting_Month\", \"Accounting_Weekday\", \"Invoice_Month\", \"Invoice_Weekday\", \"Posted_Month\", \"Posted_Weekday\",\n",
    "        \"Model_Score\", \"Final_Score\",\n",
    "        \n",
    "        # Explanation columns (positions 42-44)\n",
    "        \"Top_Risky_Feature_Groups\", \"Top_Safe_Feature_Groups\", \"Explanation_Summary\",\n",
    "        \n",
    "        # Control points columns (18 total)\n",
    "        \"CP_01\", \"CP_02\", \"CP_03\", \"CP_04\", \"CP_07\", \"CP_08\", \"CP_09\", \"CP_15\", \"Currency\", \"CP_16\", \"CP_17\", \n",
    "        \"CP_19\", \"CP_21\", \"CP_22\", \"CP_23\", \"CP_24\", \"CP_26\", \"CP_30\", \"CP_32\", \"Triggered_CPs\", \"CP_Score\"\n",
    "    ]\n",
    "    \n",
    "    # Validate all expected columns exist\n",
    "    missing_columns = [col for col in expected_columns if col not in test_df.columns]\n",
    "    if missing_columns:\n",
    "        raise ValueError(f\"Missing required columns: {missing_columns}\")\n",
    "    \n",
    "    # Reorder columns to exact specification\n",
    "    final_df = test_df[expected_columns].copy()\n",
    "    \n",
    "    # Final validation: ensure exactly 65 columns\n",
    "    if len(final_df.columns) != 65:\n",
    "        raise ValueError(f\"Expected 65 columns, got {len(final_df.columns)}\")\n",
    "    \n",
    "    logging.debug(\"run_full_pipeline complete\")\n",
    "    return final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bf62c97c-d746-4408-b2a4-841b6fc3a39a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-20 12:57:08,584 - INFO - Use pytorch device_name: cpu\n",
      "2025-09-20 12:57:08,586 - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading CatBoost model...\n",
      "CatBoost model loaded\n",
      "Downloading SentenceTransformer...\n",
      "SentenceTransformer loaded\n",
      "Total columns: 65\n",
      "Column names: ['S. No', 'Entity Name', 'Accounting Date', 'Approval Type', 'Document Type', 'Invoice Date', 'Day', 'Nature', 'Account Code', 'PL/ BS', 'Report Group', 'Account Name', 'Nature in balance sheet', 'Document Number', 'Je Line Num', 'Source Number', 'Source Name', 'Source Voucher Name', 'Source Desc', 'Line Desc', 'Project Code', 'Internal Reference', 'Posted Date', 'Branch', 'Batch Name', 'Entered Dr SUM', 'Entered Cr SUM', 'Accounted Dr SUM', 'Accounted Cr SUM', 'Net Amount', 'Combined_Text', 'Narration_Cluster', 'Narration_Cluster_Label', 'Accounting_Month', 'Accounting_Weekday', 'Invoice_Month', 'Invoice_Weekday', 'Posted_Month', 'Posted_Weekday', 'Model_Score', 'Final_Score', 'Top_Risky_Feature_Groups', 'Top_Safe_Feature_Groups', 'Explanation_Summary', 'CP_01', 'CP_02', 'CP_03', 'CP_04', 'CP_07', 'CP_08', 'CP_09', 'CP_15', 'Currency', 'CP_16', 'CP_17', 'CP_19', 'CP_21', 'CP_22', 'CP_23', 'CP_24', 'CP_26', 'CP_30', 'CP_32', 'Triggered_CPs', 'CP_Score']\n",
      "\n",
      "High risk transactions: 165\n",
      "\n",
      "Sample explanation:\n",
      "Document Imbalance - Document entries do not balance to zero\n",
      "Period-End Timing - Transaction occurs on month-end date\n",
      "Unusual Amount Pattern - Transaction amount follows rare sequential patterns\n"
     ]
    }
   ],
   "source": [
    "# Test the corrected model_logic directly\n",
    "result_df = run_full_pipeline(\"old_test_file.csv\")\n",
    "\n",
    "# Check the results\n",
    "print(f\"Total columns: {len(result_df.columns)}\")\n",
    "print(f\"Column names: {list(result_df.columns)}\")\n",
    "\n",
    "# Check high risk explanations\n",
    "high_risk = result_df[result_df['Final_Score'] >= 0.8]\n",
    "print(f\"\\nHigh risk transactions: {len(high_risk)}\")\n",
    "\n",
    "# Show sample explanation\n",
    "if len(high_risk) > 0:\n",
    "    sample_explanation = high_risk.iloc[0]['Explanation_Summary']\n",
    "    print(f\"\\nSample explanation:\\n{sample_explanation}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a902e052-0b20-4b19-bda4-18c2832e06fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-20 12:59:46,961 - INFO - Use pytorch device_name: cpu\n",
      "2025-09-20 12:59:46,963 - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "TESTING SAVED MODEL_LOGIC.PY\n",
      "======================================================================\n",
      "Running pipeline on test file...\n",
      "Loading CatBoost model...\n",
      "CatBoost model loaded\n",
      "Downloading SentenceTransformer...\n",
      "SentenceTransformer loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-20 13:00:11,075 - INFO - Initializing BERT Risk Explainer...\n",
      "2025-09-20 13:00:11,084 - INFO - Initialized with 9 Priority 1 patterns, 6 Priority 2 dimensions, and 8 Priority 3 clusters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing transactions with enhanced explanations...\n",
      "Pipeline completed successfully!\n",
      "Total transactions processed: 874\n",
      "Total columns in output: 68\n",
      "❌ Column count mismatch: Expected 65, Got 68\n",
      "\n",
      "Risk Distribution:\n",
      "   High Risk: 165\n",
      "   Medium Risk: 2\n",
      "   Low Risk: 707\n",
      "\n",
      "Explanation Coverage:\n",
      "   Transactions with explanations: 394\n",
      "   Coverage rate: 45.1%\n",
      "\n",
      "Saving output to: Enhanced_Risk_Output_20250920_130014.xlsx\n",
      "✅ Output file saved successfully!\n",
      "\n",
      "First 3 rows preview:\n",
      "   S. No  Final_Score  CP_Score                                                                                                                                                                                                                                                                                                            Explanation_Summary\n",
      "0      1        0.042    0.9648  Duplicate Patterns - Transaction matches multiple duplicate detection criteria indicating potential data quality issues (CP_03)\\nDocument Imbalance - Document entries do not balance to zero, indicating posting errors requiring correction (CP_07)\\nMultiple risk indicators: documentation and processing issues detected\n",
      "1      2        0.045    0.9648  Duplicate Patterns - Transaction matches multiple duplicate detection criteria indicating potential data quality issues (CP_03)\\nDocument Imbalance - Document entries do not balance to zero, indicating posting errors requiring correction (CP_07)\\nMultiple risk indicators: documentation and processing issues detected\n",
      "2      3        0.024    0.9891                   Document Imbalance - Document entries do not balance to zero, indicating posting errors requiring correction (CP_07)\\nPeriod-End Timing - Transaction occurs on month-end date when period-end adjustments are commonly made (CP_21)\\nMultiple risk indicators: documentation and processing issues detected\n",
      "\n",
      "Sample High Risk Explanation:\n",
      "Transaction 4: Score=0.924\n",
      "Explanation: Document Imbalance - Document entries do not balance to zero, indicating posting errors requiring correction (CP_07)\n",
      "Period-End Timing - Transaction occurs on month-end date when period-end adjustments are commonly made (CP_21)\n",
      "Unusual Amount Pattern - Transaction amount ending in 000 follows rare sequential or repetitive digit patterns (CP_24)\n",
      "\n",
      "======================================================================\n",
      "TEST COMPLETED SUCCESSFULLY\n",
      "Output saved as: Enhanced_Risk_Output_20250920_130014.xlsx\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# === TEST CODE FOR SAVED MODEL_LOGIC ===\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "# Add the app directory to Python path\n",
    "sys.path.append('./app')\n",
    "\n",
    "# Import your saved model_logic\n",
    "from model_logic import run_full_pipeline\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"TESTING SAVED MODEL_LOGIC.PY\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "try:\n",
    "    # Run the pipeline with your test file\n",
    "    print(\"Running pipeline on test file...\")\n",
    "    result_df = run_full_pipeline(\"old_test_file.csv\")\n",
    "    \n",
    "    print(f\"Pipeline completed successfully!\")\n",
    "    print(f\"Total transactions processed: {len(result_df):,}\")\n",
    "    print(f\"Total columns in output: {len(result_df.columns)}\")\n",
    "    \n",
    "    # Verify column structure\n",
    "    expected_count = 65\n",
    "    if len(result_df.columns) == expected_count:\n",
    "        print(f\"✅ Column count correct: {len(result_df.columns)}\")\n",
    "    else:\n",
    "        print(f\"❌ Column count mismatch: Expected {expected_count}, Got {len(result_df.columns)}\")\n",
    "    \n",
    "    # Check risk distribution\n",
    "    if 'Final_Score' in result_df.columns:\n",
    "        high_risk = len(result_df[result_df['Final_Score'] >= 0.8])\n",
    "        medium_risk = len(result_df[(result_df['Final_Score'] >= 0.5) & (result_df['Final_Score'] < 0.8)])\n",
    "        low_risk = len(result_df[result_df['Final_Score'] < 0.5])\n",
    "        \n",
    "        print(f\"\\nRisk Distribution:\")\n",
    "        print(f\"   High Risk: {high_risk:,}\")\n",
    "        print(f\"   Medium Risk: {medium_risk:,}\")\n",
    "        print(f\"   Low Risk: {low_risk:,}\")\n",
    "    \n",
    "    # Check explanation coverage\n",
    "    if 'Explanation_Summary' in result_df.columns:\n",
    "        explained_transactions = sum(1 for exp in result_df['Explanation_Summary'] if str(exp).strip() != '')\n",
    "        print(f\"\\nExplanation Coverage:\")\n",
    "        print(f\"   Transactions with explanations: {explained_transactions:,}\")\n",
    "        print(f\"   Coverage rate: {(explained_transactions/len(result_df)*100):.1f}%\")\n",
    "    \n",
    "    # Generate timestamped output filename\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    output_filename = f\"Enhanced_Risk_Output_{timestamp}.xlsx\"\n",
    "    \n",
    "    # Save the output\n",
    "    print(f\"\\nSaving output to: {output_filename}\")\n",
    "    result_df.to_excel(output_filename, index=False)\n",
    "    print(f\"✅ Output file saved successfully!\")\n",
    "    \n",
    "    # Display first few rows for verification\n",
    "    print(f\"\\nFirst 3 rows preview:\")\n",
    "    preview_cols = ['S. No', 'Final_Score', 'CP_Score', 'Explanation_Summary']\n",
    "    available_cols = [col for col in preview_cols if col in result_df.columns]\n",
    "    print(result_df[available_cols].head(3).to_string())\n",
    "    \n",
    "    # Show sample high-risk explanation\n",
    "    high_risk_with_explanations = result_df[\n",
    "        (result_df['Final_Score'] >= 0.8) & \n",
    "        (result_df['Explanation_Summary'].str.strip() != '')\n",
    "    ]\n",
    "    \n",
    "    if len(high_risk_with_explanations) > 0:\n",
    "        print(f\"\\nSample High Risk Explanation:\")\n",
    "        sample = high_risk_with_explanations.iloc[0]\n",
    "        print(f\"Transaction {sample['S. No']}: Score={sample['Final_Score']:.3f}\")\n",
    "        print(f\"Explanation: {sample['Explanation_Summary']}\")\n",
    "    \n",
    "    print(f\"\\n\" + \"=\" * 70)\n",
    "    print(\"TEST COMPLETED SUCCESSFULLY\")\n",
    "    print(f\"Output saved as: {output_filename}\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ TEST FAILED: {str(e)}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4576f1e3-292f-4b14-8aba-0c02721e72c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-20 13:02:28,876 - INFO - Use pytorch device_name: cpu\n",
      "2025-09-20 13:02:28,877 - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading CatBoost model...\n",
      "CatBoost model loaded\n",
      "Downloading SentenceTransformer...\n",
      "SentenceTransformer loaded\n",
      "ACTUAL COLUMNS (68):\n",
      " 1. S. No\n",
      " 2. Entity Name\n",
      " 3. Accounting Date\n",
      " 4. Approval Type\n",
      " 5. Document Type\n",
      " 6. Invoice Date\n",
      " 7. Day\n",
      " 8. Nature\n",
      " 9. Account Code\n",
      "10. PL/ BS\n",
      "11. Report Group\n",
      "12. Account Name\n",
      "13. Nature in balance sheet\n",
      "14. Document Number\n",
      "15. Je Line Num\n",
      "16. Source Number\n",
      "17. Source Name\n",
      "18. Source Voucher Name\n",
      "19. Source Desc\n",
      "20. Line Desc\n",
      "21. Project Code\n",
      "22. Internal Reference\n",
      "23. Posted Date\n",
      "24. Branch\n",
      "25. Batch Name\n",
      "26. Entered Dr SUM\n",
      "27. Entered Cr SUM\n",
      "28. Accounted Dr SUM\n",
      "29. Accounted Cr SUM\n",
      "30. Net Amount\n",
      "31. Combined_Text\n",
      "32. Narration_Cluster\n",
      "33. Narration_Cluster_Label\n",
      "34. Accounting_Month\n",
      "35. Accounting_Weekday\n",
      "36. Invoice_Month\n",
      "37. Invoice_Weekday\n",
      "38. Posted_Month\n",
      "39. Posted_Weekday\n",
      "40. Model_Score\n",
      "41. Final_Score\n",
      "42. Top_Risky_Feature_Groups\n",
      "43. Top_Safe_Feature_Groups\n",
      "44. Explanation_Summary\n",
      "45. CP_01\n",
      "46. CP_02\n",
      "47. CP_03\n",
      "48. CP_04\n",
      "49. CP_07\n",
      "50. CP_08\n",
      "51. CP_09\n",
      "52. CP_15\n",
      "53. Currency\n",
      "54. CP_16\n",
      "55. CP_17\n",
      "56. CP_19\n",
      "57. CP_21\n",
      "58. CP_22\n",
      "59. CP_23\n",
      "60. CP_24\n",
      "61. CP_26\n",
      "62. CP_30\n",
      "63. CP_32\n",
      "64. Triggered_CPs\n",
      "65. CP_Score\n"
     ]
    }
   ],
   "source": [
    "# Check what the extra 3 columns are\n",
    "result_df = run_full_pipeline(\"old_test_file.csv\")\n",
    "print(\"ACTUAL COLUMNS (68):\")\n",
    "for i, col in enumerate(result_df.columns, 1):\n",
    "    print(f\"{i:2d}. {col}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "db281706-4b65-4ade-aaa8-ef856c26e424",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === CORRECTED model_logic.py - 65 Columns with August 25 Integration ===\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import warnings\n",
    "import logging\n",
    "from typing import Dict, List, Tuple, Optional, Union\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# === BERT RISK EXPLAINER CLASS (Internal Use Only) ===\n",
    "class BERTRiskExplainer:\n",
    "    def __init__(self):\n",
    "        self.business_risk_patterns = {\n",
    "            'sunday_payment_processing': {\n",
    "                'trigger': self._check_sunday_payment_processing,\n",
    "                'explanation': \"Sunday payment processing bypassing standard authorization controls\",\n",
    "            },\n",
    "            'vague_account_classification': {\n",
    "                'trigger': self._check_vague_account_classification,\n",
    "                'explanation': \"Vague account classifications lacking transaction specificity\",\n",
    "            },\n",
    "            'high_value_escrow_processing': {\n",
    "                'trigger': self._check_high_value_escrow_processing,\n",
    "                'explanation': \"High-value escrow processing requiring enhanced fiduciary oversight\",\n",
    "            },\n",
    "            'system_integration_processing': {\n",
    "                'trigger': self._check_system_integration_processing,\n",
    "                'explanation': \"System integration processing with data integrity vulnerabilities\",\n",
    "            },\n",
    "            'manual_ecommerce_operations': {\n",
    "                'trigger': self._check_manual_ecommerce_operations,\n",
    "                'explanation': \"Manual e-commerce operations bypassing automated controls\",\n",
    "            },\n",
    "            'cod_settlement_verification': {\n",
    "                'trigger': self._check_cod_settlement_verification,\n",
    "                'explanation': \"COD settlement with logistics coordination timing differences\",\n",
    "            },\n",
    "            'payment_gateway_reconciliation': {\n",
    "                'trigger': self._check_payment_gateway_reconciliation,\n",
    "                'explanation': \"Payment gateway requiring multi-party reconciliation processes\",\n",
    "            },\n",
    "            'revenue_recognition_timing': {\n",
    "                'trigger': self._check_revenue_recognition_timing,\n",
    "                'explanation': \"Revenue recognition timing requiring compliance assessment\",\n",
    "            },\n",
    "            'adjustment_entry_documentation': {\n",
    "                'trigger': self._check_adjustment_entry_documentation,\n",
    "                'explanation': \"Manual adjustments deviating from standard processing workflows\",\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    def _check_sunday_payment_processing(self, row_data: Dict, text: str) -> bool:\n",
    "        try:\n",
    "            day = str(row_data.get('Day', '')).strip()\n",
    "            sunday_days = ['Sun', 'Sunday']\n",
    "            payment_terms = ['wallet', 'hadoop', 'payment', 'cashfree']\n",
    "            return (day in sunday_days and any(term in text.lower() for term in payment_terms))\n",
    "        except: return False\n",
    "    \n",
    "    def _check_vague_account_classification(self, row_data: Dict, text: str) -> bool:\n",
    "        try:\n",
    "            account_name = str(row_data.get('Account Name', '')).lower()\n",
    "            adjustment_terms = ['adjustment', 'settlement', 'liability']\n",
    "            return ('other' in account_name and any(term in text.lower() for term in adjustment_terms))\n",
    "        except: return False\n",
    "    \n",
    "    def _check_high_value_escrow_processing(self, row_data: Dict, text: str) -> bool:\n",
    "        try:\n",
    "            escrow_terms = ['escrow', 'wallet', 'liability']\n",
    "            account_name = str(row_data.get('Account Name', '')).lower()\n",
    "            amount = self._safe_float_conversion(row_data.get('Net Amount', 0))\n",
    "            return (any(term in text.lower() for term in escrow_terms) and\n",
    "                    'escrow' in account_name and abs(amount) > 500000)\n",
    "        except: return False\n",
    "    \n",
    "    def _check_system_integration_processing(self, row_data: Dict, text: str) -> bool:\n",
    "        try:\n",
    "            system_terms = ['hadoop', 'system', 'automated', 'verified', 'matched']\n",
    "            processing_terms = ['processing', 'settlement', 'reconciliation']\n",
    "            return (any(term in text.lower() for term in system_terms) and\n",
    "                    any(term in text.lower() for term in processing_terms))\n",
    "        except: return False\n",
    "    \n",
    "    def _check_manual_ecommerce_operations(self, row_data: Dict, text: str) -> bool:\n",
    "        try:\n",
    "            manual_terms = ['spreadsheet', 'manual']\n",
    "            ecommerce_terms = ['gmv', 'seller', 'rebate', 'voucher']\n",
    "            amount = self._safe_float_conversion(row_data.get('Net Amount', 0))\n",
    "            return (any(term in text.lower() for term in manual_terms) and\n",
    "                    any(term in text.lower() for term in ecommerce_terms) and\n",
    "                    abs(amount) > 1000000)\n",
    "        except: return False\n",
    "    \n",
    "    def _check_cod_settlement_verification(self, row_data: Dict, text: str) -> bool:\n",
    "        try:\n",
    "            cod_terms = ['cod', 'delhivery', 'delivery']\n",
    "            settlement_terms = ['settlement', 'collection', 'payment']\n",
    "            return (any(term in text.lower() for term in cod_terms) and\n",
    "                    any(term in text.lower() for term in settlement_terms))\n",
    "        except: return False\n",
    "    \n",
    "    def _check_payment_gateway_reconciliation(self, row_data: Dict, text: str) -> bool:\n",
    "        try:\n",
    "            gateway_terms = ['cashfree', 'gateway', 'payment']\n",
    "            process_terms = ['settlement', 'reconciliation', 'processing']\n",
    "            return (any(term in text.lower() for term in gateway_terms) and\n",
    "                    any(term in text.lower() for term in process_terms))\n",
    "        except: return False\n",
    "    \n",
    "    def _check_revenue_recognition_timing(self, row_data: Dict, text: str) -> bool:\n",
    "        try:\n",
    "            pl_bs = str(row_data.get('PL/ BS', '')).upper().strip()\n",
    "            revenue_terms = ['revenue', 'income', 'sales']\n",
    "            amount = self._safe_float_conversion(row_data.get('Net Amount', 0))\n",
    "            return (pl_bs == 'PL' and\n",
    "                    any(term in text.lower() for term in revenue_terms) and\n",
    "                    abs(amount) > 500000)\n",
    "        except: return False\n",
    "    \n",
    "    def _check_adjustment_entry_documentation(self, row_data: Dict, text: str) -> bool:\n",
    "        try:\n",
    "            adjustment_terms = ['adjustment', 'correction', 'manual']\n",
    "            process_terms = ['settlement', 'reconciliation', 'variance']\n",
    "            return (any(term in text.lower() for term in adjustment_terms) and\n",
    "                    any(term in text.lower() for term in process_terms))\n",
    "        except: return False\n",
    "    \n",
    "    def _safe_float_conversion(self, value: Union[str, int, float], default: float = 0.0) -> float:\n",
    "        try:\n",
    "            if pd.isna(value) or value is None: return default\n",
    "            return float(value)\n",
    "        except: return default\n",
    "    \n",
    "    def explain_bert_risk(self, transaction_data: Dict, bert_impact: float) -> Optional[str]:\n",
    "        try:\n",
    "            if pd.isna(bert_impact) or bert_impact < 0.05: return None\n",
    "            \n",
    "            text = str(transaction_data.get('Combined_Text', '')).lower().strip()\n",
    "            if not text or text == 'nan': return None\n",
    "            \n",
    "            # Check business risk patterns first\n",
    "            for pattern_name, pattern_config in self.business_risk_patterns.items():\n",
    "                try:\n",
    "                    if pattern_config['trigger'](transaction_data, text):\n",
    "                        return pattern_config['explanation']\n",
    "                except: continue\n",
    "            \n",
    "            # Fallback explanations based on text patterns\n",
    "            if any(term in text for term in ['other', 'miscellaneous', 'various', 'general']):\n",
    "                return \"Vague transaction descriptions lacking specific business purpose\"\n",
    "            elif any(term in text for term in ['spreadsheet', 'manual', 'excel']):\n",
    "                return \"Manual processing bypassing automated control frameworks\"\n",
    "            elif any(term in text for term in ['urgent', 'emergency', 'immediate']):\n",
    "                return \"Urgency indicators suggesting potential workflow bypass\"\n",
    "            else:\n",
    "                return \"Text-based risk patterns requiring enhanced verification\"\n",
    "                \n",
    "        except Exception as e:\n",
    "            return None\n",
    "\n",
    "def calculate_bert_impact(shap_values: np.ndarray, feature_names: List[str]) -> float:\n",
    "    try:\n",
    "        bert_features = [i for i, name in enumerate(feature_names) if name.startswith('text_emb_')]\n",
    "        bert_impact = sum(shap_values[i] for i in bert_features if shap_values[i] > 0)\n",
    "        return bert_impact\n",
    "    except: return 0.0\n",
    "\n",
    "def check_bert_in_top3(shap_values: np.ndarray, feature_names: List[str]) -> Tuple[bool, float]:\n",
    "    try:\n",
    "        feature_impacts = [(feature, shap_val) for feature, shap_val in zip(feature_names, shap_values) if shap_val > 0]\n",
    "        feature_impacts.sort(key=lambda x: x[1], reverse=True)\n",
    "        top_3_features = feature_impacts[:3]\n",
    "        bert_in_top3 = any(feature.startswith('text_emb_') for feature, shap_val in top_3_features)\n",
    "        bert_impact = calculate_bert_impact(shap_values, feature_names)\n",
    "        return bert_in_top3, bert_impact\n",
    "    except: return False, 0.0\n",
    "\n",
    "def get_auditor_friendly_explanation(feature, value, shap_impact):\n",
    "    try:\n",
    "        if feature == \"Day\":\n",
    "            try:\n",
    "                day_num = int(value)\n",
    "                if day_num == 6:  # Sunday\n",
    "                    return \"Transaction processed on Sunday when standard business operations are typically not active\"\n",
    "                else: return None\n",
    "            except: return None\n",
    "        \n",
    "        elif feature == \"Account Name\":\n",
    "            value_str = str(value).lower()\n",
    "            if \"other debtors\" in value_str:\n",
    "                return \"Vague account classification lacking proper transaction specificity\"\n",
    "            elif \"other professional fees\" in value_str:\n",
    "                return \"General professional fee account lacking vendor specificity\"\n",
    "            elif \"legal fee\" in value_str:\n",
    "                return \"Legal fee account susceptible to inappropriate payments\"\n",
    "            elif \"receivables from cod\" in value_str:\n",
    "                return \"Cash-on-delivery receivables requiring enhanced verification\"\n",
    "            elif \"cash in bank\" in value_str:\n",
    "                return \"Cash account requiring verification of bank reconciliations\"\n",
    "            elif \"receivables from payment gateway\" in value_str:\n",
    "                return \"Payment gateway receivables requiring verification of settlement timing\"\n",
    "            else: return None\n",
    "        \n",
    "        elif feature == \"Nature in balance sheet\":\n",
    "            value_str = str(value).lower()\n",
    "            if \"logistic\" in value_str and \"packing\" in value_str:\n",
    "                return \"Logistics expense category prone to cost inflation\"\n",
    "            elif \"legal\" in value_str and \"professional\" in value_str:\n",
    "                return \"Professional services expense susceptible to manipulation\"\n",
    "            elif \"provision\" in value_str:\n",
    "                return \"Provision account lacking detailed substantiation\"\n",
    "            else: return None\n",
    "        \n",
    "        elif feature in [\"Net Amount\", \"Entered Dr SUM\", \"Entered Cr SUM\", \"Accounted Dr SUM\", \"Accounted Cr SUM\"]:\n",
    "            try:\n",
    "                amount = float(value)\n",
    "                if amount >= 10000000000:  # 1000 Cr\n",
    "                    return \"Exceptionally high transaction value representing significant financial exposure\"\n",
    "                elif amount >= 5000000000:  # 500 Cr\n",
    "                    return \"Very high transaction value exceeding typical business thresholds\"\n",
    "                elif amount >= 1000000000:  # 100 Cr\n",
    "                    return \"High transaction value exceeding standard materiality thresholds\"\n",
    "                elif amount >= 500000000:  # 50 Cr\n",
    "                    return \"Material transaction amount warranting enhanced scrutiny\"\n",
    "                else: return None\n",
    "            except: return \"Transaction amount requiring verification due to data quality issues\"\n",
    "        \n",
    "        elif feature == \"Batch Name\":\n",
    "            value_str = str(value).lower()\n",
    "            if \"spreadsheet\" in value_str:\n",
    "                return \"Bulk spreadsheet processing bypassing individual transaction controls\"\n",
    "            else: return None\n",
    "        \n",
    "        elif feature == \"Document Type\":\n",
    "            if str(value) == \"Manual\":\n",
    "                return \"Manual entry increasing error risk and bypassing automated validation\"\n",
    "            elif str(value) == \"Spreadsheet\":\n",
    "                return \"Spreadsheet-based entry bypassing automated controls\"\n",
    "            else: return None\n",
    "        \n",
    "        elif \"Weekday\" in feature:\n",
    "            try:\n",
    "                day_num = int(value)\n",
    "                if day_num == 6:  # Sunday only\n",
    "                    return \"Transaction processed on Sunday when standard business operations are typically not active\"\n",
    "                else: return None\n",
    "            except: return None\n",
    "        \n",
    "        else: return None\n",
    "    \n",
    "    except Exception as e:\n",
    "        return None\n",
    "\n",
    "def run_full_pipeline(file_path: str) -> pd.DataFrame:\n",
    "    logging.debug(\"Starting run_full_pipeline\")\n",
    "    # Lazy load heavy packages here\n",
    "    import shap\n",
    "    from catboost import CatBoostClassifier\n",
    "    from sentence_transformers import SentenceTransformer\n",
    "    from umap import UMAP\n",
    "    from sklearn.cluster import KMeans\n",
    "    import requests\n",
    "    from io import BytesIO\n",
    "\n",
    "    # === Load Models ===\n",
    "    try:\n",
    "        print(\"Loading CatBoost model...\")\n",
    "        model = CatBoostClassifier()\n",
    "        model.load_model(\"models/catboost_v2_model.cbm\")\n",
    "        print(\"CatBoost model loaded\")\n",
    "    except Exception as e:\n",
    "        print(\"Failed to load CatBoost model:\", str(e))\n",
    "        raise\n",
    "\n",
    "    try:\n",
    "        print(\"Downloading SentenceTransformer...\")\n",
    "        model_bert = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "        print(\"SentenceTransformer loaded\")\n",
    "    except Exception as e:\n",
    "        print(\"Failed to load BERT model:\", str(e))\n",
    "        raise\n",
    "\n",
    "    # === Load and Clean Data ===\n",
    "    test_df = pd.read_csv(file_path, encoding='ISO-8859-1')\n",
    "    test_df.columns = test_df.columns.str.strip()\n",
    "\n",
    "    # Clean numeric columns\n",
    "    comma_cols = [\"Entered Dr SUM\", \"Entered Cr SUM\", \"Accounted Dr SUM\", \"Accounted Cr SUM\", \"Net Amount\"]\n",
    "    for col in comma_cols:\n",
    "        if col in test_df.columns:\n",
    "            test_df[col] = test_df[col].astype(str).str.replace(\",\", \"\").replace(\"nan\", np.nan).astype(float)\n",
    "\n",
    "    # Combine text fields\n",
    "    text_fields = [\"Line Desc\", \"Source Desc\", \"Batch Name\"]\n",
    "    test_df[text_fields] = test_df[text_fields].fillna(\"\")\n",
    "    test_df[\"Combined_Text\"] = test_df[\"Line Desc\"] + \" | \" + test_df[\"Source Desc\"] + \" | \" + test_df[\"Batch Name\"]\n",
    "\n",
    "    # === BERT Embeddings and Clustering ===\n",
    "    embeddings = model_bert.encode(test_df[\"Combined_Text\"].tolist(), show_progress_bar=False)\n",
    "    embedding_df = pd.DataFrame(embeddings, columns=[f\"text_emb_{i}\" for i in range(embeddings.shape[1])])\n",
    "    test_df = pd.concat([test_df.reset_index(drop=True), embedding_df], axis=1)\n",
    "\n",
    "    umap_model = UMAP(n_neighbors=15, min_dist=0.1, n_components=2, random_state=42)\n",
    "    reduced = umap_model.fit_transform(embeddings)\n",
    "    kmeans = KMeans(n_clusters=10, random_state=42)\n",
    "    test_df[\"Narration_Cluster\"] = kmeans.fit_predict(reduced)\n",
    "\n",
    "    cluster_summary = (\n",
    "        test_df.groupby(\"Narration_Cluster\")[\"Combined_Text\"]\n",
    "        .apply(lambda x: \"; \".join(x.head(3)))\n",
    "        .reset_index(name=\"Narration_Cluster_Label\")\n",
    "    )\n",
    "    test_df = test_df.merge(cluster_summary, on=\"Narration_Cluster\", how=\"left\")\n",
    "\n",
    "    # === Date Features ===\n",
    "    date_cols = [\"Accounting Date\", \"Invoice Date\", \"Posted Date\"]\n",
    "    for col in date_cols:\n",
    "        test_df[col] = pd.to_datetime(test_df[col], errors=\"coerce\")\n",
    "\n",
    "    test_df[\"Accounting_Month\"] = test_df[\"Accounting Date\"].dt.month\n",
    "    test_df[\"Accounting_Weekday\"] = test_df[\"Accounting Date\"].dt.weekday\n",
    "    test_df[\"Invoice_Month\"] = test_df[\"Invoice Date\"].dt.month\n",
    "    test_df[\"Invoice_Weekday\"] = test_df[\"Invoice Date\"].dt.weekday\n",
    "    test_df[\"Posted_Month\"] = test_df[\"Posted Date\"].dt.month\n",
    "    test_df[\"Posted_Weekday\"] = test_df[\"Posted Date\"].dt.weekday\n",
    "\n",
    "    # === Feature Preparation ===\n",
    "    exclude_cols = [\"S. No\", \"Combined_Text\", \"Accounting Date\", \"Invoice Date\", \"Posted Date\"]\n",
    "    model_feature_names = model.feature_names_\n",
    "    feature_cols = [col for col in test_df.columns if col in model_feature_names and col not in exclude_cols and not col.startswith(\"Unnamed\")]\n",
    "\n",
    "    for col in feature_cols:\n",
    "        if test_df[col].dtype == object or test_df[col].isnull().any():\n",
    "            test_df[col] = test_df[col].astype(str).fillna(\"Missing\")\n",
    "\n",
    "    X_final = test_df[feature_cols].copy()\n",
    "\n",
    "    # === Model Predictions ===\n",
    "    test_df[\"Model_Score\"] = model.predict_proba(X_final)[:, 1]\n",
    "    test_df[\"Final_Score\"] = test_df[\"Model_Score\"].round(3)\n",
    "\n",
    "    # === SHAP Analysis ===\n",
    "    explainer = shap.TreeExplainer(model)\n",
    "    shap_values = explainer.shap_values(X_final)\n",
    "\n",
    "    # === Initialize BERT Explainer ===\n",
    "    bert_explainer = BERTRiskExplainer()\n",
    "\n",
    "    # === Control Points Setup ===\n",
    "    cp_score_dict = {\n",
    "        \"CP_01\": 83, \"CP_02\": 86, \"CP_03\": 78, \"CP_04\": 81, \"CP_07\": 84, \"CP_08\": 80,\n",
    "        \"CP_09\": 76, \"CP_15\": 88, \"CP_16\": 73, \"CP_17\": 75, \"CP_19\": 60,\n",
    "        \"CP_21\": 69, \"CP_22\": 66, \"CP_23\": 87, \"CP_24\": 78, \"CP_26\": 0,\n",
    "        \"CP_30\": 72, \"CP_32\": 72\n",
    "    }\n",
    "    valid_cps = list(cp_score_dict.keys())\n",
    "\n",
    "    pl_net_total = test_df[test_df[\"PL/ BS\"] == \"PL\"][\"Net Amount\"].abs().sum()\n",
    "    pl_net_threshold = 0.10 * pl_net_total\n",
    "    total_net = test_df[\"Net Amount\"].abs().sum()\n",
    "\n",
    "    # === Control Point Functions ===\n",
    "    def cp_01(row):\n",
    "        keywords = ['fraud','bribe','kickback','suspicious','fake','dummy','gift','prize','token','reward','favour']\n",
    "        text = f\"{str(row.get('Line Desc', '')).lower()} {str(row.get('Source Desc', '')).lower()}\"\n",
    "        return int(any(k in text for k in keywords))\n",
    "\n",
    "    def cp_02(row):\n",
    "        return int(row.get(\"PL/ BS\") == \"PL\" and abs(row.get(\"Net Amount\", 0)) > pl_net_threshold)\n",
    "\n",
    "    def cp_03_flags(df):\n",
    "        a = df.duplicated(subset=[\"Accounting Date\", \"Line Desc\", \"Source Desc\", \"Source Name\"], keep=False)\n",
    "        b = df.duplicated(subset=[\"Accounting Date\", \"Account Code\", \"Net Amount\"], keep=False)\n",
    "        c = df.duplicated(subset=[\"Document Number\"], keep=False) & ~df.duplicated(subset=[\"Accounting Date\", \"Document Number\"], keep=False)\n",
    "        d = df.duplicated(subset=[\"Accounting Date\", \"Line Desc\", \"Account Code\"], keep=False)\n",
    "        return ((a | b | c | d).astype(int))\n",
    "\n",
    "    def cp_04(row): return cp_02(row)\n",
    "\n",
    "    def cp_07_flags(df): return (df.groupby(\"Document Number\")[\"Net Amount\"].transform(\"sum\").round(2) != 0).astype(int)\n",
    "\n",
    "    def cp_08(row):\n",
    "        text = f\"{row.get('Account Name', '')} {row.get('Line Desc', '')} {row.get('Source Desc', '')}\".lower()\n",
    "        return int(\"cash in hand\" in text)\n",
    "\n",
    "    def cp_09_flags(df):\n",
    "        result = pd.Series(0, index=df.index)\n",
    "        for doc_id, group in df.groupby(\"Document Number\"):\n",
    "            accs = group[\"Account Name\"].dropna().str.lower().tolist()\n",
    "            if any(\"cash\" in a for a in accs) and any(\"bad debt\" in a for a in accs):\n",
    "                result[group.index] = 1\n",
    "        return result\n",
    "\n",
    "    def cp_15_flags(df):\n",
    "        grp_sum = df.groupby([\"Account Code\", \"Accounting Date\"])[[\"Entered Dr SUM\", \"Entered Cr SUM\"]].sum().sum(axis=1)\n",
    "        keys = grp_sum[grp_sum > 0.03 * total_net].index\n",
    "        return df.set_index([\"Account Code\", \"Accounting Date\"]).index.isin(keys).astype(int)\n",
    "\n",
    "    def cp_16_flags(df):\n",
    "        if \"Currency\" not in df.columns:\n",
    "            df[\"Currency\"] = \"INR\"\n",
    "        docs = df.groupby(\"Document Number\")[\"Currency\"].nunique()\n",
    "        flagged = docs[docs > 1].index\n",
    "        return df[\"Document Number\"].isin(flagged).astype(int)\n",
    "\n",
    "    def cp_17_flags(df):\n",
    "        sums = df[df[\"PL/ BS\"] == \"PL\"].groupby(\"Source Name\")[\"Net Amount\"].sum().abs()\n",
    "        risky = sums[sums > 0.03 * pl_net_total].index\n",
    "        return df[\"Source Name\"].isin(risky).astype(int)\n",
    "\n",
    "    def cp_19(row):\n",
    "        try: return int(pd.to_datetime(row[\"Accounting Date\"]).weekday() == 6)\n",
    "        except: return 0\n",
    "\n",
    "    def cp_21(row):\n",
    "        try:\n",
    "            date = pd.to_datetime(row.get(\"Accounting Date\"))\n",
    "            return int(date == (date + pd.offsets.MonthEnd(0)))\n",
    "        except: return 0\n",
    "\n",
    "    def cp_22(row):\n",
    "        try:\n",
    "            date = pd.to_datetime(row.get(\"Accounting Date\"))\n",
    "            return int(date.day == 1)\n",
    "        except: return 0\n",
    "\n",
    "    def cp_23(row):\n",
    "        text = f\"{row.get('Line Desc', '')} {row.get('Account Name', '')}\".lower()\n",
    "        return int(any(t in text for t in ['derivative', 'spv', 'structured', 'note', 'swap']))\n",
    "\n",
    "    def cp_24(row):\n",
    "        try:\n",
    "            last = str(int(abs(row.get(\"Net Amount\", 0))))[-3:]\n",
    "            seqs = {'123','234','345','456','567','678','789','890','321','432','543','654','765','876','987','098'}\n",
    "            repeats = {str(i)*3 for i in range(10)} | {'000'}\n",
    "            return int(last in seqs or last in repeats and last != '901')\n",
    "        except: return 0\n",
    "\n",
    "    def cp_26_flags(df):\n",
    "        try:\n",
    "            doc_ids = sorted(df[\"Document Number\"].dropna().astype(int).unique())\n",
    "            missing = {doc_ids[i]+1 for i in range(len(doc_ids)-1) if doc_ids[i+1] - doc_ids[i] > 1}\n",
    "            flagged = set()\n",
    "            for miss in missing:\n",
    "                flagged.update([miss-1, miss+1])\n",
    "            return df[\"Document Number\"].astype(int).isin(flagged).astype(int)\n",
    "        except: return pd.Series(0, index=df.index)\n",
    "\n",
    "    def cp_30(row):\n",
    "        text = f\"{row.get('Line Desc', '')} {row.get('Account Name', '')}\".lower()\n",
    "        return int(any(t in text for t in ['derivative','option','swap','future','structured']))\n",
    "\n",
    "    def cp_32(row): return int(row.get(\"Net Amount\", 0) == 0)\n",
    "\n",
    "    # === Apply All Control Points ===\n",
    "    test_df[\"CP_01\"] = test_df.apply(cp_01, axis=1)\n",
    "    test_df[\"CP_02\"] = test_df.apply(cp_02, axis=1)\n",
    "    test_df[\"CP_03\"] = cp_03_flags(test_df)\n",
    "    test_df[\"CP_04\"] = test_df.apply(cp_04, axis=1)\n",
    "    test_df[\"CP_07\"] = cp_07_flags(test_df)\n",
    "    test_df[\"CP_08\"] = test_df.apply(cp_08, axis=1)\n",
    "    test_df[\"CP_09\"] = cp_09_flags(test_df)\n",
    "    test_df[\"CP_15\"] = cp_15_flags(test_df)\n",
    "    \n",
    "    # Ensure Currency column is created before CP_16\n",
    "    if \"Currency\" not in test_df.columns:\n",
    "        test_df[\"Currency\"] = \"INR\"\n",
    "    test_df[\"CP_16\"] = cp_16_flags(test_df)\n",
    "    \n",
    "    test_df[\"CP_17\"] = cp_17_flags(test_df)\n",
    "    test_df[\"CP_19\"] = test_df.apply(cp_19, axis=1)\n",
    "    test_df[\"CP_21\"] = test_df.apply(cp_21, axis=1)\n",
    "    test_df[\"CP_22\"] = test_df.apply(cp_22, axis=1)\n",
    "    test_df[\"CP_23\"] = test_df.apply(cp_23, axis=1)\n",
    "    test_df[\"CP_24\"] = test_df.apply(cp_24, axis=1)\n",
    "    test_df[\"CP_26\"] = cp_26_flags(test_df)\n",
    "    test_df[\"CP_30\"] = test_df.apply(cp_30, axis=1)\n",
    "    test_df[\"CP_32\"] = test_df.apply(cp_32, axis=1)\n",
    "\n",
    "    def compute_cp_score(row):\n",
    "        triggered = [cp for cp in valid_cps if row.get(cp, 0) == 1]\n",
    "        if not triggered: return 0.0\n",
    "        product = 1.0\n",
    "        for cp in triggered:\n",
    "            product *= (1 - cp_score_dict[cp] / 100)\n",
    "        return round(1 - product, 4)\n",
    "\n",
    "    def list_triggered_cps(row):\n",
    "        return \", \".join([f\"{cp} ({cp_score_dict[cp]})\" for cp in valid_cps if row.get(cp, 0) == 1])\n",
    "\n",
    "    test_df[\"Triggered_CPs\"] = test_df.apply(list_triggered_cps, axis=1)\n",
    "    test_df[\"CP_Score\"] = test_df.apply(compute_cp_score, axis=1)\n",
    "\n",
    "    # === Enhanced Risk Classifications (INTERNAL LISTS ONLY) ===\n",
    "    model_class_list = []\n",
    "    cp_class_list = []\n",
    "    final_risk_list = []\n",
    "    \n",
    "    for i in range(len(test_df)):\n",
    "        score = test_df.iloc[i][\"Final_Score\"]\n",
    "        cp_score = test_df.iloc[i][\"CP_Score\"]\n",
    "        \n",
    "        model_class = \"High\" if score >= 0.995 else (\"Medium\" if score >= 0.5 else \"Low\")\n",
    "        cp_class = \"High\" if cp_score >= 0.95 else (\"Medium\" if cp_score > 0.8 else \"Low\")\n",
    "        final_risk = \"High\" if model_class == \"High\" or cp_class == \"High\" else (\"Medium\" if model_class == \"Medium\" or cp_class == \"Medium\" else \"Low\")\n",
    "        \n",
    "        model_class_list.append(model_class)\n",
    "        cp_class_list.append(cp_class)\n",
    "        final_risk_list.append(final_risk)\n",
    "\n",
    "    # === August 25 Enhanced Explanation Generation ===\n",
    "    def get_cp_explanation(cp_code, row):\n",
    "        cp_explanations = {\n",
    "            \"CP_01\": \"Suspicious Keywords - Transaction contains high-risk terms requiring verification\",\n",
    "            \"CP_02\": f\"High Monetary Value - Amount of Rs{row.get('Net Amount', 0):,.0f} exceeds materiality threshold\",\n",
    "            \"CP_03\": \"Duplicate Patterns - Transaction matches multiple duplicate detection criteria\",\n",
    "            \"CP_07\": \"Document Imbalance - Document entries do not balance to zero\",\n",
    "            \"CP_08\": \"Cash Expenditure - Cash-in-hand transaction bypassing standard payment controls\",\n",
    "            \"CP_09\": \"Cash to Bad Debt - Transaction involves both cash and bad debt accounts\",\n",
    "            \"CP_15\": \"Split Transactions - Account activity exceeds normal volume threshold\",\n",
    "            \"CP_16\": \"Multiple Currencies - Document contains multiple currencies\",\n",
    "            \"CP_17\": \"Vendor Concentration - Source transactions exceed concentration limits\",\n",
    "            \"CP_19\": \"Weekend Processing - Transaction processed when standard approvals typically unavailable\",\n",
    "            \"CP_21\": \"Period-End Timing - Transaction occurs on month-end date\",\n",
    "            \"CP_22\": \"Period-Start Timing - Transaction occurs on first day of month\",\n",
    "            \"CP_23\": \"Complex Structure - Transaction involves derivative or structured instruments\",\n",
    "            \"CP_24\": \"Unusual Amount Pattern - Transaction amount follows rare sequential patterns\",\n",
    "            \"CP_26\": \"Document Gap - Document number is missing from sequence\",\n",
    "            \"CP_30\": \"Complex Instrument - Transaction involves sophisticated financial instruments\",\n",
    "            \"CP_32\": \"Zero Amount - Transaction recorded with zero net amount\"\n",
    "        }\n",
    "        return cp_explanations.get(cp_code, f\"Control Point {cp_code} triggered\")\n",
    "    \n",
    "    def parse_triggered_cps(triggered_cps_str):\n",
    "        try:\n",
    "            if not triggered_cps_str or triggered_cps_str.strip() == \"\": return []\n",
    "            cp_codes = []\n",
    "            for cp_part in triggered_cps_str.split(\", \"):\n",
    "                if \"CP_\" in cp_part:\n",
    "                    cp_code = cp_part.split(\" \")[0]\n",
    "                    cp_codes.append(cp_code)\n",
    "            return cp_codes\n",
    "        except: return []\n",
    "\n",
    "    # Generate Enhanced Explanations using August 25 Logic\n",
    "    explanation_summaries = []\n",
    "    \n",
    "    for i in range(len(X_final)):\n",
    "        try:\n",
    "            row_shap = shap_values[i]\n",
    "            row = test_df.iloc[i]\n",
    "            final_risk = final_risk_list[i]\n",
    "            model_class = model_class_list[i]\n",
    "            cp_class = cp_class_list[i]\n",
    "            \n",
    "            # Only generate enhanced explanations for High Risk transactions\n",
    "            if final_risk == \"High\":\n",
    "                bert_in_top3, bert_impact = check_bert_in_top3(row_shap, feature_cols)\n",
    "                explanations = []\n",
    "                \n",
    "                if model_class == \"High\" and cp_class != \"High\":\n",
    "                    # Model-driven risk\n",
    "                    feature_impacts = []\n",
    "                    for j, feature in enumerate(feature_cols):\n",
    "                        if row_shap[j] > 0 and not feature.startswith('text_emb_'):\n",
    "                            feature_value = row.get(feature, \"N/A\")\n",
    "                            feature_impacts.append((feature, feature_value, row_shap[j]))\n",
    "                    \n",
    "                    feature_impacts.sort(key=lambda x: x[2], reverse=True)\n",
    "                    num_regular = 2 if bert_in_top3 and bert_impact >= 0.05 else 3\n",
    "                    \n",
    "                    used_explanations = set()\n",
    "                    consolidated_amounts = []\n",
    "                    \n",
    "                    for feature, value, shap_val in feature_impacts:\n",
    "                        if feature in [\"Net Amount\", \"Entered Dr SUM\", \"Entered Cr SUM\", \"Accounted Dr SUM\", \"Accounted Cr SUM\"]:\n",
    "                            consolidated_amounts.append((feature, value, shap_val))\n",
    "                            continue\n",
    "                        \n",
    "                        explanation_text = get_auditor_friendly_explanation(feature, value, shap_val)\n",
    "                        if explanation_text is not None and explanation_text not in used_explanations:\n",
    "                            formatted_explanation = f'\"{feature}: {value}\" - {explanation_text}'\n",
    "                            explanations.append(formatted_explanation)\n",
    "                            used_explanations.add(explanation_text)\n",
    "                            if len(explanations) >= num_regular:\n",
    "                                break\n",
    "                    \n",
    "                    if consolidated_amounts and len(explanations) < num_regular:\n",
    "                        highest_amount = max(consolidated_amounts, key=lambda x: x[2])\n",
    "                        feature, value, shap_val = highest_amount\n",
    "                        explanation_text = get_auditor_friendly_explanation(feature, value, shap_val)\n",
    "                        if explanation_text is not None:\n",
    "                            formatted_explanation = f'\"Transaction Amount: Rs{float(value):,.0f}\" - {explanation_text}'\n",
    "                            explanations.insert(0, formatted_explanation)\n",
    "                    \n",
    "                    if bert_in_top3 and bert_impact >= 0.05:\n",
    "                        row_dict = row.to_dict()\n",
    "                        bert_exp = bert_explainer.explain_bert_risk(row_dict, bert_impact)\n",
    "                        if bert_exp:\n",
    "                            explanations.append(bert_exp)\n",
    "                    \n",
    "                elif model_class != \"High\" and cp_class == \"High\":\n",
    "                    # CP-driven risk\n",
    "                    triggered_cps = parse_triggered_cps(row.get(\"Triggered_CPs\", \"\"))\n",
    "                    num_cp = 2 if bert_in_top3 and bert_impact >= 0.05 else 3\n",
    "                    \n",
    "                    for cp_code in triggered_cps[:num_cp]:\n",
    "                        cp_explanation = get_cp_explanation(cp_code, row)\n",
    "                        explanations.append(cp_explanation)\n",
    "                    \n",
    "                    if bert_in_top3 and bert_impact >= 0.05:\n",
    "                        row_dict = row.to_dict()\n",
    "                        bert_exp = bert_explainer.explain_bert_risk(row_dict, bert_impact)\n",
    "                        if bert_exp:\n",
    "                            explanations.append(bert_exp)\n",
    "                    \n",
    "                elif model_class == \"High\" and cp_class == \"High\":\n",
    "                    # Both model and CP high risk\n",
    "                    num_model = 1 if bert_in_top3 and bert_impact >= 0.05 else 2\n",
    "                    num_cp = 1\n",
    "                    \n",
    "                    # Model features\n",
    "                    feature_impacts = []\n",
    "                    for j, feature in enumerate(feature_cols):\n",
    "                        if row_shap[j] > 0 and not feature.startswith('text_emb_'):\n",
    "                            feature_value = row.get(feature, \"N/A\")\n",
    "                            feature_impacts.append((feature, feature_value, row_shap[j]))\n",
    "                    \n",
    "                    feature_impacts.sort(key=lambda x: x[2], reverse=True)\n",
    "                    \n",
    "                    for k, (feature, value, shap_val) in enumerate(feature_impacts[:num_model]):\n",
    "                        explanation_text = get_auditor_friendly_explanation(feature, value, shap_val)\n",
    "                        if explanation_text is not None:\n",
    "                            if feature in [\"Net Amount\", \"Entered Dr SUM\", \"Entered Cr SUM\", \"Accounted Dr SUM\", \"Accounted Cr SUM\"]:\n",
    "                                formatted_explanation = f'\"Transaction Amount: Rs{float(value):,.0f}\" - {explanation_text}'\n",
    "                            else:\n",
    "                                formatted_explanation = f'\"{feature}: {value}\" - {explanation_text}'\n",
    "                            explanations.append(formatted_explanation)\n",
    "                    \n",
    "                    # CP features\n",
    "                    triggered_cps = parse_triggered_cps(row.get(\"Triggered_CPs\", \"\"))\n",
    "                    for cp_code in triggered_cps[:num_cp]:\n",
    "                        cp_explanation = get_cp_explanation(cp_code, row)\n",
    "                        explanations.append(cp_explanation)\n",
    "                    \n",
    "                    if bert_in_top3 and bert_impact >= 0.05:\n",
    "                        row_dict = row.to_dict()\n",
    "                        bert_exp = bert_explainer.explain_bert_risk(row_dict, bert_impact)\n",
    "                        if bert_exp:\n",
    "                            explanations.append(bert_exp)\n",
    "                \n",
    "                # Create enhanced explanation for High Risk\n",
    "                if explanations:\n",
    "                    final_explanation = \"\\n\".join(explanations[:3])\n",
    "                    explanation_summaries.append(final_explanation)\n",
    "                else:\n",
    "                    explanation_summaries.append(\"High risk transaction requiring enhanced review\")\n",
    "            else:\n",
    "                # For non-High risk transactions, use empty explanation\n",
    "                explanation_summaries.append(\"\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            explanation_summaries.append(\"\")\n",
    "\n",
    "    # === Generate Original Feature Group Explanations ===\n",
    "    amount_features = [\"Entered Dr SUM\", \"Entered Cr SUM\", \"Accounted Dr SUM\", \"Accounted Cr SUM\", \"Net Amount\"]\n",
    "    date_features = [\"Accounting_Month\", \"Accounting_Weekday\", \"Invoice_Month\", \"Invoice_Weekday\", \"Posted_Month\", \"Posted_Weekday\"]\n",
    "    account_info_features = [\"Account Name\", \"Nature in balance sheet\", \"Source Name\", \"Document Type\", \"Tax Rate\", \"Tax Rate Name\"]\n",
    "    other_features = [col for col in model.feature_names_ if col not in amount_features + date_features + account_info_features and not col.startswith(\"text_emb_\")]\n",
    "\n",
    "    feature_groups = {\n",
    "        \"Amount\": amount_features,\n",
    "        \"Date\": date_features,\n",
    "        \"Source Info\": account_info_features,\n",
    "        \"Batch\": other_features,\n",
    "        \"Narration\": [\"Narration_Cluster_Label\"]\n",
    "    }\n",
    "\n",
    "    explanation_templates = {\n",
    "        \"Narration\": \"Narration pattern resembles high-value or structured payouts\",\n",
    "        \"Amount\": \"High {feature} = ₹{value:,.0f}\",\n",
    "        \"Date\": \"Posted on {feature} = {value}\",\n",
    "        \"Source Info\": \"{feature} = '{value}' is missing or looks suspicious\",\n",
    "        \"Batch\": \"Batch reference '{value}' appears frequently in vendor payments\"\n",
    "    }\n",
    "\n",
    "    top_risky_texts, top_safe_texts = [], []\n",
    "    for i in range(len(X_final)):\n",
    "        row_shap = shap_values[i]\n",
    "        row = test_df.iloc[i]\n",
    "\n",
    "        impact_by_group = {}\n",
    "        feature_info = {}\n",
    "        for group, features in feature_groups.items():\n",
    "            valid_feats = [f for f in features if f in feature_cols]\n",
    "            if not valid_feats:\n",
    "                continue\n",
    "            group_shap_sum = sum(row_shap[feature_cols.index(f)] for f in valid_feats)\n",
    "            impact_by_group[group] = group_shap_sum\n",
    "            top_feat = max(valid_feats, key=lambda f: abs(row_shap[feature_cols.index(f)]))\n",
    "            value = row.get(top_feat, \"N/A\")\n",
    "            feature_info[group] = (top_feat, value)\n",
    "\n",
    "        sorted_risk = sorted(impact_by_group.items(), key=lambda x: x[1], reverse=True)\n",
    "        sorted_safe = sorted(impact_by_group.items(), key=lambda x: x[1])\n",
    "\n",
    "        def render(group, feature, value):\n",
    "            if group == \"Narration\":\n",
    "                return explanation_templates[group]\n",
    "            elif group in explanation_templates:\n",
    "                return explanation_templates[group].format(feature=feature, value=value)\n",
    "            else:\n",
    "                return f\"{group}: {feature} = {value}\"\n",
    "\n",
    "        top_risk = [render(g, *feature_info[g]) for g, _ in sorted_risk[:3]]\n",
    "        top_safe = [render(g, *feature_info[g]) for g, _ in sorted_safe if g not in [r[0] for r in sorted_risk[:3]][:2]]\n",
    "\n",
    "        top_risky_texts.append(\"\\n\".join(f\"- {t}\" for t in top_risk))\n",
    "        top_safe_texts.append(\"\\n\".join(f\"- {t}\" for t in top_safe[:2]))\n",
    "\n",
    "    # Remove BERT embedding columns before creating final columns\n",
    "    test_df = test_df.drop(columns=[col for col in test_df.columns if col.startswith(\"text_emb_\")])\n",
    "\n",
    "    # === Create Explanation Columns (Positions 42-44) ===\n",
    "    test_df[\"Top_Risky_Feature_Groups\"] = top_risky_texts\n",
    "    test_df[\"Top_Safe_Feature_Groups\"] = top_safe_texts\n",
    "    test_df[\"Explanation_Summary\"] = explanation_summaries\n",
    "\n",
    "    # === Final Column Order (65 columns) ===\n",
    "    expected_columns = [\n",
    "        # Original 30 columns\n",
    "        \"S. No\", \"Entity Name\", \"Accounting Date\", \"Approval Type\", \"Document Type\", \"Invoice Date\", \"Day\", \"Nature\",\n",
    "        \"Account Code\", \"PL/ BS\", \"Report Group\", \"Account Name\", \"Nature in balance sheet\", \"Document Number\", \"Je Line Num\",\n",
    "        \"Source Number\", \"Source Name\", \"Source Voucher Name\", \"Source Desc\", \"Line Desc\", \"Project Code\", \"Internal Reference\", \n",
    "        \"Posted Date\", \"Branch\", \"Batch Name\", \"Entered Dr SUM\", \"Entered Cr SUM\", \"Accounted Dr SUM\", \"Accounted Cr SUM\", \"Net Amount\",\n",
    "        \n",
    "        # Generated analysis columns\n",
    "        \"Combined_Text\", \"Narration_Cluster\", \"Narration_Cluster_Label\",\n",
    "        \"Accounting_Month\", \"Accounting_Weekday\", \"Invoice_Month\", \"Invoice_Weekday\", \"Posted_Month\", \"Posted_Weekday\",\n",
    "        \"Model_Score\", \"Final_Score\",\n",
    "        \n",
    "        # Explanation columns (positions 42-44)\n",
    "        \"Top_Risky_Feature_Groups\", \"Top_Safe_Feature_Groups\", \"Explanation_Summary\",\n",
    "        \n",
    "        # Control points columns (18 total)\n",
    "        \"CP_01\", \"CP_02\", \"CP_03\", \"CP_04\", \"CP_07\", \"CP_08\", \"CP_09\", \"CP_15\", \"Currency\", \"CP_16\", \"CP_17\", \n",
    "        \"CP_19\", \"CP_21\", \"CP_22\", \"CP_23\", \"CP_24\", \"CP_26\", \"CP_30\", \"CP_32\", \"Triggered_CPs\", \"CP_Score\"\n",
    "    ]\n",
    "    \n",
    "    # Validate all expected columns exist\n",
    "    missing_columns = [col for col in expected_columns if col not in test_df.columns]\n",
    "    if missing_columns:\n",
    "        raise ValueError(f\"Missing required columns: {missing_columns}\")\n",
    "    \n",
    "    # Reorder columns to exact specification\n",
    "    final_df = test_df[expected_columns].copy()\n",
    "    \n",
    "    # Final validation: ensure exactly 65 columns\n",
    "    if len(final_df.columns) != 65:\n",
    "        raise ValueError(f\"Expected 65 columns, got {len(final_df.columns)}\")\n",
    "    \n",
    "    logging.debug(\"run_full_pipeline complete\")\n",
    "    return final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "15d85e16-9d24-492b-bfbb-77b16257ee31",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-20 13:07:53,005 - INFO - Use pytorch device_name: cpu\n",
      "2025-09-20 13:07:53,007 - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "TESTING CORRECTED MODEL_LOGIC DIRECTLY IN JUPYTER\n",
      "======================================================================\n",
      "Loading CatBoost model...\n",
      "CatBoost model loaded\n",
      "Downloading SentenceTransformer...\n",
      "SentenceTransformer loaded\n",
      "Pipeline completed!\n",
      "Total transactions: 874\n",
      "Total columns: 65\n",
      "\n",
      "COLUMN LIST (65 total):\n",
      " 1. S. No\n",
      " 2. Entity Name\n",
      " 3. Accounting Date\n",
      " 4. Approval Type\n",
      " 5. Document Type\n",
      " 6. Invoice Date\n",
      " 7. Day\n",
      " 8. Nature\n",
      " 9. Account Code\n",
      "10. PL/ BS\n",
      "11. Report Group\n",
      "12. Account Name\n",
      "13. Nature in balance sheet\n",
      "14. Document Number\n",
      "15. Je Line Num\n",
      "16. Source Number\n",
      "17. Source Name\n",
      "18. Source Voucher Name\n",
      "19. Source Desc\n",
      "20. Line Desc\n",
      "21. Project Code\n",
      "22. Internal Reference\n",
      "23. Posted Date\n",
      "24. Branch\n",
      "25. Batch Name\n",
      "26. Entered Dr SUM\n",
      "27. Entered Cr SUM\n",
      "28. Accounted Dr SUM\n",
      "29. Accounted Cr SUM\n",
      "30. Net Amount\n",
      "31. Combined_Text\n",
      "32. Narration_Cluster\n",
      "33. Narration_Cluster_Label\n",
      "34. Accounting_Month\n",
      "35. Accounting_Weekday\n",
      "36. Invoice_Month\n",
      "37. Invoice_Weekday\n",
      "38. Posted_Month\n",
      "39. Posted_Weekday\n",
      "40. Model_Score\n",
      "41. Final_Score\n",
      "42. Top_Risky_Feature_Groups\n",
      "43. Top_Safe_Feature_Groups\n",
      "44. Explanation_Summary\n",
      "45. CP_01\n",
      "46. CP_02\n",
      "47. CP_03\n",
      "48. CP_04\n",
      "49. CP_07\n",
      "50. CP_08\n",
      "51. CP_09\n",
      "52. CP_15\n",
      "53. Currency\n",
      "54. CP_16\n",
      "55. CP_17\n",
      "56. CP_19\n",
      "57. CP_21\n",
      "58. CP_22\n",
      "59. CP_23\n",
      "60. CP_24\n",
      "61. CP_26\n",
      "62. CP_30\n",
      "63. CP_32\n",
      "64. Triggered_CPs\n",
      "65. CP_Score\n",
      "\n",
      "✅ NO PROBLEM COLUMNS FOUND\n",
      "\n",
      "EXPLANATION COLUMN POSITIONS:\n",
      "   Top_Risky_Feature_Groups: Position 42\n",
      "   Top_Safe_Feature_Groups: Position 43\n",
      "   Explanation_Summary: Position 44\n",
      "\n",
      "💾 Output saved as: Jupyter_Test_Output_20250920_130821.xlsx\n",
      "\n",
      "Sample High Risk Explanation:\n",
      "Score: 0.924\n",
      "Explanation: Document Imbalance - Document entries do not balance to zero\n",
      "Period-End Timing - Transaction occurs on month-end date\n",
      "Unusual Amount Pattern - Transaction amount follows rare sequential patterns\n",
      "\n",
      "======================================================================\n",
      "✅ SUCCESS: 65 columns achieved!\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# === COMPLETE CORRECTED MODEL_LOGIC TEST IN JUPYTER ===\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import warnings\n",
    "import logging\n",
    "from typing import Dict, List, Tuple, Optional, Union\n",
    "from datetime import datetime\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# [PASTE THE ENTIRE CORRECTED MODEL_LOGIC CODE HERE - from the artifact]\n",
    "# Just copy the complete corrected model_logic code from the artifact above\n",
    "\n",
    "# === TEST THE PIPELINE ===\n",
    "print(\"=\" * 70)\n",
    "print(\"TESTING CORRECTED MODEL_LOGIC DIRECTLY IN JUPYTER\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "try:\n",
    "    # Run the pipeline\n",
    "    result_df = run_full_pipeline(\"old_test_file.csv\")\n",
    "    \n",
    "    # Check results\n",
    "    print(f\"Pipeline completed!\")\n",
    "    print(f\"Total transactions: {len(result_df):,}\")\n",
    "    print(f\"Total columns: {len(result_df.columns)}\")\n",
    "    \n",
    "    # List all columns with positions\n",
    "    print(f\"\\nCOLUMN LIST ({len(result_df.columns)} total):\")\n",
    "    for i, col in enumerate(result_df.columns, 1):\n",
    "        print(f\"{i:2d}. {col}\")\n",
    "    \n",
    "    # Check for the problematic classification columns\n",
    "    problem_columns = ['Model Classification', 'CP Classification', 'Final Risk Classification']\n",
    "    found_problems = [col for col in problem_columns if col in result_df.columns]\n",
    "    \n",
    "    if found_problems:\n",
    "        print(f\"\\n❌ PROBLEM COLUMNS FOUND: {found_problems}\")\n",
    "    else:\n",
    "        print(f\"\\n✅ NO PROBLEM COLUMNS FOUND\")\n",
    "    \n",
    "    # Verify column order\n",
    "    expected_explanation_positions = [42, 43, 44]  # Top_Risky_Feature_Groups, Top_Safe_Feature_Groups, Explanation_Summary\n",
    "    actual_explanation_columns = ['Top_Risky_Feature_Groups', 'Top_Safe_Feature_Groups', 'Explanation_Summary']\n",
    "    \n",
    "    print(f\"\\nEXPLANATION COLUMN POSITIONS:\")\n",
    "    for col in actual_explanation_columns:\n",
    "        if col in result_df.columns:\n",
    "            position = list(result_df.columns).index(col) + 1\n",
    "            print(f\"   {col}: Position {position}\")\n",
    "    \n",
    "    # Generate output file\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    output_filename = f\"Jupyter_Test_Output_{timestamp}.xlsx\"\n",
    "    result_df.to_excel(output_filename, index=False)\n",
    "    print(f\"\\n💾 Output saved as: {output_filename}\")\n",
    "    \n",
    "    # Show sample explanation\n",
    "    high_risk = result_df[result_df['Final_Score'] >= 0.8]\n",
    "    if len(high_risk) > 0:\n",
    "        sample = high_risk.iloc[0]\n",
    "        print(f\"\\nSample High Risk Explanation:\")\n",
    "        print(f\"Score: {sample['Final_Score']:.3f}\")\n",
    "        print(f\"Explanation: {sample['Explanation_Summary']}\")\n",
    "    \n",
    "    print(f\"\\n\" + \"=\" * 70)\n",
    "    if len(result_df.columns) == 65:\n",
    "        print(\"✅ SUCCESS: 65 columns achieved!\")\n",
    "    else:\n",
    "        print(f\"❌ ISSUE: Got {len(result_df.columns)} columns instead of 65\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ ERROR: {str(e)}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "77e0dc44-8e0f-4307-ba26-29e11ecfa043",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: C:\\Users\\Acer\\Audit-Risk-V2\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(\"Current working directory:\", os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "45d84739-e162-444a-930c-86059f2aa739",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python path:\n",
      "  C:\\Users\\Acer\\anaconda3\\python313.zip\n",
      "  C:\\Users\\Acer\\anaconda3\\DLLs\n",
      "  C:\\Users\\Acer\\anaconda3\\Lib\n",
      "  C:\\Users\\Acer\\anaconda3\n",
      "  \n",
      "  C:\\Users\\Acer\\anaconda3\\Lib\\site-packages\n",
      "  C:\\Users\\Acer\\anaconda3\\Lib\\site-packages\\win32\n",
      "  C:\\Users\\Acer\\anaconda3\\Lib\\site-packages\\win32\\lib\n",
      "  C:\\Users\\Acer\\anaconda3\\Lib\\site-packages\\Pythonwin\n",
      "  ./app\n",
      "  ./app\n",
      "  ./app\n",
      "  ./app\n",
      "  ./app\n",
      "  ./app\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(\"Python path:\")\n",
    "for path in sys.path:\n",
    "    print(f\"  {path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bdc2b156-26de-42e2-97b3-e789c9cc43a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_logic is already loaded in memory\n",
      "Loaded from: C:\\Users\\Acer\\Audit-Risk-V2\\app\\model_logic.py\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "# Check if model_logic is already loaded\n",
    "if 'model_logic' in sys.modules:\n",
    "    print(\"model_logic is already loaded in memory\")\n",
    "    import inspect\n",
    "    import model_logic\n",
    "    print(\"Loaded from:\", inspect.getfile(model_logic))\n",
    "else:\n",
    "    print(\"model_logic is not loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fab56c6b-ef7b-47c4-b469-f5bdc30d1dd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_logic imported from: C:\\Users\\Acer\\Audit-Risk-V2\\app\\model_logic.py\n",
      "File size: 38826 bytes\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    # This will show you which file it's trying to load\n",
    "    import model_logic\n",
    "    import inspect\n",
    "    print(\"model_logic imported from:\", inspect.getfile(model_logic))\n",
    "    print(\"File size:\", os.path.getsize(inspect.getfile(model_logic)), \"bytes\")\n",
    "except ImportError as e:\n",
    "    print(\"Import error:\", e)\n",
    "except Exception as e:\n",
    "    print(\"Other error:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "afe89cec-45d5-4e98-bf87-82fe3701504d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-20 13:14:06,624 - INFO - Use pytorch device_name: cpu\n",
      "2025-09-20 13:14:06,625 - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed cache file: C:\\Users\\Acer\\Audit-Risk-V2\\app\\__pycache__\\model_logic.cpython-311.pyc\n",
      "Removed cache file: C:\\Users\\Acer\\Audit-Risk-V2\\app\\__pycache__\\model_logic.cpython-313.pyc\n",
      "Importing fresh version...\n",
      "Loading CatBoost model...\n",
      "CatBoost model loaded\n",
      "Downloading SentenceTransformer...\n",
      "SentenceTransformer loaded\n",
      "Columns after clearing cache: 65\n",
      "✅ Fixed - now getting 65 columns\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import importlib\n",
    "import os\n",
    "\n",
    "# Step 1: Clear the module from memory completely\n",
    "if 'model_logic' in sys.modules:\n",
    "    del sys.modules['model_logic']\n",
    "\n",
    "# Step 2: Clear any cached bytecode\n",
    "model_logic_path = \"C:\\\\Users\\\\Acer\\\\Audit-Risk-V2\\\\app\\\\model_logic.py\"\n",
    "pycache_file = \"C:\\\\Users\\\\Acer\\\\Audit-Risk-V2\\\\app\\\\__pycache__\\\\model_logic.cpython-*.pyc\"\n",
    "\n",
    "# Remove pycache files if they exist\n",
    "import glob\n",
    "for cache_file in glob.glob(\"C:\\\\Users\\\\Acer\\\\Audit-Risk-V2\\\\app\\\\__pycache__\\\\model_logic.*\"):\n",
    "    try:\n",
    "        os.remove(cache_file)\n",
    "        print(f\"Removed cache file: {cache_file}\")\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "# Step 3: Restart Python path\n",
    "if \"./app\" in sys.path:\n",
    "    sys.path.remove(\"./app\")\n",
    "sys.path.insert(0, \"./app\")\n",
    "\n",
    "# Step 4: Fresh import\n",
    "print(\"Importing fresh version...\")\n",
    "from model_logic import run_full_pipeline\n",
    "\n",
    "# Step 5: Test immediately\n",
    "result_df = run_full_pipeline(\"old_test_file.csv\")\n",
    "print(f\"Columns after clearing cache: {len(result_df.columns)}\")\n",
    "\n",
    "if len(result_df.columns) == 68:\n",
    "    print(\"❌ Still getting 68 columns - the file wasn't updated correctly\")\n",
    "elif len(result_df.columns) == 65:\n",
    "    print(\"✅ Fixed - now getting 65 columns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d5954c9d-7831-4a10-a37d-523614bee426",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-21 11:49:57,263 - INFO - Use pytorch device_name: cpu\n",
      "2025-09-21 11:49:57,265 - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "ENHANCED RISK CLASSIFICATION PIPELINE - PRODUCTION RUN\n",
      "================================================================================\n",
      "Input file: old_test_file.csv\n",
      "Output file: Enhanced_Risk_Classification_Output_20250921_114957.xlsx\n",
      "\n",
      "Starting pipeline processing...\n",
      "Loading CatBoost model...\n",
      "CatBoost model loaded\n",
      "Downloading SentenceTransformer...\n",
      "SentenceTransformer loaded\n",
      "\n",
      "Pipeline completed successfully!\n",
      "Transactions processed: 874\n",
      "Output columns: 65\n",
      "✅ Column count correct: 65 columns\n",
      "\n",
      "Risk Classification Results:\n",
      "   High Risk: 69 (7.9%)\n",
      "   Medium Risk: 98 (11.2%)\n",
      "   Low Risk: 707 (80.9%)\n",
      "\n",
      "Enhanced Explanations:\n",
      "   Transactions with explanations: 102\n",
      "   Explanation coverage: 11.7%\n",
      "\n",
      "Saving results to: Enhanced_Risk_Classification_Output_20250921_114957.xlsx\n",
      "✅ Output file saved successfully!\n",
      "\n",
      "Sample Enhanced Explanation:\n",
      "Transaction ID: 4\n",
      "Risk Score: 0.924\n",
      "Explanation:\n",
      "Period-End Timing - Transaction occurs on month-end date when adjustments are commonly made and misstatements possible  # Aug-25 explanation restored\n",
      "Unusual Amount Pattern - Transaction amount follows rare sequential or repetitive digit patterns requiring enhanced review  # Aug-25 explanation restored\n",
      "\n",
      "================================================================================\n",
      "PIPELINE EXECUTION COMPLETED SUCCESSFULLY\n",
      "Output file ready: Enhanced_Risk_Classification_Output_20250921_114957.xlsx\n",
      "================================================================================\n",
      "\n",
      "Success! Your enhanced risk classification file is ready:\n",
      "📄 Enhanced_Risk_Classification_Output_20250921_114957.xlsx\n",
      "📊 874 transactions analyzed with enhanced explanations\n"
     ]
    }
   ],
   "source": [
    "# === COMPLETE PIPELINE TEST - LOAD MODEL AND GENERATE OUTPUT ===\n",
    "\n",
    "import sys\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "# Ensure the app directory is in Python path\n",
    "sys.path.insert(0, './app')\n",
    "\n",
    "# Clear any cached imports to ensure fresh load\n",
    "if 'model_logic' in sys.modules:\n",
    "    del sys.modules['model_logic']\n",
    "\n",
    "# Import the corrected model\n",
    "from model_logic import run_full_pipeline\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"ENHANCED RISK CLASSIFICATION PIPELINE - PRODUCTION RUN\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "def process_risk_file(input_file_path, output_file_name=None):\n",
    "    \"\"\"\n",
    "    Process input file through enhanced risk classification pipeline\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Generate output filename if not provided\n",
    "        if output_file_name is None:\n",
    "            timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "            output_file_name = f\"Enhanced_Risk_Classification_Output_{timestamp}.xlsx\"\n",
    "        \n",
    "        print(f\"Input file: {input_file_path}\")\n",
    "        print(f\"Output file: {output_file_name}\")\n",
    "        print(\"\\nStarting pipeline processing...\")\n",
    "        \n",
    "        # Run the enhanced pipeline\n",
    "        result_df = run_full_pipeline(input_file_path)\n",
    "        \n",
    "        # Validate results\n",
    "        print(f\"\\nPipeline completed successfully!\")\n",
    "        print(f\"Transactions processed: {len(result_df):,}\")\n",
    "        print(f\"Output columns: {len(result_df.columns)}\")\n",
    "        \n",
    "        # Verify 65 columns\n",
    "        if len(result_df.columns) == 65:\n",
    "            print(\"✅ Column count correct: 65 columns\")\n",
    "        else:\n",
    "            print(f\"⚠️  Column count: {len(result_df.columns)} (expected 65)\")\n",
    "        \n",
    "        # Risk distribution analysis\n",
    "        high_risk = len(result_df[result_df['Final_Score'] >= 0.98])\n",
    "        medium_risk = len(result_df[(result_df['Final_Score'] >= 0.5) & (result_df['Final_Score'] < 0.98)])\n",
    "        low_risk = len(result_df[result_df['Final_Score'] < 0.5])\n",
    "        \n",
    "        print(f\"\\nRisk Classification Results:\")\n",
    "        print(f\"   High Risk: {high_risk:,} ({high_risk/len(result_df)*100:.1f}%)\")\n",
    "        print(f\"   Medium Risk: {medium_risk:,} ({medium_risk/len(result_df)*100:.1f}%)\")\n",
    "        print(f\"   Low Risk: {low_risk:,} ({low_risk/len(result_df)*100:.1f}%)\")\n",
    "        \n",
    "        # Explanation coverage\n",
    "        explained_transactions = sum(1 for exp in result_df['Explanation_Summary'] if str(exp).strip() != '')\n",
    "        print(f\"\\nEnhanced Explanations:\")\n",
    "        print(f\"   Transactions with explanations: {explained_transactions:,}\")\n",
    "        print(f\"   Explanation coverage: {explained_transactions/len(result_df)*100:.1f}%\")\n",
    "        \n",
    "        # Save output file\n",
    "        print(f\"\\nSaving results to: {output_file_name}\")\n",
    "        result_df.to_excel(output_file_name, index=False)\n",
    "        print(\"✅ Output file saved successfully!\")\n",
    "        \n",
    "        # Show sample high-risk explanation\n",
    "        high_risk_with_explanations = result_df[\n",
    "            (result_df['Final_Score'] >= 0.8) & \n",
    "            (result_df['Explanation_Summary'].str.strip() != '')\n",
    "        ]\n",
    "        \n",
    "        if len(high_risk_with_explanations) > 0:\n",
    "            print(f\"\\nSample Enhanced Explanation:\")\n",
    "            sample = high_risk_with_explanations.iloc[0]\n",
    "            print(f\"Transaction ID: {sample['S. No']}\")\n",
    "            print(f\"Risk Score: {sample['Final_Score']:.3f}\")\n",
    "            print(f\"Explanation:\\n{sample['Explanation_Summary']}\")\n",
    "        \n",
    "        print(f\"\\n\" + \"=\" * 80)\n",
    "        print(\"PIPELINE EXECUTION COMPLETED SUCCESSFULLY\")\n",
    "        print(f\"Output file ready: {output_file_name}\")\n",
    "        print(\"=\" * 80)\n",
    "        \n",
    "        return result_df, output_file_name\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Pipeline failed: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None, None\n",
    "\n",
    "# === RUN THE PIPELINE ===\n",
    "if __name__ == \"__main__\":\n",
    "    # Process your test file\n",
    "    input_file = \"old_test_file.csv\"  # Replace with your input file path\n",
    "    \n",
    "    # Optional: specify custom output filename\n",
    "    # output_file = \"My_Custom_Output.xlsx\"\n",
    "    # result_df, output_file = process_risk_file(input_file, output_file)\n",
    "    \n",
    "    # Or use auto-generated filename\n",
    "    result_df, output_file = process_risk_file(input_file)\n",
    "    \n",
    "    if result_df is not None:\n",
    "        print(f\"\\nSuccess! Your enhanced risk classification file is ready:\")\n",
    "        print(f\"📄 {output_file}\")\n",
    "        print(f\"📊 {len(result_df):,} transactions analyzed with enhanced explanations\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base]",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
